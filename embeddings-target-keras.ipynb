{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Feature Encoding Challenge II V2\n",
    "\n",
    "Ideas:\n",
    "* Replace missing values with constant\n",
    "* Add number of missing values in row as a feature\n",
    "* Apply StandardScaler to created feature\n",
    "* Apply Target to features that have many unique values\n",
    "* Apply entity embedding layers for other features + Keras\n",
    "* Apply OHE for other features + Logit\n",
    "* Blend Logit and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input/submission.csv\n",
      "./input/train.csv\n",
      "./input/train_folds.csv\n",
      "./input/test.csv\n",
      "./input/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/train.csv', index_col='id')\n",
    "test = pd.read_csv('./input/test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bin_0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_3</th>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin_4</th>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_0</th>\n",
       "      <td>Red</td>\n",
       "      <td>Red</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_1</th>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Star</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_2</th>\n",
       "      <td>Hamster</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>Hamster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_3</th>\n",
       "      <td>Russia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_4</th>\n",
       "      <td>Bassoon</td>\n",
       "      <td>Theremin</td>\n",
       "      <td>Bassoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_5</th>\n",
       "      <td>de4c57ee2</td>\n",
       "      <td>2bb3c3e5c</td>\n",
       "      <td>b574c9841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_6</th>\n",
       "      <td>a64bc7ddf</td>\n",
       "      <td>3a3a936e8</td>\n",
       "      <td>708248125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_7</th>\n",
       "      <td>598080a91</td>\n",
       "      <td>1dddb8473</td>\n",
       "      <td>5ddc9a726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_8</th>\n",
       "      <td>0256c7a4b</td>\n",
       "      <td>52ead350c</td>\n",
       "      <td>745b909d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom_9</th>\n",
       "      <td>02e7c8990</td>\n",
       "      <td>f37df64af</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ord_0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ord_1</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ord_2</th>\n",
       "      <td>Hot</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Freezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ord_3</th>\n",
       "      <td>c</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ord_4</th>\n",
       "      <td>U</td>\n",
       "      <td>X</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ord_5</th>\n",
       "      <td>Pw</td>\n",
       "      <td>pE</td>\n",
       "      <td>eN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "id                0            1          2\n",
       "bin_0             0            1          0\n",
       "bin_1             0            1          1\n",
       "bin_2             0            0          0\n",
       "bin_3             F            F          F\n",
       "bin_4             N            Y          N\n",
       "nom_0           Red          Red        Red\n",
       "nom_1     Trapezoid         Star        NaN\n",
       "nom_2       Hamster      Axolotl    Hamster\n",
       "nom_3        Russia          NaN     Canada\n",
       "nom_4       Bassoon     Theremin    Bassoon\n",
       "nom_5     de4c57ee2    2bb3c3e5c  b574c9841\n",
       "nom_6     a64bc7ddf    3a3a936e8  708248125\n",
       "nom_7     598080a91    1dddb8473  5ddc9a726\n",
       "nom_8     0256c7a4b    52ead350c  745b909d1\n",
       "nom_9     02e7c8990    f37df64af        NaN\n",
       "ord_0             3            3          3\n",
       "ord_1   Contributor  Grandmaster        NaN\n",
       "ord_2           Hot         Warm   Freezing\n",
       "ord_3             c            e          n\n",
       "ord_4             U            X          P\n",
       "ord_5            Pw           pE         eN\n",
       "day               6            7          5\n",
       "month             3            7          9\n",
       "target            0            0          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Uniques</th>\n",
       "      <th>First Value</th>\n",
       "      <th>Second Value</th>\n",
       "      <th>Third Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bin_0</td>\n",
       "      <td>float64</td>\n",
       "      <td>17894</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bin_1</td>\n",
       "      <td>float64</td>\n",
       "      <td>18003</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bin_2</td>\n",
       "      <td>float64</td>\n",
       "      <td>17930</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bin_3</td>\n",
       "      <td>object</td>\n",
       "      <td>18014</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bin_4</td>\n",
       "      <td>object</td>\n",
       "      <td>18047</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nom_0</td>\n",
       "      <td>object</td>\n",
       "      <td>18252</td>\n",
       "      <td>3</td>\n",
       "      <td>Red</td>\n",
       "      <td>Red</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nom_1</td>\n",
       "      <td>object</td>\n",
       "      <td>18156</td>\n",
       "      <td>6</td>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Star</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nom_2</td>\n",
       "      <td>object</td>\n",
       "      <td>18035</td>\n",
       "      <td>6</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>Hamster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nom_3</td>\n",
       "      <td>object</td>\n",
       "      <td>18121</td>\n",
       "      <td>6</td>\n",
       "      <td>Russia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nom_4</td>\n",
       "      <td>object</td>\n",
       "      <td>18035</td>\n",
       "      <td>4</td>\n",
       "      <td>Bassoon</td>\n",
       "      <td>Theremin</td>\n",
       "      <td>Bassoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nom_5</td>\n",
       "      <td>object</td>\n",
       "      <td>17778</td>\n",
       "      <td>1220</td>\n",
       "      <td>de4c57ee2</td>\n",
       "      <td>2bb3c3e5c</td>\n",
       "      <td>b574c9841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nom_6</td>\n",
       "      <td>object</td>\n",
       "      <td>18131</td>\n",
       "      <td>1519</td>\n",
       "      <td>a64bc7ddf</td>\n",
       "      <td>3a3a936e8</td>\n",
       "      <td>708248125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nom_7</td>\n",
       "      <td>object</td>\n",
       "      <td>18003</td>\n",
       "      <td>222</td>\n",
       "      <td>598080a91</td>\n",
       "      <td>1dddb8473</td>\n",
       "      <td>5ddc9a726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nom_8</td>\n",
       "      <td>object</td>\n",
       "      <td>17755</td>\n",
       "      <td>222</td>\n",
       "      <td>0256c7a4b</td>\n",
       "      <td>52ead350c</td>\n",
       "      <td>745b909d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nom_9</td>\n",
       "      <td>object</td>\n",
       "      <td>18073</td>\n",
       "      <td>2218</td>\n",
       "      <td>02e7c8990</td>\n",
       "      <td>f37df64af</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ord_0</td>\n",
       "      <td>float64</td>\n",
       "      <td>18288</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ord_1</td>\n",
       "      <td>object</td>\n",
       "      <td>18041</td>\n",
       "      <td>5</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ord_2</td>\n",
       "      <td>object</td>\n",
       "      <td>18075</td>\n",
       "      <td>6</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Freezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ord_3</td>\n",
       "      <td>object</td>\n",
       "      <td>17916</td>\n",
       "      <td>15</td>\n",
       "      <td>c</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ord_4</td>\n",
       "      <td>object</td>\n",
       "      <td>17930</td>\n",
       "      <td>26</td>\n",
       "      <td>U</td>\n",
       "      <td>X</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ord_5</td>\n",
       "      <td>object</td>\n",
       "      <td>17713</td>\n",
       "      <td>190</td>\n",
       "      <td>Pw</td>\n",
       "      <td>pE</td>\n",
       "      <td>eN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>day</td>\n",
       "      <td>float64</td>\n",
       "      <td>17952</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>month</td>\n",
       "      <td>float64</td>\n",
       "      <td>17988</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name   dtypes  Missing  Uniques  First Value Second Value Third Value\n",
       "0    bin_0  float64    17894        2            0            1           0\n",
       "1    bin_1  float64    18003        2            0            1           1\n",
       "2    bin_2  float64    17930        2            0            0           0\n",
       "3    bin_3   object    18014        2            F            F           F\n",
       "4    bin_4   object    18047        2            N            Y           N\n",
       "5    nom_0   object    18252        3          Red          Red         Red\n",
       "6    nom_1   object    18156        6    Trapezoid         Star         NaN\n",
       "7    nom_2   object    18035        6      Hamster      Axolotl     Hamster\n",
       "8    nom_3   object    18121        6       Russia          NaN      Canada\n",
       "9    nom_4   object    18035        4      Bassoon     Theremin     Bassoon\n",
       "10   nom_5   object    17778     1220    de4c57ee2    2bb3c3e5c   b574c9841\n",
       "11   nom_6   object    18131     1519    a64bc7ddf    3a3a936e8   708248125\n",
       "12   nom_7   object    18003      222    598080a91    1dddb8473   5ddc9a726\n",
       "13   nom_8   object    17755      222    0256c7a4b    52ead350c   745b909d1\n",
       "14   nom_9   object    18073     2218    02e7c8990    f37df64af         NaN\n",
       "15   ord_0  float64    18288        3            3            3           3\n",
       "16   ord_1   object    18041        5  Contributor  Grandmaster         NaN\n",
       "17   ord_2   object    18075        6          Hot         Warm    Freezing\n",
       "18   ord_3   object    17916       15            c            e           n\n",
       "19   ord_4   object    17930       26            U            X           P\n",
       "20   ord_5   object    17713      190           Pw           pE          eN\n",
       "21     day  float64    17952        7            6            7           5\n",
       "22   month  float64    17988       12            3            7           9\n",
       "23  target    int64        0        2            0            0           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summary(df):\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name', 'dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['First Value'] = df.loc[0].values\n",
    "    summary['Second Value'] = df.loc[1].values\n",
    "    summary['Third Value'] = df.loc[2].values\n",
    "    return summary\n",
    "\n",
    "\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add number of missing values in row as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['missing_count'] = train.isnull().sum(axis=1)\n",
    "test['missing_count'] = test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing values with constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_number = -99999\n",
    "missing_string = 'MISSING_STRING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    'bin_0', 'bin_1', 'bin_2',\n",
    "    'ord_0',\n",
    "    'day', 'month'\n",
    "]\n",
    "\n",
    "string_features = [\n",
    "    'bin_3', 'bin_4',\n",
    "    'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5',\n",
    "    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(train, test, columns, value):\n",
    "    for column in columns:\n",
    "        train[column] = train[column].fillna(value)\n",
    "        test[column] = test[column].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute(train, test, numerical_features, missing_number)\n",
    "impute(train, test, string_features, missing_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split 'ord_5' preserving missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ord_5_1'] = train['ord_5'].str[0]\n",
    "train['ord_5_2'] = train['ord_5'].str[1]\n",
    "\n",
    "train.loc[train['ord_5'] == missing_string, 'ord_5_1'] = missing_string\n",
    "train.loc[train['ord_5'] == missing_string, 'ord_5_2'] = missing_string\n",
    "\n",
    "train = train.drop('ord_5', axis=1)\n",
    "\n",
    "\n",
    "test['ord_5_1'] = test['ord_5'].str[0]\n",
    "test['ord_5_2'] = test['ord_5'].str[1]\n",
    "\n",
    "test.loc[test['ord_5'] == missing_string, 'ord_5_1'] = missing_string\n",
    "test.loc[test['ord_5'] == missing_string, 'ord_5_2'] = missing_string\n",
    "\n",
    "test = test.drop('ord_5', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = [\n",
    "    'missing_count'\n",
    "]\n",
    "\n",
    "oe_features = [\n",
    "    'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4',\n",
    "    'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4',\n",
    "    'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5_1', 'ord_5_2',\n",
    "    'day', 'month'\n",
    "]\n",
    "\n",
    "ohe_features = oe_features\n",
    "\n",
    "target_features = [\n",
    "    'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target'].copy()\n",
    "x_train = train.drop('target', axis=1)\n",
    "del train\n",
    "\n",
    "x_test = test.copy()\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "simple_x_train = scaler.fit_transform(x_train[simple_features])\n",
    "simple_x_test = scaler.transform(x_test[simple_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(dtype='uint16', handle_unknown=\"ignore\")\n",
    "ohe_x_train = ohe.fit_transform(x_train[ohe_features])\n",
    "ohe_x_test = ohe.transform(x_test[ohe_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "oe_x_train = oe.fit_transform(x_train[oe_features])\n",
    "oe_x_test = oe.transform(x_test[oe_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(transformer, x_train, y_train, cv):\n",
    "    oof = pd.DataFrame(index=x_train.index, columns=x_train.columns)\n",
    "    for train_idx, valid_idx in cv.split(x_train, y_train):\n",
    "        x_train_train = x_train.loc[train_idx]\n",
    "        y_train_train = y_train.loc[train_idx]\n",
    "        x_train_valid = x_train.loc[valid_idx]\n",
    "        transformer.fit(x_train_train, y_train_train)\n",
    "        oof_part = transformer.transform(x_train_valid)\n",
    "        oof.loc[valid_idx] = oof_part\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = TargetEncoder(drop_invariant=True, smoothing=0.2)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "target_x_train = transform(target, x_train[target_features], y_train, cv).astype('float')\n",
    "\n",
    "target.fit(x_train[target_features], y_train)\n",
    "target_x_test = target.transform(x_test[target_features]).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge for Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "x_train = scipy.sparse.hstack([ohe_x_train, simple_x_train, target_x_train]).tocsr()\n",
    "x_test = scipy.sparse.hstack([ohe_x_test, simple_x_test, target_x_test]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "logit = LogisticRegression(C=0.54321, solver='lbfgs', max_iter=10000)\n",
    "rdc=RandomForestClassifier(random_state=0,n_jobs=-1)\n",
    "logit.fit(x_train, y_train)\n",
    "rdc.fit(x_train, y_train)\n",
    "y_pred_logit = logit.predict_proba(x_test)[:, 1]\n",
    "y_pred_rdc = rdc.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((oe_x_train, simple_x_train, target_x_train), axis=1)\n",
    "x_test = np.concatenate((oe_x_test, simple_x_test, target_x_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_part = oe_x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(data, categorial_part):\n",
    "    \n",
    "    inputs = []\n",
    "    \n",
    "    categorial_outputs = []\n",
    "    for idx in range(categorial_part):\n",
    "        n_unique = np.unique(data[:,idx]).shape[0]\n",
    "        n_embeddings = int(min(np.ceil(n_unique / 2), 50))\n",
    "        inp = tf.keras.layers.Input(shape=(1,))\n",
    "        inputs.append(inp)\n",
    "        x = tf.keras.layers.Embedding(n_unique + 1, n_embeddings)(inp)\n",
    "        x = tf.keras.layers.SpatialDropout1D(0.3)(x)\n",
    "        x = tf.keras.layers.Reshape((n_embeddings,))(x)\n",
    "        categorial_outputs.append(x)\n",
    "    \n",
    "    x1 = tf.keras.layers.Concatenate()(categorial_outputs)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(data.shape[1] - categorial_part,))\n",
    "    inputs.append(inp)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(inp)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "    x = tf.keras.layers.Dense(300, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='dense_output')(x)\n",
    "    \n",
    "    print('Expected number of inputs:', len(inputs))\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), metrics=['accuracy', auc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inputs(data, categorial_part):\n",
    "    inputs = []\n",
    "    for idx in range(categorial_part):\n",
    "        inputs.append(data[:, idx])\n",
    "    inputs.append(data[:, categorial_part:])\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniss/.local/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/aniss/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Expected number of inputs: 20\n",
      "WARNING:tensorflow:From /home/aniss/.local/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5250 - acc: 0.7705 - auc: 0.5658 - val_loss: 0.4942 - val_acc: 0.8123 - val_auc: 0.6854\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4634 - acc: 0.8083 - auc: 0.6554 - val_loss: 0.4295 - val_acc: 0.8173 - val_auc: 0.7421\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4437 - acc: 0.8119 - auc: 0.7025 - val_loss: 0.4153 - val_acc: 0.8230 - val_auc: 0.7635\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4327 - acc: 0.8141 - auc: 0.7268 - val_loss: 0.4089 - val_acc: 0.8244 - val_auc: 0.7705\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4269 - acc: 0.8155 - auc: 0.7386 - val_loss: 0.4063 - val_acc: 0.8249 - val_auc: 0.7767\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4232 - acc: 0.8164 - auc: 0.7458 - val_loss: 0.4051 - val_acc: 0.8250 - val_auc: 0.7761\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4207 - acc: 0.8174 - auc: 0.7497 - val_loss: 0.4044 - val_acc: 0.8248 - val_auc: 0.7778\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4185 - acc: 0.8181 - auc: 0.7532 - val_loss: 0.4040 - val_acc: 0.8245 - val_auc: 0.7767\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4168 - acc: 0.8189 - auc: 0.7559 - val_loss: 0.4038 - val_acc: 0.8247 - val_auc: 0.7767\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4160 - acc: 0.8188 - auc: 0.7573 - val_loss: 0.4037 - val_acc: 0.8253 - val_auc: 0.7783\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8199 - auc: 0.7598 - val_loss: 0.4033 - val_acc: 0.8255 - val_auc: 0.7785\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4131 - acc: 0.8201 - auc: 0.7616 - val_loss: 0.4033 - val_acc: 0.8254 - val_auc: 0.7788\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4123 - acc: 0.8205 - auc: 0.7626 - val_loss: 0.4031 - val_acc: 0.8266 - val_auc: 0.7784\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8210 - auc: 0.7640 - val_loss: 0.4032 - val_acc: 0.8267 - val_auc: 0.7783\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8206 - auc: 0.7645 - val_loss: 0.4030 - val_acc: 0.8273 - val_auc: 0.7796\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8210 - auc: 0.7654 - val_loss: 0.4029 - val_acc: 0.8274 - val_auc: 0.7797\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8212 - auc: 0.7675 - val_loss: 0.4026 - val_acc: 0.8274 - val_auc: 0.7798\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8209 - auc: 0.7674 - val_loss: 0.4029 - val_acc: 0.8266 - val_auc: 0.7798\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8214 - auc: 0.7686 - val_loss: 0.4026 - val_acc: 0.8270 - val_auc: 0.7806\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8218 - auc: 0.7690 - val_loss: 0.4023 - val_acc: 0.8269 - val_auc: 0.7808\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8217 - auc: 0.7692 - val_loss: 0.4026 - val_acc: 0.8270 - val_auc: 0.7813\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8216 - auc: 0.7698 - val_loss: 0.4024 - val_acc: 0.8273 - val_auc: 0.7813\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8222 - auc: 0.7708 - val_loss: 0.4024 - val_acc: 0.8267 - val_auc: 0.7812\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8226 - auc: 0.7709 - val_loss: 0.4022 - val_acc: 0.8269 - val_auc: 0.7815\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8220 - auc: 0.7711 - val_loss: 0.4021 - val_acc: 0.8264 - val_auc: 0.7803\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8223 - auc: 0.7718 - val_loss: 0.4021 - val_acc: 0.8272 - val_auc: 0.7812\n",
      "Epoch 27/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8227 - auc: 0.7719\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8227 - auc: 0.7717 - val_loss: 0.4022 - val_acc: 0.8267 - val_auc: 0.7807\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4064 - acc: 0.8226 - auc: 0.7722 - val_loss: 0.4021 - val_acc: 0.8264 - val_auc: 0.7809\n",
      "Epoch 29/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8222 - auc: 0.7721Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4064 - acc: 0.8222 - auc: 0.7722 - val_loss: 0.4022 - val_acc: 0.8270 - val_auc: 0.7804\n",
      "Epoch 00029: early stopping\n",
      "Fold score: 0.7803197955856604\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5082 - acc: 0.7905 - auc: 0.5640 - val_loss: 0.4861 - val_acc: 0.8120 - val_auc: 0.6908\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4580 - acc: 0.8100 - auc: 0.6674 - val_loss: 0.4251 - val_acc: 0.8167 - val_auc: 0.7538\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4396 - acc: 0.8126 - auc: 0.7117 - val_loss: 0.4121 - val_acc: 0.8176 - val_auc: 0.7730\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4306 - acc: 0.8148 - auc: 0.7306 - val_loss: 0.4062 - val_acc: 0.8197 - val_auc: 0.7814\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4261 - acc: 0.8159 - auc: 0.7399 - val_loss: 0.4031 - val_acc: 0.8213 - val_auc: 0.7848\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4225 - acc: 0.8166 - auc: 0.7465 - val_loss: 0.4015 - val_acc: 0.8235 - val_auc: 0.7862\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4204 - acc: 0.8173 - auc: 0.7502 - val_loss: 0.4006 - val_acc: 0.8229 - val_auc: 0.7880\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4182 - acc: 0.8181 - auc: 0.7537 - val_loss: 0.4000 - val_acc: 0.8240 - val_auc: 0.7888\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4165 - acc: 0.8185 - auc: 0.7561 - val_loss: 0.3993 - val_acc: 0.8239 - val_auc: 0.7889\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4153 - acc: 0.8190 - auc: 0.7581 - val_loss: 0.3988 - val_acc: 0.8241 - val_auc: 0.7901\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4139 - acc: 0.8197 - auc: 0.7603 - val_loss: 0.3984 - val_acc: 0.8235 - val_auc: 0.7908\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4131 - acc: 0.8196 - auc: 0.7614 - val_loss: 0.3981 - val_acc: 0.8242 - val_auc: 0.7915\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8204 - auc: 0.7635 - val_loss: 0.3981 - val_acc: 0.8238 - val_auc: 0.7913\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8206 - auc: 0.7646 - val_loss: 0.3977 - val_acc: 0.8240 - val_auc: 0.7925\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8210 - auc: 0.7652 - val_loss: 0.3978 - val_acc: 0.8244 - val_auc: 0.7913\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8207 - auc: 0.7657 - val_loss: 0.3976 - val_acc: 0.8242 - val_auc: 0.7915\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8212 - auc: 0.7664 - val_loss: 0.3975 - val_acc: 0.8242 - val_auc: 0.7928\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7674 - val_loss: 0.3974 - val_acc: 0.8242 - val_auc: 0.7933\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8214 - auc: 0.7682 - val_loss: 0.3972 - val_acc: 0.8238 - val_auc: 0.7932\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8218 - auc: 0.7684 - val_loss: 0.3971 - val_acc: 0.8246 - val_auc: 0.7931\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7693 - val_loss: 0.3969 - val_acc: 0.8245 - val_auc: 0.7941\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8221 - auc: 0.7699 - val_loss: 0.3967 - val_acc: 0.8242 - val_auc: 0.7936\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8222 - auc: 0.7704 - val_loss: 0.3967 - val_acc: 0.8242 - val_auc: 0.7937\n",
      "Epoch 24/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8224 - auc: 0.7710\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8224 - auc: 0.7709 - val_loss: 0.3966 - val_acc: 0.8241 - val_auc: 0.7940\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8223 - auc: 0.7711 - val_loss: 0.3965 - val_acc: 0.8248 - val_auc: 0.7941\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8224 - auc: 0.7712 - val_loss: 0.3966 - val_acc: 0.8246 - val_auc: 0.7938\n",
      "Epoch 27/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8221 - auc: 0.7717\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8221 - auc: 0.7716 - val_loss: 0.3964 - val_acc: 0.8247 - val_auc: 0.7935\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4067 - acc: 0.8223 - auc: 0.7716 - val_loss: 0.3965 - val_acc: 0.8247 - val_auc: 0.7948\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4069 - acc: 0.8221 - auc: 0.7712 - val_loss: 0.3964 - val_acc: 0.8250 - val_auc: 0.7941\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8224 - auc: 0.7716 - val_loss: 0.3964 - val_acc: 0.8249 - val_auc: 0.7934\n",
      "Epoch 31/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8227 - auc: 0.7718Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4065 - acc: 0.8227 - auc: 0.7717 - val_loss: 0.3964 - val_acc: 0.8247 - val_auc: 0.7942\n",
      "Epoch 00031: early stopping\n",
      "Fold score: 0.7934629799567516\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5967 - acc: 0.6882 - auc: 0.5498 - val_loss: 0.5175 - val_acc: 0.8097 - val_auc: 0.6790\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4733 - acc: 0.8029 - auc: 0.6462 - val_loss: 0.4308 - val_acc: 0.8135 - val_auc: 0.7565\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4449 - acc: 0.8111 - auc: 0.7009 - val_loss: 0.4132 - val_acc: 0.8191 - val_auc: 0.7762\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4328 - acc: 0.8139 - auc: 0.7265 - val_loss: 0.4061 - val_acc: 0.8203 - val_auc: 0.7839\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4279 - acc: 0.8146 - auc: 0.7376 - val_loss: 0.4032 - val_acc: 0.8215 - val_auc: 0.7866\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4248 - acc: 0.8157 - auc: 0.7433 - val_loss: 0.4019 - val_acc: 0.8216 - val_auc: 0.7882\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4215 - acc: 0.8164 - auc: 0.7490 - val_loss: 0.4010 - val_acc: 0.8214 - val_auc: 0.7888\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4196 - acc: 0.8175 - auc: 0.7518 - val_loss: 0.4005 - val_acc: 0.8217 - val_auc: 0.7901\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4182 - acc: 0.8177 - auc: 0.7541 - val_loss: 0.4002 - val_acc: 0.8217 - val_auc: 0.7908\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4165 - acc: 0.8185 - auc: 0.7567 - val_loss: 0.3999 - val_acc: 0.8214 - val_auc: 0.7921\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8192 - auc: 0.7576 - val_loss: 0.3996 - val_acc: 0.8212 - val_auc: 0.7911\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4139 - acc: 0.8195 - auc: 0.7607 - val_loss: 0.3995 - val_acc: 0.8212 - val_auc: 0.7906\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4130 - acc: 0.8196 - auc: 0.7618 - val_loss: 0.3992 - val_acc: 0.8212 - val_auc: 0.7922\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8205 - auc: 0.7629 - val_loss: 0.3991 - val_acc: 0.8212 - val_auc: 0.7924\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8207 - auc: 0.7646 - val_loss: 0.3990 - val_acc: 0.8211 - val_auc: 0.7919\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8207 - auc: 0.7652 - val_loss: 0.3988 - val_acc: 0.8211 - val_auc: 0.7928\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8210 - auc: 0.7664 - val_loss: 0.3988 - val_acc: 0.8213 - val_auc: 0.7931\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8217 - auc: 0.7675 - val_loss: 0.3988 - val_acc: 0.8216 - val_auc: 0.7934\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8216 - auc: 0.7684 - val_loss: 0.3986 - val_acc: 0.8218 - val_auc: 0.7920\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7691 - val_loss: 0.3986 - val_acc: 0.8219 - val_auc: 0.7926\n",
      "Epoch 21/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8220 - auc: 0.7691\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8219 - auc: 0.7691 - val_loss: 0.3986 - val_acc: 0.8219 - val_auc: 0.7931\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8217 - auc: 0.7699 - val_loss: 0.3985 - val_acc: 0.8221 - val_auc: 0.7935\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8215 - auc: 0.7700 - val_loss: 0.3984 - val_acc: 0.8221 - val_auc: 0.7938\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8221 - auc: 0.7703 - val_loss: 0.3985 - val_acc: 0.8221 - val_auc: 0.7940\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8219 - auc: 0.7706 - val_loss: 0.3984 - val_acc: 0.8223 - val_auc: 0.7925\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8221 - auc: 0.7711 - val_loss: 0.3985 - val_acc: 0.8227 - val_auc: 0.7934\n",
      "Epoch 27/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8223 - auc: 0.7707Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8223 - auc: 0.7707 - val_loss: 0.3984 - val_acc: 0.8226 - val_auc: 0.7936\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7927588747750127\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6195 - acc: 0.6580 - auc: 0.5654 - val_loss: 0.5273 - val_acc: 0.8148 - val_auc: 0.6571\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4798 - acc: 0.7982 - auc: 0.6434 - val_loss: 0.4341 - val_acc: 0.8175 - val_auc: 0.7273\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4481 - acc: 0.8097 - auc: 0.6962 - val_loss: 0.4159 - val_acc: 0.8218 - val_auc: 0.7545\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4364 - acc: 0.8128 - auc: 0.7206 - val_loss: 0.4081 - val_acc: 0.8241 - val_auc: 0.7652\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4295 - acc: 0.8144 - auc: 0.7350 - val_loss: 0.4049 - val_acc: 0.8248 - val_auc: 0.7695\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4262 - acc: 0.8152 - auc: 0.7413 - val_loss: 0.4035 - val_acc: 0.8253 - val_auc: 0.7718\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4226 - acc: 0.8164 - auc: 0.7473 - val_loss: 0.4026 - val_acc: 0.8242 - val_auc: 0.7729\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4206 - acc: 0.8172 - auc: 0.7506 - val_loss: 0.4021 - val_acc: 0.8248 - val_auc: 0.7748\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4188 - acc: 0.8178 - auc: 0.7529 - val_loss: 0.4018 - val_acc: 0.8255 - val_auc: 0.7745\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4167 - acc: 0.8188 - auc: 0.7565 - val_loss: 0.4013 - val_acc: 0.8257 - val_auc: 0.7750\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4156 - acc: 0.8193 - auc: 0.7581 - val_loss: 0.4012 - val_acc: 0.8256 - val_auc: 0.7754\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4144 - acc: 0.8193 - auc: 0.7599 - val_loss: 0.4009 - val_acc: 0.8253 - val_auc: 0.7754\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8198 - auc: 0.7613 - val_loss: 0.4010 - val_acc: 0.8253 - val_auc: 0.7767\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4125 - acc: 0.8201 - auc: 0.7627 - val_loss: 0.4006 - val_acc: 0.8253 - val_auc: 0.7766\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8203 - auc: 0.7634 - val_loss: 0.4005 - val_acc: 0.8257 - val_auc: 0.7767\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8208 - auc: 0.7652 - val_loss: 0.4005 - val_acc: 0.8259 - val_auc: 0.7773\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8213 - auc: 0.7662 - val_loss: 0.4002 - val_acc: 0.8259 - val_auc: 0.7779\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8212 - auc: 0.7668 - val_loss: 0.4001 - val_acc: 0.8257 - val_auc: 0.7780\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8214 - auc: 0.7682 - val_loss: 0.4000 - val_acc: 0.8257 - val_auc: 0.7775\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8215 - auc: 0.7679 - val_loss: 0.3998 - val_acc: 0.8260 - val_auc: 0.7779\n",
      "Epoch 21/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8214 - auc: 0.7690\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8215 - auc: 0.7692 - val_loss: 0.3999 - val_acc: 0.8263 - val_auc: 0.7781\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8214 - auc: 0.7697 - val_loss: 0.3998 - val_acc: 0.8263 - val_auc: 0.7777\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8214 - auc: 0.7698 - val_loss: 0.3999 - val_acc: 0.8264 - val_auc: 0.7780\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8218 - auc: 0.7697 - val_loss: 0.3998 - val_acc: 0.8260 - val_auc: 0.7784\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8221 - auc: 0.7705 - val_loss: 0.3998 - val_acc: 0.8260 - val_auc: 0.7783\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8222 - auc: 0.7707 - val_loss: 0.3997 - val_acc: 0.8259 - val_auc: 0.7771\n",
      "Epoch 27/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8219 - auc: 0.7707Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8219 - auc: 0.7705 - val_loss: 0.3997 - val_acc: 0.8261 - val_auc: 0.7780\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7772195856585831\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5832 - acc: 0.7036 - auc: 0.5509 - val_loss: 0.5080 - val_acc: 0.8111 - val_auc: 0.6775\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4737 - acc: 0.8018 - auc: 0.6437 - val_loss: 0.4310 - val_acc: 0.8151 - val_auc: 0.7449\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4487 - acc: 0.8096 - auc: 0.6943 - val_loss: 0.4165 - val_acc: 0.8191 - val_auc: 0.7635\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4368 - acc: 0.8122 - auc: 0.7205 - val_loss: 0.4106 - val_acc: 0.8212 - val_auc: 0.7702\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4312 - acc: 0.8137 - auc: 0.7323 - val_loss: 0.4082 - val_acc: 0.8204 - val_auc: 0.7726\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4272 - acc: 0.8148 - auc: 0.7398 - val_loss: 0.4069 - val_acc: 0.8212 - val_auc: 0.7743\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4240 - acc: 0.8158 - auc: 0.7455 - val_loss: 0.4062 - val_acc: 0.8220 - val_auc: 0.7756\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4218 - acc: 0.8167 - auc: 0.7487 - val_loss: 0.4057 - val_acc: 0.8220 - val_auc: 0.7777\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4190 - acc: 0.8178 - auc: 0.7527 - val_loss: 0.4052 - val_acc: 0.8216 - val_auc: 0.7769\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4180 - acc: 0.8180 - auc: 0.7542 - val_loss: 0.4050 - val_acc: 0.8210 - val_auc: 0.7782\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4161 - acc: 0.8188 - auc: 0.7573 - val_loss: 0.4047 - val_acc: 0.8208 - val_auc: 0.7778\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4145 - acc: 0.8195 - auc: 0.7594 - val_loss: 0.4047 - val_acc: 0.8214 - val_auc: 0.7782\n",
      "Epoch 13/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8195 - auc: 0.7602\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4139 - acc: 0.8196 - auc: 0.7603 - val_loss: 0.4044 - val_acc: 0.8219 - val_auc: 0.7773\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8200 - auc: 0.7625 - val_loss: 0.4044 - val_acc: 0.8217 - val_auc: 0.7787\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4128 - acc: 0.8200 - auc: 0.7621 - val_loss: 0.4043 - val_acc: 0.8218 - val_auc: 0.7795\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8203 - auc: 0.7634 - val_loss: 0.4043 - val_acc: 0.8217 - val_auc: 0.7788\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8205 - auc: 0.7635 - val_loss: 0.4042 - val_acc: 0.8215 - val_auc: 0.7791\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8206 - auc: 0.7640 - val_loss: 0.4042 - val_acc: 0.8217 - val_auc: 0.7797\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4111 - acc: 0.8211 - auc: 0.7646 - val_loss: 0.4041 - val_acc: 0.8216 - val_auc: 0.7790\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8210 - auc: 0.7658 - val_loss: 0.4041 - val_acc: 0.8217 - val_auc: 0.7792\n",
      "Epoch 21/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8213 - auc: 0.7667\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8212 - auc: 0.7666 - val_loss: 0.4038 - val_acc: 0.8225 - val_auc: 0.7794\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8213 - auc: 0.7668 - val_loss: 0.4039 - val_acc: 0.8221 - val_auc: 0.7796\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7671 - val_loss: 0.4039 - val_acc: 0.8220 - val_auc: 0.7790\n",
      "Epoch 24/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8210 - auc: 0.7665\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8211 - auc: 0.7665 - val_loss: 0.4039 - val_acc: 0.8219 - val_auc: 0.7785\n",
      "Epoch 25/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8211 - auc: 0.7666Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8211 - auc: 0.7665 - val_loss: 0.4039 - val_acc: 0.8221 - val_auc: 0.7804\n",
      "Epoch 00025: early stopping\n",
      "Fold score: 0.7788756607815566\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.7404 - acc: 0.5218 - auc: 0.5462 - val_loss: 0.5633 - val_acc: 0.8004 - val_auc: 0.6365\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.5078 - acc: 0.7776 - auc: 0.6236 - val_loss: 0.4445 - val_acc: 0.8149 - val_auc: 0.7236\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4566 - acc: 0.8069 - auc: 0.6815 - val_loss: 0.4210 - val_acc: 0.8173 - val_auc: 0.7575\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4396 - acc: 0.8114 - auc: 0.7145 - val_loss: 0.4114 - val_acc: 0.8193 - val_auc: 0.7695\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4319 - acc: 0.8131 - auc: 0.7306 - val_loss: 0.4069 - val_acc: 0.8225 - val_auc: 0.7752\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4266 - acc: 0.8149 - auc: 0.7408 - val_loss: 0.4048 - val_acc: 0.8225 - val_auc: 0.7785\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4240 - acc: 0.8160 - auc: 0.7454 - val_loss: 0.4036 - val_acc: 0.8232 - val_auc: 0.7791\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4212 - acc: 0.8167 - auc: 0.7499 - val_loss: 0.4029 - val_acc: 0.8235 - val_auc: 0.7799\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4195 - acc: 0.8170 - auc: 0.7529 - val_loss: 0.4025 - val_acc: 0.8241 - val_auc: 0.7804\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4173 - acc: 0.8186 - auc: 0.7555 - val_loss: 0.4021 - val_acc: 0.8240 - val_auc: 0.7823\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4158 - acc: 0.8189 - auc: 0.7579 - val_loss: 0.4019 - val_acc: 0.8246 - val_auc: 0.7810\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4148 - acc: 0.8196 - auc: 0.7592 - val_loss: 0.4017 - val_acc: 0.8243 - val_auc: 0.7823\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4137 - acc: 0.8197 - auc: 0.7609 - val_loss: 0.4015 - val_acc: 0.8238 - val_auc: 0.7839\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8202 - auc: 0.7626 - val_loss: 0.4014 - val_acc: 0.8245 - val_auc: 0.7835\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8207 - auc: 0.7637 - val_loss: 0.4013 - val_acc: 0.8244 - val_auc: 0.7842\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8207 - auc: 0.7652 - val_loss: 0.4012 - val_acc: 0.8242 - val_auc: 0.7829\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8213 - auc: 0.7659 - val_loss: 0.4011 - val_acc: 0.8245 - val_auc: 0.7834\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8211 - auc: 0.7667 - val_loss: 0.4010 - val_acc: 0.8246 - val_auc: 0.7851\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7673 - val_loss: 0.4011 - val_acc: 0.8246 - val_auc: 0.7840\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8215 - auc: 0.7678 - val_loss: 0.4009 - val_acc: 0.8248 - val_auc: 0.7843\n",
      "Epoch 21/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8217 - auc: 0.7689\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8217 - auc: 0.7688 - val_loss: 0.4009 - val_acc: 0.8242 - val_auc: 0.7844\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8218 - auc: 0.7699 - val_loss: 0.4008 - val_acc: 0.8241 - val_auc: 0.7850\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8218 - auc: 0.7700 - val_loss: 0.4008 - val_acc: 0.8245 - val_auc: 0.7841\n",
      "Epoch 24/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8219 - auc: 0.7705\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7704 - val_loss: 0.4007 - val_acc: 0.8247 - val_auc: 0.7845\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8218 - auc: 0.7705 - val_loss: 0.4008 - val_acc: 0.8249 - val_auc: 0.7849\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8220 - auc: 0.7698 - val_loss: 0.4008 - val_acc: 0.8250 - val_auc: 0.7839\n",
      "Epoch 27/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8225 - auc: 0.7700\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8224 - auc: 0.7700 - val_loss: 0.4007 - val_acc: 0.8247 - val_auc: 0.7841\n",
      "Epoch 28/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8221 - auc: 0.7703Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8220 - auc: 0.7703 - val_loss: 0.4007 - val_acc: 0.8247 - val_auc: 0.7849\n",
      "Epoch 00028: early stopping\n",
      "Fold score: 0.783791276486694\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6381 - acc: 0.6374 - auc: 0.5529 - val_loss: 0.5343 - val_acc: 0.8114 - val_auc: 0.6558\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4852 - acc: 0.7964 - auc: 0.6345 - val_loss: 0.4398 - val_acc: 0.8133 - val_auc: 0.7322\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4515 - acc: 0.8088 - auc: 0.6883 - val_loss: 0.4217 - val_acc: 0.8170 - val_auc: 0.7578\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4375 - acc: 0.8124 - auc: 0.7176 - val_loss: 0.4124 - val_acc: 0.8203 - val_auc: 0.7700\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4294 - acc: 0.8142 - auc: 0.7343 - val_loss: 0.4082 - val_acc: 0.8223 - val_auc: 0.7746\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4251 - acc: 0.8157 - auc: 0.7426 - val_loss: 0.4065 - val_acc: 0.8219 - val_auc: 0.7773\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4218 - acc: 0.8165 - auc: 0.7487 - val_loss: 0.4055 - val_acc: 0.8225 - val_auc: 0.7793\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4196 - acc: 0.8177 - auc: 0.7520 - val_loss: 0.4050 - val_acc: 0.8229 - val_auc: 0.7802\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4182 - acc: 0.8181 - auc: 0.7542 - val_loss: 0.4045 - val_acc: 0.8226 - val_auc: 0.7804\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4166 - acc: 0.8187 - auc: 0.7564 - val_loss: 0.4041 - val_acc: 0.8221 - val_auc: 0.7808\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4151 - acc: 0.8192 - auc: 0.7587 - val_loss: 0.4040 - val_acc: 0.8227 - val_auc: 0.7806\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4138 - acc: 0.8199 - auc: 0.7604 - val_loss: 0.4037 - val_acc: 0.8233 - val_auc: 0.7818\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4132 - acc: 0.8198 - auc: 0.7616 - val_loss: 0.4038 - val_acc: 0.8238 - val_auc: 0.7831\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4121 - acc: 0.8201 - auc: 0.7632 - val_loss: 0.4035 - val_acc: 0.8242 - val_auc: 0.7822\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8203 - auc: 0.7646 - val_loss: 0.4034 - val_acc: 0.8238 - val_auc: 0.7820\n",
      "Epoch 16/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8202 - auc: 0.7651\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8202 - auc: 0.7651 - val_loss: 0.4034 - val_acc: 0.8238 - val_auc: 0.7819\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8208 - auc: 0.7659 - val_loss: 0.4033 - val_acc: 0.8237 - val_auc: 0.7823\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8212 - auc: 0.7661 - val_loss: 0.4032 - val_acc: 0.8238 - val_auc: 0.7820\n",
      "Epoch 19/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8209 - auc: 0.7671\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8209 - auc: 0.7671 - val_loss: 0.4031 - val_acc: 0.8236 - val_auc: 0.7830\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8212 - auc: 0.7671 - val_loss: 0.4031 - val_acc: 0.8236 - val_auc: 0.7825\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8215 - auc: 0.7674 - val_loss: 0.4031 - val_acc: 0.8237 - val_auc: 0.7838\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8214 - auc: 0.7672 - val_loss: 0.4031 - val_acc: 0.8238 - val_auc: 0.7825\n",
      "Epoch 23/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8214 - auc: 0.7685Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8214 - auc: 0.7684 - val_loss: 0.4031 - val_acc: 0.8237 - val_auc: 0.7836\n",
      "Epoch 00023: early stopping\n",
      "Fold score: 0.7817325980796838\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5776 - acc: 0.7090 - auc: 0.5626 - val_loss: 0.5066 - val_acc: 0.8136 - val_auc: 0.6764\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4693 - acc: 0.8029 - auc: 0.6530 - val_loss: 0.4286 - val_acc: 0.8169 - val_auc: 0.7433\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4450 - acc: 0.8099 - auc: 0.7026 - val_loss: 0.4144 - val_acc: 0.8193 - val_auc: 0.7634\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4348 - acc: 0.8122 - auc: 0.7247 - val_loss: 0.4085 - val_acc: 0.8207 - val_auc: 0.7706\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4296 - acc: 0.8137 - auc: 0.7355 - val_loss: 0.4057 - val_acc: 0.8217 - val_auc: 0.7750\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4254 - acc: 0.8156 - auc: 0.7429 - val_loss: 0.4041 - val_acc: 0.8218 - val_auc: 0.7762\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4222 - acc: 0.8163 - auc: 0.7481 - val_loss: 0.4031 - val_acc: 0.8228 - val_auc: 0.7792\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4202 - acc: 0.8169 - auc: 0.7513 - val_loss: 0.4023 - val_acc: 0.8227 - val_auc: 0.7792\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4183 - acc: 0.8177 - auc: 0.7541 - val_loss: 0.4018 - val_acc: 0.8232 - val_auc: 0.7816\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4164 - acc: 0.8182 - auc: 0.7569 - val_loss: 0.4014 - val_acc: 0.8242 - val_auc: 0.7821\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4153 - acc: 0.8191 - auc: 0.7586 - val_loss: 0.4010 - val_acc: 0.8242 - val_auc: 0.7817\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4140 - acc: 0.8195 - auc: 0.7605 - val_loss: 0.4008 - val_acc: 0.8238 - val_auc: 0.7820\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4128 - acc: 0.8199 - auc: 0.7624 - val_loss: 0.4005 - val_acc: 0.8240 - val_auc: 0.7824\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8206 - auc: 0.7638 - val_loss: 0.4003 - val_acc: 0.8240 - val_auc: 0.7833\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8206 - auc: 0.7648 - val_loss: 0.4002 - val_acc: 0.8242 - val_auc: 0.7847\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4111 - acc: 0.8206 - auc: 0.7648 - val_loss: 0.4002 - val_acc: 0.8238 - val_auc: 0.7851\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4100 - acc: 0.8208 - auc: 0.7664 - val_loss: 0.4000 - val_acc: 0.8233 - val_auc: 0.7838\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8213 - auc: 0.7670 - val_loss: 0.3999 - val_acc: 0.8227 - val_auc: 0.7837\n",
      "Epoch 19/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8212 - auc: 0.7680\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4092 - acc: 0.8211 - auc: 0.7678 - val_loss: 0.3999 - val_acc: 0.8227 - val_auc: 0.7846\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8217 - auc: 0.7684 - val_loss: 0.3997 - val_acc: 0.8228 - val_auc: 0.7843\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8215 - auc: 0.7689 - val_loss: 0.3996 - val_acc: 0.8233 - val_auc: 0.7843\n",
      "Epoch 22/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8218 - auc: 0.7690\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8218 - auc: 0.7692 - val_loss: 0.3996 - val_acc: 0.8231 - val_auc: 0.7846\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8222 - auc: 0.7696 - val_loss: 0.3997 - val_acc: 0.8234 - val_auc: 0.7854\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8217 - auc: 0.7700 - val_loss: 0.3995 - val_acc: 0.8230 - val_auc: 0.7845\n",
      "Epoch 25/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8215 - auc: 0.7692Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8215 - auc: 0.7693 - val_loss: 0.3996 - val_acc: 0.8233 - val_auc: 0.7844\n",
      "Epoch 00025: early stopping\n",
      "Fold score: 0.7839157340585889\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5806 - acc: 0.7050 - auc: 0.5604 - val_loss: 0.5048 - val_acc: 0.8152 - val_auc: 0.6830\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4707 - acc: 0.8026 - auc: 0.6512 - val_loss: 0.4241 - val_acc: 0.8205 - val_auc: 0.7512\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4460 - acc: 0.8097 - auc: 0.7005 - val_loss: 0.4091 - val_acc: 0.8233 - val_auc: 0.7697\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4357 - acc: 0.8124 - auc: 0.7235 - val_loss: 0.4033 - val_acc: 0.8246 - val_auc: 0.7776\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4298 - acc: 0.8137 - auc: 0.7354 - val_loss: 0.4005 - val_acc: 0.8248 - val_auc: 0.7810\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4264 - acc: 0.8148 - auc: 0.7420 - val_loss: 0.3989 - val_acc: 0.8242 - val_auc: 0.7827\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4234 - acc: 0.8161 - auc: 0.7465 - val_loss: 0.3981 - val_acc: 0.8240 - val_auc: 0.7831\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4208 - acc: 0.8169 - auc: 0.7506 - val_loss: 0.3975 - val_acc: 0.8239 - val_auc: 0.7852\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4191 - acc: 0.8175 - auc: 0.7531 - val_loss: 0.3969 - val_acc: 0.8245 - val_auc: 0.7856\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4171 - acc: 0.8184 - auc: 0.7560 - val_loss: 0.3965 - val_acc: 0.8249 - val_auc: 0.7868\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8189 - auc: 0.7583 - val_loss: 0.3962 - val_acc: 0.8247 - val_auc: 0.7861\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4143 - acc: 0.8193 - auc: 0.7600 - val_loss: 0.3958 - val_acc: 0.8257 - val_auc: 0.7883\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8196 - auc: 0.7611 - val_loss: 0.3958 - val_acc: 0.8253 - val_auc: 0.7878\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8200 - auc: 0.7627 - val_loss: 0.3956 - val_acc: 0.8259 - val_auc: 0.7891\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8204 - auc: 0.7644 - val_loss: 0.3955 - val_acc: 0.8256 - val_auc: 0.7884\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8207 - auc: 0.7652 - val_loss: 0.3953 - val_acc: 0.8266 - val_auc: 0.7881\n",
      "Epoch 17/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8210 - auc: 0.7660\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8210 - auc: 0.7660 - val_loss: 0.3952 - val_acc: 0.8260 - val_auc: 0.7881\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8212 - auc: 0.7672 - val_loss: 0.3951 - val_acc: 0.8262 - val_auc: 0.7892\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8209 - auc: 0.7676 - val_loss: 0.3950 - val_acc: 0.8264 - val_auc: 0.7896\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8215 - auc: 0.7679 - val_loss: 0.3950 - val_acc: 0.8264 - val_auc: 0.7896\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8215 - auc: 0.7687 - val_loss: 0.3949 - val_acc: 0.8263 - val_auc: 0.7896\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8216 - auc: 0.7687 - val_loss: 0.3949 - val_acc: 0.8267 - val_auc: 0.7900\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8219 - auc: 0.7691 - val_loss: 0.3949 - val_acc: 0.8264 - val_auc: 0.7895\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8213 - auc: 0.7690 - val_loss: 0.3948 - val_acc: 0.8262 - val_auc: 0.7899\n",
      "Epoch 25/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8221 - auc: 0.7700\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8220 - auc: 0.7701 - val_loss: 0.3948 - val_acc: 0.8263 - val_auc: 0.7892\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8217 - auc: 0.7693 - val_loss: 0.3948 - val_acc: 0.8262 - val_auc: 0.7897\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8217 - auc: 0.7702 - val_loss: 0.3947 - val_acc: 0.8264 - val_auc: 0.7892\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8221 - auc: 0.7701 - val_loss: 0.3948 - val_acc: 0.8265 - val_auc: 0.7909\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8221 - auc: 0.7699 - val_loss: 0.3948 - val_acc: 0.8264 - val_auc: 0.7902\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8221 - auc: 0.7707 - val_loss: 0.3946 - val_acc: 0.8263 - val_auc: 0.7891\n",
      "Epoch 31/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8227 - auc: 0.7710\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8227 - auc: 0.7709 - val_loss: 0.3946 - val_acc: 0.8265 - val_auc: 0.7898\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8223 - auc: 0.7710 - val_loss: 0.3946 - val_acc: 0.8263 - val_auc: 0.7891\n",
      "Epoch 33/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8218 - auc: 0.7701 - val_loss: 0.3947 - val_acc: 0.8265 - val_auc: 0.7901\n",
      "Epoch 34/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8217 - auc: 0.7707\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8217 - auc: 0.7706 - val_loss: 0.3946 - val_acc: 0.8263 - val_auc: 0.7901\n",
      "Epoch 35/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8221 - auc: 0.7702 - val_loss: 0.3946 - val_acc: 0.8263 - val_auc: 0.7893\n",
      "Epoch 36/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8217 - auc: 0.7707 - val_loss: 0.3946 - val_acc: 0.8263 - val_auc: 0.7895\n",
      "Epoch 37/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8221 - auc: 0.7708\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8221 - auc: 0.7709 - val_loss: 0.3947 - val_acc: 0.8263 - val_auc: 0.7898\n",
      "Epoch 38/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8218 - auc: 0.7702Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8218 - auc: 0.7702 - val_loss: 0.3946 - val_acc: 0.8263 - val_auc: 0.7900\n",
      "Epoch 00038: early stopping\n",
      "Fold score: 0.7895926664559327\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5594 - acc: 0.7351 - auc: 0.5502 - val_loss: 0.5129 - val_acc: 0.8070 - val_auc: 0.6737\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4668 - acc: 0.8074 - auc: 0.6509 - val_loss: 0.4370 - val_acc: 0.8103 - val_auc: 0.7436\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4421 - acc: 0.8123 - auc: 0.7050 - val_loss: 0.4212 - val_acc: 0.8142 - val_auc: 0.7655\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4318 - acc: 0.8145 - auc: 0.7272 - val_loss: 0.4139 - val_acc: 0.8166 - val_auc: 0.7737\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4259 - acc: 0.8159 - auc: 0.7396 - val_loss: 0.4103 - val_acc: 0.8193 - val_auc: 0.7784\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4222 - acc: 0.8166 - auc: 0.7464 - val_loss: 0.4087 - val_acc: 0.8199 - val_auc: 0.7806\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4195 - acc: 0.8176 - auc: 0.7514 - val_loss: 0.4078 - val_acc: 0.8201 - val_auc: 0.7820\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4174 - acc: 0.8181 - auc: 0.7548 - val_loss: 0.4073 - val_acc: 0.8201 - val_auc: 0.7822\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4161 - acc: 0.8188 - auc: 0.7570 - val_loss: 0.4069 - val_acc: 0.8198 - val_auc: 0.7831\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8196 - auc: 0.7588 - val_loss: 0.4066 - val_acc: 0.8201 - val_auc: 0.7830\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8197 - auc: 0.7609 - val_loss: 0.4064 - val_acc: 0.8204 - val_auc: 0.7834\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8198 - auc: 0.7631 - val_loss: 0.4064 - val_acc: 0.8202 - val_auc: 0.7841\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8206 - auc: 0.7642 - val_loss: 0.4062 - val_acc: 0.8202 - val_auc: 0.7840\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8208 - auc: 0.7648 - val_loss: 0.4061 - val_acc: 0.8207 - val_auc: 0.7850\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8212 - auc: 0.7661 - val_loss: 0.4059 - val_acc: 0.8202 - val_auc: 0.7848\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8209 - auc: 0.7669 - val_loss: 0.4059 - val_acc: 0.8202 - val_auc: 0.7842\n",
      "Epoch 17/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8213 - auc: 0.7675\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8213 - auc: 0.7675 - val_loss: 0.4057 - val_acc: 0.8210 - val_auc: 0.7845\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8214 - auc: 0.7677 - val_loss: 0.4058 - val_acc: 0.8207 - val_auc: 0.7847\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8218 - auc: 0.7682 - val_loss: 0.4058 - val_acc: 0.8208 - val_auc: 0.7853\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8220 - auc: 0.7690 - val_loss: 0.4057 - val_acc: 0.8206 - val_auc: 0.7856\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8214 - auc: 0.7687 - val_loss: 0.4058 - val_acc: 0.8205 - val_auc: 0.7841\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8217 - auc: 0.7695 - val_loss: 0.4056 - val_acc: 0.8200 - val_auc: 0.7856\n",
      "Epoch 23/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8220 - auc: 0.7693\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8220 - auc: 0.7694 - val_loss: 0.4056 - val_acc: 0.8201 - val_auc: 0.7850\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8221 - auc: 0.7700 - val_loss: 0.4056 - val_acc: 0.8201 - val_auc: 0.7844\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8218 - auc: 0.7694 - val_loss: 0.4056 - val_acc: 0.8201 - val_auc: 0.7851\n",
      "Epoch 26/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8223 - auc: 0.7702\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8223 - auc: 0.7702 - val_loss: 0.4055 - val_acc: 0.8202 - val_auc: 0.7848\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8218 - auc: 0.7705 - val_loss: 0.4056 - val_acc: 0.8204 - val_auc: 0.7853\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8222 - auc: 0.7701 - val_loss: 0.4056 - val_acc: 0.8204 - val_auc: 0.7847\n",
      "Epoch 29/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8220 - auc: 0.7694Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8220 - auc: 0.7696 - val_loss: 0.4056 - val_acc: 0.8206 - val_auc: 0.7846\n",
      "Epoch 00029: early stopping\n",
      "Fold score: 0.7846614461431792\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5992 - acc: 0.6834 - auc: 0.5571 - val_loss: 0.5144 - val_acc: 0.8146 - val_auc: 0.6800\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4754 - acc: 0.8008 - auc: 0.6444 - val_loss: 0.4272 - val_acc: 0.8189 - val_auc: 0.7488\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4461 - acc: 0.8105 - auc: 0.6991 - val_loss: 0.4094 - val_acc: 0.8230 - val_auc: 0.7719\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4349 - acc: 0.8132 - auc: 0.7234 - val_loss: 0.4022 - val_acc: 0.8249 - val_auc: 0.7811\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4290 - acc: 0.8144 - auc: 0.7359 - val_loss: 0.3993 - val_acc: 0.8267 - val_auc: 0.7839\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4253 - acc: 0.8158 - auc: 0.7429 - val_loss: 0.3980 - val_acc: 0.8272 - val_auc: 0.7848\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4224 - acc: 0.8165 - auc: 0.7479 - val_loss: 0.3972 - val_acc: 0.8274 - val_auc: 0.7863\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4201 - acc: 0.8172 - auc: 0.7515 - val_loss: 0.3970 - val_acc: 0.8272 - val_auc: 0.7863\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4180 - acc: 0.8180 - auc: 0.7548 - val_loss: 0.3966 - val_acc: 0.8277 - val_auc: 0.7866\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4167 - acc: 0.8185 - auc: 0.7566 - val_loss: 0.3965 - val_acc: 0.8281 - val_auc: 0.7871\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4156 - acc: 0.8190 - auc: 0.7582 - val_loss: 0.3965 - val_acc: 0.8275 - val_auc: 0.7870\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4140 - acc: 0.8195 - auc: 0.7606 - val_loss: 0.3961 - val_acc: 0.8273 - val_auc: 0.7869\n",
      "Epoch 13/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8196 - auc: 0.7618\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4131 - acc: 0.8196 - auc: 0.7618 - val_loss: 0.3962 - val_acc: 0.8282 - val_auc: 0.7867\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8201 - auc: 0.7628 - val_loss: 0.3961 - val_acc: 0.8279 - val_auc: 0.7887\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8202 - auc: 0.7639 - val_loss: 0.3962 - val_acc: 0.8283 - val_auc: 0.7870\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4117 - acc: 0.8204 - auc: 0.7641 - val_loss: 0.3960 - val_acc: 0.8284 - val_auc: 0.7879\n",
      "Epoch 17/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8207 - auc: 0.7652\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8208 - auc: 0.7651 - val_loss: 0.3961 - val_acc: 0.8277 - val_auc: 0.7880\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8209 - auc: 0.7651 - val_loss: 0.3961 - val_acc: 0.8278 - val_auc: 0.7879\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8205 - auc: 0.7659 - val_loss: 0.3961 - val_acc: 0.8278 - val_auc: 0.7882\n",
      "Epoch 20/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8208 - auc: 0.7658\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8209 - auc: 0.7660 - val_loss: 0.3961 - val_acc: 0.8275 - val_auc: 0.7874\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8211 - auc: 0.7661 - val_loss: 0.3961 - val_acc: 0.8277 - val_auc: 0.7868\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8207 - auc: 0.7660 - val_loss: 0.3960 - val_acc: 0.8276 - val_auc: 0.7883\n",
      "Epoch 23/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8207 - auc: 0.7666\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8207 - auc: 0.7666 - val_loss: 0.3960 - val_acc: 0.8274 - val_auc: 0.7881\n",
      "Epoch 24/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8209 - auc: 0.7660Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8209 - auc: 0.7660 - val_loss: 0.3960 - val_acc: 0.8275 - val_auc: 0.7881\n",
      "Epoch 00024: early stopping\n",
      "Fold score: 0.7875458381627279\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5625 - acc: 0.7262 - auc: 0.5614 - val_loss: 0.5014 - val_acc: 0.8140 - val_auc: 0.6925\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4677 - acc: 0.8040 - auc: 0.6550 - val_loss: 0.4250 - val_acc: 0.8155 - val_auc: 0.7542\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4448 - acc: 0.8108 - auc: 0.7025 - val_loss: 0.4114 - val_acc: 0.8189 - val_auc: 0.7723\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4347 - acc: 0.8129 - auc: 0.7244 - val_loss: 0.4057 - val_acc: 0.8198 - val_auc: 0.7795\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4290 - acc: 0.8146 - auc: 0.7360 - val_loss: 0.4034 - val_acc: 0.8211 - val_auc: 0.7827\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4255 - acc: 0.8158 - auc: 0.7422 - val_loss: 0.4023 - val_acc: 0.8211 - val_auc: 0.7834\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4227 - acc: 0.8163 - auc: 0.7473 - val_loss: 0.4017 - val_acc: 0.8212 - val_auc: 0.7846\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4202 - acc: 0.8176 - auc: 0.7510 - val_loss: 0.4013 - val_acc: 0.8207 - val_auc: 0.7844\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4186 - acc: 0.8179 - auc: 0.7534 - val_loss: 0.4011 - val_acc: 0.8213 - val_auc: 0.7857\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4164 - acc: 0.8187 - auc: 0.7569 - val_loss: 0.4010 - val_acc: 0.8214 - val_auc: 0.7854\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4154 - acc: 0.8192 - auc: 0.7582 - val_loss: 0.4006 - val_acc: 0.8221 - val_auc: 0.7859\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4143 - acc: 0.8197 - auc: 0.7597 - val_loss: 0.4006 - val_acc: 0.8214 - val_auc: 0.7857\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4134 - acc: 0.8199 - auc: 0.7610 - val_loss: 0.4006 - val_acc: 0.8207 - val_auc: 0.7854\n",
      "Epoch 14/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8201 - auc: 0.7627\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8201 - auc: 0.7631 - val_loss: 0.4007 - val_acc: 0.8208 - val_auc: 0.7852\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8207 - auc: 0.7640 - val_loss: 0.4005 - val_acc: 0.8208 - val_auc: 0.7863\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4111 - acc: 0.8207 - auc: 0.7648 - val_loss: 0.4005 - val_acc: 0.8208 - val_auc: 0.7852\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8206 - auc: 0.7652 - val_loss: 0.4004 - val_acc: 0.8212 - val_auc: 0.7863\n",
      "Epoch 18/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8207 - auc: 0.7649\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8207 - auc: 0.7649 - val_loss: 0.4004 - val_acc: 0.8207 - val_auc: 0.7858\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8209 - auc: 0.7657 - val_loss: 0.4003 - val_acc: 0.8213 - val_auc: 0.7877\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8208 - auc: 0.7666 - val_loss: 0.4003 - val_acc: 0.8210 - val_auc: 0.7868\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8213 - auc: 0.7665 - val_loss: 0.4003 - val_acc: 0.8210 - val_auc: 0.7870\n",
      "Epoch 22/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8210 - auc: 0.7660\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8209 - auc: 0.7660 - val_loss: 0.4003 - val_acc: 0.8208 - val_auc: 0.7867\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8208 - auc: 0.7666 - val_loss: 0.4003 - val_acc: 0.8209 - val_auc: 0.7864\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8211 - auc: 0.7670 - val_loss: 0.4002 - val_acc: 0.8214 - val_auc: 0.7864\n",
      "Epoch 25/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8212 - auc: 0.7671\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8212 - auc: 0.7670 - val_loss: 0.4003 - val_acc: 0.8211 - val_auc: 0.7869\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8215 - auc: 0.7664 - val_loss: 0.4003 - val_acc: 0.8212 - val_auc: 0.7861\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8216 - auc: 0.7669 - val_loss: 0.4003 - val_acc: 0.8212 - val_auc: 0.7868\n",
      "Epoch 28/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8210 - auc: 0.7668\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8210 - auc: 0.7669 - val_loss: 0.4003 - val_acc: 0.8213 - val_auc: 0.7872\n",
      "Epoch 29/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8213 - auc: 0.7673Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7675 - val_loss: 0.4003 - val_acc: 0.8212 - val_auc: 0.7863\n",
      "Epoch 00029: early stopping\n",
      "Fold score: 0.7865270551693584\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.7031 - acc: 0.5616 - auc: 0.5344 - val_loss: 0.5482 - val_acc: 0.8048 - val_auc: 0.6380\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.5002 - acc: 0.7849 - auc: 0.6227 - val_loss: 0.4332 - val_acc: 0.8180 - val_auc: 0.7395\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4547 - acc: 0.8070 - auc: 0.6853 - val_loss: 0.4092 - val_acc: 0.8238 - val_auc: 0.7717\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4391 - acc: 0.8117 - auc: 0.7158 - val_loss: 0.3994 - val_acc: 0.8277 - val_auc: 0.7814\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4318 - acc: 0.8136 - auc: 0.7315 - val_loss: 0.3951 - val_acc: 0.8290 - val_auc: 0.7870\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4276 - acc: 0.8141 - auc: 0.7398 - val_loss: 0.3933 - val_acc: 0.8289 - val_auc: 0.7887\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4246 - acc: 0.8157 - auc: 0.7446 - val_loss: 0.3921 - val_acc: 0.8298 - val_auc: 0.7906\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4215 - acc: 0.8169 - auc: 0.7497 - val_loss: 0.3914 - val_acc: 0.8303 - val_auc: 0.7901\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4204 - acc: 0.8167 - auc: 0.7514 - val_loss: 0.3911 - val_acc: 0.8302 - val_auc: 0.7920\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4184 - acc: 0.8180 - auc: 0.7541 - val_loss: 0.3907 - val_acc: 0.8298 - val_auc: 0.7923\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4166 - acc: 0.8187 - auc: 0.7565 - val_loss: 0.3904 - val_acc: 0.8299 - val_auc: 0.7923\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4154 - acc: 0.8188 - auc: 0.7585 - val_loss: 0.3902 - val_acc: 0.8305 - val_auc: 0.7933\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8195 - auc: 0.7603 - val_loss: 0.3900 - val_acc: 0.8302 - val_auc: 0.7932\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4132 - acc: 0.8195 - auc: 0.7622 - val_loss: 0.3900 - val_acc: 0.8299 - val_auc: 0.7943\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8204 - auc: 0.7632 - val_loss: 0.3898 - val_acc: 0.8301 - val_auc: 0.7938\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8202 - auc: 0.7640 - val_loss: 0.3898 - val_acc: 0.8300 - val_auc: 0.7946\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8206 - auc: 0.7652 - val_loss: 0.3896 - val_acc: 0.8298 - val_auc: 0.7957\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8212 - auc: 0.7663 - val_loss: 0.3895 - val_acc: 0.8303 - val_auc: 0.7940\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7674 - val_loss: 0.3895 - val_acc: 0.8300 - val_auc: 0.7940\n",
      "Epoch 20/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8210 - auc: 0.7673\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7673 - val_loss: 0.3894 - val_acc: 0.8301 - val_auc: 0.7947\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8215 - auc: 0.7688 - val_loss: 0.3894 - val_acc: 0.8302 - val_auc: 0.7956\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8213 - auc: 0.7690 - val_loss: 0.3893 - val_acc: 0.8300 - val_auc: 0.7959\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8214 - auc: 0.7695 - val_loss: 0.3893 - val_acc: 0.8301 - val_auc: 0.7948\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8216 - auc: 0.7693 - val_loss: 0.3892 - val_acc: 0.8304 - val_auc: 0.7953\n",
      "Epoch 25/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8216 - auc: 0.7693\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8216 - auc: 0.7694 - val_loss: 0.3892 - val_acc: 0.8303 - val_auc: 0.7955\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7695 - val_loss: 0.3892 - val_acc: 0.8301 - val_auc: 0.7954\n",
      "Epoch 27/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8216 - auc: 0.7697Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8216 - auc: 0.7697 - val_loss: 0.3892 - val_acc: 0.8302 - val_auc: 0.7948\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7944481645238823\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.5401 - acc: 0.7550 - auc: 0.5648 - val_loss: 0.5034 - val_acc: 0.8095 - val_auc: 0.6887\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4628 - acc: 0.8087 - auc: 0.6581 - val_loss: 0.4309 - val_acc: 0.8133 - val_auc: 0.7520\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4428 - acc: 0.8115 - auc: 0.7047 - val_loss: 0.4171 - val_acc: 0.8200 - val_auc: 0.7679\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4329 - acc: 0.8141 - auc: 0.7262 - val_loss: 0.4108 - val_acc: 0.8227 - val_auc: 0.7760\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4279 - acc: 0.8148 - auc: 0.7368 - val_loss: 0.4083 - val_acc: 0.8238 - val_auc: 0.7765\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4241 - acc: 0.8160 - auc: 0.7441 - val_loss: 0.4070 - val_acc: 0.8232 - val_auc: 0.7792\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4212 - acc: 0.8169 - auc: 0.7489 - val_loss: 0.4063 - val_acc: 0.8235 - val_auc: 0.7803\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4185 - acc: 0.8179 - auc: 0.7534 - val_loss: 0.4059 - val_acc: 0.8233 - val_auc: 0.7809\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4174 - acc: 0.8183 - auc: 0.7551 - val_loss: 0.4056 - val_acc: 0.8235 - val_auc: 0.7808\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8190 - auc: 0.7579 - val_loss: 0.4055 - val_acc: 0.8238 - val_auc: 0.7808\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4140 - acc: 0.8196 - auc: 0.7602 - val_loss: 0.4053 - val_acc: 0.8238 - val_auc: 0.7831\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8197 - auc: 0.7608 - val_loss: 0.4051 - val_acc: 0.8234 - val_auc: 0.7824\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8200 - auc: 0.7627 - val_loss: 0.4051 - val_acc: 0.8236 - val_auc: 0.7842\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4115 - acc: 0.8207 - auc: 0.7638 - val_loss: 0.4049 - val_acc: 0.8234 - val_auc: 0.7831\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8207 - auc: 0.7652 - val_loss: 0.4047 - val_acc: 0.8236 - val_auc: 0.7829\n",
      "Epoch 16/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8206 - auc: 0.7658\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8206 - auc: 0.7657 - val_loss: 0.4044 - val_acc: 0.8238 - val_auc: 0.7834\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8213 - auc: 0.7658 - val_loss: 0.4045 - val_acc: 0.8239 - val_auc: 0.7845\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8212 - auc: 0.7669 - val_loss: 0.4045 - val_acc: 0.8232 - val_auc: 0.7840\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8215 - auc: 0.7675 - val_loss: 0.4045 - val_acc: 0.8235 - val_auc: 0.7842\n",
      "Epoch 20/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8212 - auc: 0.7680\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8212 - auc: 0.7679 - val_loss: 0.4045 - val_acc: 0.8232 - val_auc: 0.7832\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8211 - auc: 0.7683 - val_loss: 0.4045 - val_acc: 0.8231 - val_auc: 0.7823\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8215 - auc: 0.7680 - val_loss: 0.4044 - val_acc: 0.8232 - val_auc: 0.7842\n",
      "Epoch 23/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8217 - auc: 0.7685Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8217 - auc: 0.7686 - val_loss: 0.4043 - val_acc: 0.8234 - val_auc: 0.7838\n",
      "Epoch 00023: early stopping\n",
      "Fold score: 0.7828394555826207\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5207 - acc: 0.7712 - auc: 0.5808 - val_loss: 0.4939 - val_acc: 0.8104 - val_auc: 0.6940\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4567 - acc: 0.8093 - auc: 0.6724 - val_loss: 0.4295 - val_acc: 0.8152 - val_auc: 0.7479\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4380 - acc: 0.8124 - auc: 0.7149 - val_loss: 0.4191 - val_acc: 0.8167 - val_auc: 0.7622\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4298 - acc: 0.8152 - auc: 0.7323 - val_loss: 0.4147 - val_acc: 0.8177 - val_auc: 0.7671\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4249 - acc: 0.8163 - auc: 0.7421 - val_loss: 0.4128 - val_acc: 0.8179 - val_auc: 0.7694\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4220 - acc: 0.8167 - auc: 0.7474 - val_loss: 0.4116 - val_acc: 0.8179 - val_auc: 0.7732\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4191 - acc: 0.8176 - auc: 0.7525 - val_loss: 0.4108 - val_acc: 0.8187 - val_auc: 0.7737\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4176 - acc: 0.8186 - auc: 0.7546 - val_loss: 0.4103 - val_acc: 0.8188 - val_auc: 0.7733\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4166 - acc: 0.8186 - auc: 0.7561 - val_loss: 0.4099 - val_acc: 0.8193 - val_auc: 0.7743\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8191 - auc: 0.7592 - val_loss: 0.4096 - val_acc: 0.8189 - val_auc: 0.7746\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8199 - auc: 0.7610 - val_loss: 0.4093 - val_acc: 0.8192 - val_auc: 0.7754\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4128 - acc: 0.8201 - auc: 0.7621 - val_loss: 0.4091 - val_acc: 0.8199 - val_auc: 0.7758\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4121 - acc: 0.8203 - auc: 0.7631 - val_loss: 0.4090 - val_acc: 0.8201 - val_auc: 0.7766\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8206 - auc: 0.7642 - val_loss: 0.4090 - val_acc: 0.8195 - val_auc: 0.7760\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8210 - auc: 0.7664 - val_loss: 0.4087 - val_acc: 0.8195 - val_auc: 0.7762\n",
      "Epoch 16/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8210 - auc: 0.7666\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8210 - auc: 0.7666 - val_loss: 0.4086 - val_acc: 0.8195 - val_auc: 0.7763\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4095 - acc: 0.8210 - auc: 0.7672 - val_loss: 0.4085 - val_acc: 0.8193 - val_auc: 0.7776\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8214 - auc: 0.7676 - val_loss: 0.4085 - val_acc: 0.8195 - val_auc: 0.7768\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8212 - auc: 0.7685 - val_loss: 0.4083 - val_acc: 0.8196 - val_auc: 0.7766\n",
      "Epoch 20/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8216 - auc: 0.7685\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8216 - auc: 0.7686 - val_loss: 0.4083 - val_acc: 0.8195 - val_auc: 0.7765\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8218 - auc: 0.7690 - val_loss: 0.4083 - val_acc: 0.8195 - val_auc: 0.7769\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8217 - auc: 0.7691 - val_loss: 0.4083 - val_acc: 0.8195 - val_auc: 0.7767\n",
      "Epoch 23/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8218 - auc: 0.7692Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7691 - val_loss: 0.4082 - val_acc: 0.8196 - val_auc: 0.7763\n",
      "Epoch 00023: early stopping\n",
      "Fold score: 0.7758567277159877\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5632 - acc: 0.7288 - auc: 0.5474 - val_loss: 0.5026 - val_acc: 0.8169 - val_auc: 0.6653\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4719 - acc: 0.8044 - auc: 0.6413 - val_loss: 0.4272 - val_acc: 0.8199 - val_auc: 0.7397\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4478 - acc: 0.8103 - auc: 0.6953 - val_loss: 0.4117 - val_acc: 0.8212 - val_auc: 0.7622\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4369 - acc: 0.8119 - auc: 0.7206 - val_loss: 0.4054 - val_acc: 0.8217 - val_auc: 0.7709\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4307 - acc: 0.8136 - auc: 0.7334 - val_loss: 0.4020 - val_acc: 0.8230 - val_auc: 0.7753\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4259 - acc: 0.8153 - auc: 0.7420 - val_loss: 0.4005 - val_acc: 0.8238 - val_auc: 0.7790\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4232 - acc: 0.8166 - auc: 0.7462 - val_loss: 0.3995 - val_acc: 0.8227 - val_auc: 0.7793\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4209 - acc: 0.8168 - auc: 0.7505 - val_loss: 0.3989 - val_acc: 0.8227 - val_auc: 0.7808\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4190 - acc: 0.8173 - auc: 0.7530 - val_loss: 0.3986 - val_acc: 0.8234 - val_auc: 0.7810\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4175 - acc: 0.8184 - auc: 0.7553 - val_loss: 0.3981 - val_acc: 0.8236 - val_auc: 0.7822\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4156 - acc: 0.8192 - auc: 0.7583 - val_loss: 0.3979 - val_acc: 0.8247 - val_auc: 0.7831\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4140 - acc: 0.8198 - auc: 0.7606 - val_loss: 0.3976 - val_acc: 0.8234 - val_auc: 0.7835\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8197 - auc: 0.7614 - val_loss: 0.3972 - val_acc: 0.8240 - val_auc: 0.7827\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4126 - acc: 0.8201 - auc: 0.7627 - val_loss: 0.3970 - val_acc: 0.8235 - val_auc: 0.7833\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8201 - auc: 0.7637 - val_loss: 0.3970 - val_acc: 0.8238 - val_auc: 0.7844\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8205 - auc: 0.7646 - val_loss: 0.3968 - val_acc: 0.8240 - val_auc: 0.7841\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8211 - auc: 0.7663 - val_loss: 0.3966 - val_acc: 0.8246 - val_auc: 0.7846\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8212 - auc: 0.7675 - val_loss: 0.3964 - val_acc: 0.8248 - val_auc: 0.7842\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8215 - auc: 0.7676 - val_loss: 0.3965 - val_acc: 0.8248 - val_auc: 0.7848\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8216 - auc: 0.7686 - val_loss: 0.3962 - val_acc: 0.8240 - val_auc: 0.7850\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8217 - auc: 0.7690 - val_loss: 0.3963 - val_acc: 0.8246 - val_auc: 0.7843\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8217 - auc: 0.7693 - val_loss: 0.3961 - val_acc: 0.8253 - val_auc: 0.7850\n",
      "Epoch 23/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8219 - auc: 0.7704\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8219 - auc: 0.7704 - val_loss: 0.3961 - val_acc: 0.8252 - val_auc: 0.7851\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8219 - auc: 0.7707 - val_loss: 0.3962 - val_acc: 0.8253 - val_auc: 0.7843\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8218 - auc: 0.7706 - val_loss: 0.3962 - val_acc: 0.8253 - val_auc: 0.7855\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8220 - auc: 0.7707 - val_loss: 0.3960 - val_acc: 0.8256 - val_auc: 0.7848\n",
      "Epoch 27/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8220 - auc: 0.7710Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8220 - auc: 0.7710 - val_loss: 0.3961 - val_acc: 0.8264 - val_auc: 0.7848\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7843890139561726\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5229 - acc: 0.7683 - auc: 0.5771 - val_loss: 0.4865 - val_acc: 0.8141 - val_auc: 0.7056\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4570 - acc: 0.8089 - auc: 0.6722 - val_loss: 0.4190 - val_acc: 0.8211 - val_auc: 0.7615\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4381 - acc: 0.8123 - auc: 0.7157 - val_loss: 0.4064 - val_acc: 0.8255 - val_auc: 0.7760\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4295 - acc: 0.8143 - auc: 0.7340 - val_loss: 0.4016 - val_acc: 0.8276 - val_auc: 0.7819\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4252 - acc: 0.8153 - auc: 0.7425 - val_loss: 0.3993 - val_acc: 0.8289 - val_auc: 0.7837\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4230 - acc: 0.8162 - auc: 0.7468 - val_loss: 0.3982 - val_acc: 0.8284 - val_auc: 0.7860\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4207 - acc: 0.8167 - auc: 0.7500 - val_loss: 0.3976 - val_acc: 0.8285 - val_auc: 0.7869\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4178 - acc: 0.8181 - auc: 0.7548 - val_loss: 0.3971 - val_acc: 0.8278 - val_auc: 0.7883\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4161 - acc: 0.8185 - auc: 0.7573 - val_loss: 0.3967 - val_acc: 0.8282 - val_auc: 0.7892\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4151 - acc: 0.8191 - auc: 0.7587 - val_loss: 0.3967 - val_acc: 0.8284 - val_auc: 0.7899\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4138 - acc: 0.8198 - auc: 0.7608 - val_loss: 0.3966 - val_acc: 0.8281 - val_auc: 0.7898\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4133 - acc: 0.8198 - auc: 0.7615 - val_loss: 0.3963 - val_acc: 0.8284 - val_auc: 0.7889\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8204 - auc: 0.7627 - val_loss: 0.3961 - val_acc: 0.8280 - val_auc: 0.7910\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8209 - auc: 0.7651 - val_loss: 0.3961 - val_acc: 0.8283 - val_auc: 0.7906\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8209 - auc: 0.7651 - val_loss: 0.3961 - val_acc: 0.8278 - val_auc: 0.7903\n",
      "Epoch 16/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8210 - auc: 0.7664\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8210 - auc: 0.7663 - val_loss: 0.3961 - val_acc: 0.8282 - val_auc: 0.7902\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8212 - auc: 0.7666 - val_loss: 0.3960 - val_acc: 0.8282 - val_auc: 0.7913\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7675 - val_loss: 0.3959 - val_acc: 0.8277 - val_auc: 0.7908\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8214 - auc: 0.7681 - val_loss: 0.3958 - val_acc: 0.8273 - val_auc: 0.7916\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8215 - auc: 0.7677 - val_loss: 0.3958 - val_acc: 0.8274 - val_auc: 0.7906\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8217 - auc: 0.7681 - val_loss: 0.3957 - val_acc: 0.8277 - val_auc: 0.7912\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8214 - auc: 0.7692 - val_loss: 0.3956 - val_acc: 0.8272 - val_auc: 0.7918\n",
      "Epoch 23/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8219 - auc: 0.7694Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8219 - auc: 0.7693 - val_loss: 0.3956 - val_acc: 0.8271 - val_auc: 0.7906\n",
      "Epoch 00023: early stopping\n",
      "Fold score: 0.7900704232785352\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5860 - acc: 0.7030 - auc: 0.5389 - val_loss: 0.5187 - val_acc: 0.8097 - val_auc: 0.6586\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4752 - acc: 0.8037 - auc: 0.6353 - val_loss: 0.4377 - val_acc: 0.8118 - val_auc: 0.7406\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4481 - acc: 0.8107 - auc: 0.6924 - val_loss: 0.4182 - val_acc: 0.8151 - val_auc: 0.7688\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4343 - acc: 0.8137 - auc: 0.7233 - val_loss: 0.4094 - val_acc: 0.8173 - val_auc: 0.7787\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4284 - acc: 0.8144 - auc: 0.7360 - val_loss: 0.4061 - val_acc: 0.8188 - val_auc: 0.7826\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4253 - acc: 0.8153 - auc: 0.7423 - val_loss: 0.4046 - val_acc: 0.8196 - val_auc: 0.7826\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4221 - acc: 0.8166 - auc: 0.7478 - val_loss: 0.4037 - val_acc: 0.8198 - val_auc: 0.7853\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4200 - acc: 0.8174 - auc: 0.7512 - val_loss: 0.4032 - val_acc: 0.8202 - val_auc: 0.7849\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4181 - acc: 0.8181 - auc: 0.7539 - val_loss: 0.4026 - val_acc: 0.8206 - val_auc: 0.7861\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4168 - acc: 0.8188 - auc: 0.7560 - val_loss: 0.4024 - val_acc: 0.8207 - val_auc: 0.7859\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4157 - acc: 0.8190 - auc: 0.7575 - val_loss: 0.4021 - val_acc: 0.8209 - val_auc: 0.7862\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4145 - acc: 0.8192 - auc: 0.7595 - val_loss: 0.4019 - val_acc: 0.8207 - val_auc: 0.7866\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4134 - acc: 0.8197 - auc: 0.7612 - val_loss: 0.4017 - val_acc: 0.8207 - val_auc: 0.7873\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8202 - auc: 0.7637 - val_loss: 0.4017 - val_acc: 0.8210 - val_auc: 0.7880\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8204 - auc: 0.7639 - val_loss: 0.4015 - val_acc: 0.8206 - val_auc: 0.7871\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8207 - auc: 0.7647 - val_loss: 0.4015 - val_acc: 0.8208 - val_auc: 0.7872\n",
      "Epoch 17/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8207 - auc: 0.7658\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8207 - auc: 0.7657 - val_loss: 0.4013 - val_acc: 0.8205 - val_auc: 0.7878\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8215 - auc: 0.7669 - val_loss: 0.4012 - val_acc: 0.8209 - val_auc: 0.7876\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8210 - auc: 0.7674 - val_loss: 0.4013 - val_acc: 0.8208 - val_auc: 0.7873\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8215 - auc: 0.7673 - val_loss: 0.4012 - val_acc: 0.8206 - val_auc: 0.7884\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8215 - auc: 0.7678 - val_loss: 0.4011 - val_acc: 0.8205 - val_auc: 0.7887\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8220 - auc: 0.7681 - val_loss: 0.4011 - val_acc: 0.8205 - val_auc: 0.7877\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8217 - auc: 0.7686 - val_loss: 0.4011 - val_acc: 0.8204 - val_auc: 0.7881\n",
      "Epoch 24/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8216 - auc: 0.7687Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8216 - auc: 0.7688 - val_loss: 0.4011 - val_acc: 0.8203 - val_auc: 0.7876\n",
      "Epoch 00024: early stopping\n",
      "Fold score: 0.7874474992780306\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6368 - acc: 0.6421 - auc: 0.5299 - val_loss: 0.5301 - val_acc: 0.8169 - val_auc: 0.6499\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4857 - acc: 0.7971 - auc: 0.6284 - val_loss: 0.4311 - val_acc: 0.8198 - val_auc: 0.7394\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4503 - acc: 0.8096 - auc: 0.6905 - val_loss: 0.4108 - val_acc: 0.8226 - val_auc: 0.7652\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4366 - acc: 0.8125 - auc: 0.7199 - val_loss: 0.4029 - val_acc: 0.8243 - val_auc: 0.7739\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4299 - acc: 0.8146 - auc: 0.7341 - val_loss: 0.3994 - val_acc: 0.8256 - val_auc: 0.7778\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4260 - acc: 0.8154 - auc: 0.7414 - val_loss: 0.3978 - val_acc: 0.8263 - val_auc: 0.7806\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4231 - acc: 0.8166 - auc: 0.7464 - val_loss: 0.3966 - val_acc: 0.8267 - val_auc: 0.7820\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4209 - acc: 0.8168 - auc: 0.7505 - val_loss: 0.3959 - val_acc: 0.8267 - val_auc: 0.7839\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4186 - acc: 0.8177 - auc: 0.7539 - val_loss: 0.3953 - val_acc: 0.8271 - val_auc: 0.7837\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4171 - acc: 0.8187 - auc: 0.7558 - val_loss: 0.3951 - val_acc: 0.8271 - val_auc: 0.7847\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4156 - acc: 0.8187 - auc: 0.7583 - val_loss: 0.3946 - val_acc: 0.8273 - val_auc: 0.7844\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8194 - auc: 0.7594 - val_loss: 0.3946 - val_acc: 0.8273 - val_auc: 0.7853\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8197 - auc: 0.7613 - val_loss: 0.3943 - val_acc: 0.8269 - val_auc: 0.7862\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8199 - auc: 0.7631 - val_loss: 0.3940 - val_acc: 0.8278 - val_auc: 0.7860\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8204 - auc: 0.7637 - val_loss: 0.3939 - val_acc: 0.8276 - val_auc: 0.7861\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8208 - auc: 0.7654 - val_loss: 0.3941 - val_acc: 0.8277 - val_auc: 0.7867\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8210 - auc: 0.7655 - val_loss: 0.3940 - val_acc: 0.8282 - val_auc: 0.7858\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8210 - auc: 0.7665 - val_loss: 0.3937 - val_acc: 0.8286 - val_auc: 0.7868\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8213 - auc: 0.7680 - val_loss: 0.3937 - val_acc: 0.8285 - val_auc: 0.7869\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8213 - auc: 0.7682 - val_loss: 0.3936 - val_acc: 0.8287 - val_auc: 0.7860\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8218 - auc: 0.7691 - val_loss: 0.3935 - val_acc: 0.8282 - val_auc: 0.7873\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8222 - auc: 0.7695 - val_loss: 0.3934 - val_acc: 0.8284 - val_auc: 0.7867\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8220 - auc: 0.7704 - val_loss: 0.3936 - val_acc: 0.8291 - val_auc: 0.7878\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8220 - auc: 0.7708 - val_loss: 0.3934 - val_acc: 0.8288 - val_auc: 0.7877\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8223 - auc: 0.7708 - val_loss: 0.3935 - val_acc: 0.8284 - val_auc: 0.7883\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4069 - acc: 0.8223 - auc: 0.7714 - val_loss: 0.3933 - val_acc: 0.8284 - val_auc: 0.7886\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8226 - auc: 0.7719 - val_loss: 0.3933 - val_acc: 0.8285 - val_auc: 0.7876\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4064 - acc: 0.8225 - auc: 0.7723 - val_loss: 0.3932 - val_acc: 0.8290 - val_auc: 0.7882\n",
      "Epoch 29/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8225 - auc: 0.7719\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8225 - auc: 0.7718 - val_loss: 0.3931 - val_acc: 0.8283 - val_auc: 0.7887\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4063 - acc: 0.8227 - auc: 0.7723 - val_loss: 0.3932 - val_acc: 0.8286 - val_auc: 0.7882\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4061 - acc: 0.8225 - auc: 0.7728 - val_loss: 0.3931 - val_acc: 0.8287 - val_auc: 0.7884\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4059 - acc: 0.8225 - auc: 0.7731 - val_loss: 0.3931 - val_acc: 0.8286 - val_auc: 0.7888\n",
      "Epoch 33/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4059 - acc: 0.8227 - auc: 0.7730 - val_loss: 0.3931 - val_acc: 0.8284 - val_auc: 0.7882\n",
      "Epoch 34/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4060 - acc: 0.8227 - auc: 0.7727 - val_loss: 0.3931 - val_acc: 0.8284 - val_auc: 0.7876\n",
      "Epoch 35/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8225 - auc: 0.7737\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4055 - acc: 0.8225 - auc: 0.7738 - val_loss: 0.3931 - val_acc: 0.8282 - val_auc: 0.7874\n",
      "Epoch 36/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4056 - acc: 0.8223 - auc: 0.7734 - val_loss: 0.3931 - val_acc: 0.8284 - val_auc: 0.7878\n",
      "Epoch 37/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4058 - acc: 0.8224 - auc: 0.7731 - val_loss: 0.3930 - val_acc: 0.8284 - val_auc: 0.7886\n",
      "Epoch 38/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8226 - auc: 0.7734\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4055 - acc: 0.8226 - auc: 0.7733 - val_loss: 0.3931 - val_acc: 0.8285 - val_auc: 0.7886\n",
      "Epoch 39/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4052 - acc: 0.8228 - auc: 0.7740 - val_loss: 0.3931 - val_acc: 0.8284 - val_auc: 0.7885\n",
      "Epoch 40/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4054 - acc: 0.8228 - auc: 0.7738 - val_loss: 0.3930 - val_acc: 0.8284 - val_auc: 0.7873\n",
      "Epoch 41/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8228 - auc: 0.7734\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4056 - acc: 0.8227 - auc: 0.7733 - val_loss: 0.3930 - val_acc: 0.8286 - val_auc: 0.7875\n",
      "Epoch 42/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8225 - auc: 0.7738Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4053 - acc: 0.8226 - auc: 0.7740 - val_loss: 0.3930 - val_acc: 0.8286 - val_auc: 0.7883\n",
      "Epoch 00042: early stopping\n",
      "Fold score: 0.7881733998144713\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5292 - acc: 0.7710 - auc: 0.5455 - val_loss: 0.4992 - val_acc: 0.8112 - val_auc: 0.6483\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4650 - acc: 0.8085 - auc: 0.6500 - val_loss: 0.4339 - val_acc: 0.8149 - val_auc: 0.7375\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4421 - acc: 0.8117 - auc: 0.7062 - val_loss: 0.4196 - val_acc: 0.8179 - val_auc: 0.7588\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4324 - acc: 0.8139 - auc: 0.7278 - val_loss: 0.4138 - val_acc: 0.8194 - val_auc: 0.7661\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4271 - acc: 0.8148 - auc: 0.7384 - val_loss: 0.4110 - val_acc: 0.8188 - val_auc: 0.7720\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4238 - acc: 0.8163 - auc: 0.7445 - val_loss: 0.4094 - val_acc: 0.8203 - val_auc: 0.7729\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4207 - acc: 0.8170 - auc: 0.7498 - val_loss: 0.4084 - val_acc: 0.8212 - val_auc: 0.7746\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4189 - acc: 0.8172 - auc: 0.7530 - val_loss: 0.4081 - val_acc: 0.8210 - val_auc: 0.7748\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4167 - acc: 0.8186 - auc: 0.7561 - val_loss: 0.4075 - val_acc: 0.8211 - val_auc: 0.7743\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4152 - acc: 0.8190 - auc: 0.7585 - val_loss: 0.4073 - val_acc: 0.8219 - val_auc: 0.7763\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8191 - auc: 0.7592 - val_loss: 0.4069 - val_acc: 0.8213 - val_auc: 0.7750\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4131 - acc: 0.8197 - auc: 0.7616 - val_loss: 0.4066 - val_acc: 0.8217 - val_auc: 0.7770\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4121 - acc: 0.8199 - auc: 0.7632 - val_loss: 0.4066 - val_acc: 0.8218 - val_auc: 0.7768\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8210 - auc: 0.7648 - val_loss: 0.4062 - val_acc: 0.8220 - val_auc: 0.7773\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8211 - auc: 0.7654 - val_loss: 0.4063 - val_acc: 0.8218 - val_auc: 0.7776\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8211 - auc: 0.7661 - val_loss: 0.4059 - val_acc: 0.8222 - val_auc: 0.7788\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8215 - auc: 0.7675 - val_loss: 0.4060 - val_acc: 0.8223 - val_auc: 0.7780\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8215 - auc: 0.7681 - val_loss: 0.4059 - val_acc: 0.8220 - val_auc: 0.7779\n",
      "Epoch 19/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8220 - auc: 0.7686\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8220 - auc: 0.7684 - val_loss: 0.4057 - val_acc: 0.8216 - val_auc: 0.7776\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8218 - auc: 0.7693 - val_loss: 0.4056 - val_acc: 0.8218 - val_auc: 0.7786\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8220 - auc: 0.7688 - val_loss: 0.4056 - val_acc: 0.8222 - val_auc: 0.7777\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8218 - auc: 0.7697 - val_loss: 0.4055 - val_acc: 0.8218 - val_auc: 0.7796\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8218 - auc: 0.7693 - val_loss: 0.4056 - val_acc: 0.8217 - val_auc: 0.7791\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8220 - auc: 0.7702 - val_loss: 0.4055 - val_acc: 0.8219 - val_auc: 0.7789\n",
      "Epoch 25/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8218 - auc: 0.7706\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8218 - auc: 0.7705 - val_loss: 0.4054 - val_acc: 0.8217 - val_auc: 0.7793\n",
      "Epoch 26/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8222 - auc: 0.7706Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4073 - acc: 0.8221 - auc: 0.7706 - val_loss: 0.4054 - val_acc: 0.8217 - val_auc: 0.7789\n",
      "Epoch 00026: early stopping\n",
      "Fold score: 0.7779730476160293\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5902 - acc: 0.6923 - auc: 0.5735 - val_loss: 0.5142 - val_acc: 0.8102 - val_auc: 0.6824\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4700 - acc: 0.8020 - auc: 0.6560 - val_loss: 0.4304 - val_acc: 0.8152 - val_auc: 0.7505\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4431 - acc: 0.8108 - auc: 0.7060 - val_loss: 0.4160 - val_acc: 0.8206 - val_auc: 0.7656\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4325 - acc: 0.8134 - auc: 0.7290 - val_loss: 0.4111 - val_acc: 0.8241 - val_auc: 0.7723\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4278 - acc: 0.8148 - auc: 0.7386 - val_loss: 0.4092 - val_acc: 0.8246 - val_auc: 0.7734\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4251 - acc: 0.8151 - auc: 0.7435 - val_loss: 0.4083 - val_acc: 0.8246 - val_auc: 0.7745\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4225 - acc: 0.8163 - auc: 0.7478 - val_loss: 0.4078 - val_acc: 0.8245 - val_auc: 0.7757\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4202 - acc: 0.8175 - auc: 0.7513 - val_loss: 0.4072 - val_acc: 0.8237 - val_auc: 0.7766\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4186 - acc: 0.8179 - auc: 0.7535 - val_loss: 0.4069 - val_acc: 0.8237 - val_auc: 0.7765\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4166 - acc: 0.8190 - auc: 0.7563 - val_loss: 0.4066 - val_acc: 0.8232 - val_auc: 0.7774\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4152 - acc: 0.8191 - auc: 0.7586 - val_loss: 0.4064 - val_acc: 0.8241 - val_auc: 0.7767\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4140 - acc: 0.8196 - auc: 0.7604 - val_loss: 0.4060 - val_acc: 0.8239 - val_auc: 0.7777\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4130 - acc: 0.8200 - auc: 0.7618 - val_loss: 0.4059 - val_acc: 0.8240 - val_auc: 0.7792\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8200 - auc: 0.7629 - val_loss: 0.4059 - val_acc: 0.8242 - val_auc: 0.7788\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8204 - auc: 0.7643 - val_loss: 0.4057 - val_acc: 0.8244 - val_auc: 0.7793\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8205 - auc: 0.7659 - val_loss: 0.4056 - val_acc: 0.8246 - val_auc: 0.7792\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8211 - auc: 0.7662 - val_loss: 0.4056 - val_acc: 0.8234 - val_auc: 0.7799\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7673 - val_loss: 0.4055 - val_acc: 0.8241 - val_auc: 0.7796\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8213 - auc: 0.7681 - val_loss: 0.4053 - val_acc: 0.8238 - val_auc: 0.7800\n",
      "Epoch 20/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8215 - auc: 0.7684\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8215 - auc: 0.7685 - val_loss: 0.4053 - val_acc: 0.8241 - val_auc: 0.7795\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8216 - auc: 0.7696 - val_loss: 0.4053 - val_acc: 0.8246 - val_auc: 0.7818\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7694 - val_loss: 0.4053 - val_acc: 0.8244 - val_auc: 0.7803\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8220 - auc: 0.7695 - val_loss: 0.4053 - val_acc: 0.8244 - val_auc: 0.7804\n",
      "Epoch 24/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8219 - auc: 0.7701\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8219 - auc: 0.7702 - val_loss: 0.4053 - val_acc: 0.8242 - val_auc: 0.7805\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8220 - auc: 0.7703 - val_loss: 0.4052 - val_acc: 0.8243 - val_auc: 0.7805\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8217 - auc: 0.7704 - val_loss: 0.4053 - val_acc: 0.8243 - val_auc: 0.7799\n",
      "Epoch 27/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8219 - auc: 0.7707\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8220 - auc: 0.7705 - val_loss: 0.4053 - val_acc: 0.8244 - val_auc: 0.7800\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8223 - auc: 0.7710 - val_loss: 0.4053 - val_acc: 0.8242 - val_auc: 0.7793\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8221 - auc: 0.7710 - val_loss: 0.4053 - val_acc: 0.8247 - val_auc: 0.7789\n",
      "Epoch 30/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8219 - auc: 0.7705\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8219 - auc: 0.7706 - val_loss: 0.4053 - val_acc: 0.8242 - val_auc: 0.7814\n",
      "Epoch 31/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8221 - auc: 0.7716Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4067 - acc: 0.8221 - auc: 0.7716 - val_loss: 0.4053 - val_acc: 0.8245 - val_auc: 0.7804\n",
      "Epoch 00031: early stopping\n",
      "Fold score: 0.7802296826598548\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.7191 - acc: 0.5483 - auc: 0.5493 - val_loss: 0.5521 - val_acc: 0.8081 - val_auc: 0.6695\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.5003 - acc: 0.7826 - auc: 0.6313 - val_loss: 0.4395 - val_acc: 0.8138 - val_auc: 0.7460\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4539 - acc: 0.8071 - auc: 0.6875 - val_loss: 0.4189 - val_acc: 0.8167 - val_auc: 0.7685\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4395 - acc: 0.8117 - auc: 0.7147 - val_loss: 0.4107 - val_acc: 0.8192 - val_auc: 0.7775\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4329 - acc: 0.8132 - auc: 0.7286 - val_loss: 0.4070 - val_acc: 0.8202 - val_auc: 0.7812\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4287 - acc: 0.8151 - auc: 0.7366 - val_loss: 0.4049 - val_acc: 0.8207 - val_auc: 0.7837\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4251 - acc: 0.8155 - auc: 0.7433 - val_loss: 0.4038 - val_acc: 0.8212 - val_auc: 0.7840\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4222 - acc: 0.8168 - auc: 0.7478 - val_loss: 0.4031 - val_acc: 0.8218 - val_auc: 0.7852\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4203 - acc: 0.8181 - auc: 0.7505 - val_loss: 0.4025 - val_acc: 0.8227 - val_auc: 0.7860\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4188 - acc: 0.8183 - auc: 0.7529 - val_loss: 0.4022 - val_acc: 0.8234 - val_auc: 0.7862\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4165 - acc: 0.8184 - auc: 0.7564 - val_loss: 0.4020 - val_acc: 0.8240 - val_auc: 0.7870\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4152 - acc: 0.8194 - auc: 0.7583 - val_loss: 0.4018 - val_acc: 0.8234 - val_auc: 0.7881\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8197 - auc: 0.7597 - val_loss: 0.4015 - val_acc: 0.8239 - val_auc: 0.7880\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4134 - acc: 0.8200 - auc: 0.7610 - val_loss: 0.4014 - val_acc: 0.8242 - val_auc: 0.7878\n",
      "Epoch 15/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8203 - auc: 0.7631\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8203 - auc: 0.7633 - val_loss: 0.4013 - val_acc: 0.8240 - val_auc: 0.7879\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8203 - auc: 0.7638 - val_loss: 0.4012 - val_acc: 0.8244 - val_auc: 0.7875\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4111 - acc: 0.8210 - auc: 0.7643 - val_loss: 0.4012 - val_acc: 0.8243 - val_auc: 0.7883\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8211 - auc: 0.7651 - val_loss: 0.4011 - val_acc: 0.8242 - val_auc: 0.7886\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8214 - auc: 0.7661 - val_loss: 0.4011 - val_acc: 0.8239 - val_auc: 0.7882\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8212 - auc: 0.7661 - val_loss: 0.4010 - val_acc: 0.8243 - val_auc: 0.7885\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8216 - auc: 0.7673 - val_loss: 0.4009 - val_acc: 0.8236 - val_auc: 0.7893\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7670 - val_loss: 0.4009 - val_acc: 0.8237 - val_auc: 0.7888\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8214 - auc: 0.7674 - val_loss: 0.4009 - val_acc: 0.8238 - val_auc: 0.7884\n",
      "Epoch 24/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8216 - auc: 0.7674\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8216 - auc: 0.7674 - val_loss: 0.4009 - val_acc: 0.8237 - val_auc: 0.7883\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8217 - auc: 0.7681 - val_loss: 0.4009 - val_acc: 0.8236 - val_auc: 0.7891\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8215 - auc: 0.7676 - val_loss: 0.4009 - val_acc: 0.8238 - val_auc: 0.7888\n",
      "Epoch 27/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8222 - auc: 0.7686\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8221 - auc: 0.7683 - val_loss: 0.4008 - val_acc: 0.8239 - val_auc: 0.7886\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8216 - auc: 0.7683 - val_loss: 0.4008 - val_acc: 0.8239 - val_auc: 0.7888\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8218 - auc: 0.7687 - val_loss: 0.4008 - val_acc: 0.8234 - val_auc: 0.7897\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8218 - auc: 0.7693 - val_loss: 0.4008 - val_acc: 0.8236 - val_auc: 0.7899\n",
      "Epoch 31/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8218 - auc: 0.7685Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4085 - acc: 0.8217 - auc: 0.7685 - val_loss: 0.4008 - val_acc: 0.8238 - val_auc: 0.7881\n",
      "Epoch 00031: early stopping\n",
      "Fold score: 0.7887934741160655\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.6739 - acc: 0.6031 - auc: 0.5368 - val_loss: 0.5384 - val_acc: 0.8038 - val_auc: 0.6482\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4898 - acc: 0.7903 - auc: 0.6326 - val_loss: 0.4430 - val_acc: 0.8071 - val_auc: 0.7440\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4515 - acc: 0.8077 - auc: 0.6908 - val_loss: 0.4233 - val_acc: 0.8108 - val_auc: 0.7701\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4374 - acc: 0.8117 - auc: 0.7194 - val_loss: 0.4152 - val_acc: 0.8148 - val_auc: 0.7785\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4314 - acc: 0.8138 - auc: 0.7320 - val_loss: 0.4118 - val_acc: 0.8148 - val_auc: 0.7821\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4275 - acc: 0.8149 - auc: 0.7392 - val_loss: 0.4101 - val_acc: 0.8148 - val_auc: 0.7845\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4247 - acc: 0.8155 - auc: 0.7443 - val_loss: 0.4092 - val_acc: 0.8151 - val_auc: 0.7853\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4223 - acc: 0.8166 - auc: 0.7483 - val_loss: 0.4086 - val_acc: 0.8151 - val_auc: 0.7862\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4195 - acc: 0.8173 - auc: 0.7523 - val_loss: 0.4081 - val_acc: 0.8162 - val_auc: 0.7864\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4182 - acc: 0.8175 - auc: 0.7544 - val_loss: 0.4076 - val_acc: 0.8163 - val_auc: 0.7881\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4159 - acc: 0.8184 - auc: 0.7576 - val_loss: 0.4073 - val_acc: 0.8160 - val_auc: 0.7883\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4152 - acc: 0.8191 - auc: 0.7586 - val_loss: 0.4070 - val_acc: 0.8158 - val_auc: 0.7882\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4140 - acc: 0.8196 - auc: 0.7603 - val_loss: 0.4069 - val_acc: 0.8165 - val_auc: 0.7894\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4129 - acc: 0.8197 - auc: 0.7617 - val_loss: 0.4068 - val_acc: 0.8158 - val_auc: 0.7891\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8201 - auc: 0.7635 - val_loss: 0.4066 - val_acc: 0.8160 - val_auc: 0.7893\n",
      "Epoch 16/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8206 - auc: 0.7643\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8206 - auc: 0.7643 - val_loss: 0.4065 - val_acc: 0.8163 - val_auc: 0.7887\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8208 - auc: 0.7657 - val_loss: 0.4063 - val_acc: 0.8168 - val_auc: 0.7889\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8212 - auc: 0.7662 - val_loss: 0.4063 - val_acc: 0.8170 - val_auc: 0.7896\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8212 - auc: 0.7659 - val_loss: 0.4062 - val_acc: 0.8174 - val_auc: 0.7898\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7671 - val_loss: 0.4062 - val_acc: 0.8177 - val_auc: 0.7902\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8211 - auc: 0.7666 - val_loss: 0.4062 - val_acc: 0.8178 - val_auc: 0.7906\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8215 - auc: 0.7676 - val_loss: 0.4061 - val_acc: 0.8182 - val_auc: 0.7897\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8215 - auc: 0.7676 - val_loss: 0.4061 - val_acc: 0.8182 - val_auc: 0.7908\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8217 - auc: 0.7678 - val_loss: 0.4060 - val_acc: 0.8181 - val_auc: 0.7903\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8218 - auc: 0.7681 - val_loss: 0.4059 - val_acc: 0.8177 - val_auc: 0.7915\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7689 - val_loss: 0.4059 - val_acc: 0.8178 - val_auc: 0.7906\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8219 - auc: 0.7696 - val_loss: 0.4058 - val_acc: 0.8179 - val_auc: 0.7909\n",
      "Epoch 28/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8222 - auc: 0.7695\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8222 - auc: 0.7694 - val_loss: 0.4058 - val_acc: 0.8181 - val_auc: 0.7899\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8219 - auc: 0.7696 - val_loss: 0.4058 - val_acc: 0.8177 - val_auc: 0.7909\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8222 - auc: 0.7699 - val_loss: 0.4058 - val_acc: 0.8177 - val_auc: 0.7911\n",
      "Epoch 31/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8224 - auc: 0.7705Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8223 - auc: 0.7706 - val_loss: 0.4057 - val_acc: 0.8177 - val_auc: 0.7893\n",
      "Epoch 00031: early stopping\n",
      "Fold score: 0.7900233939242322\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6455 - acc: 0.6303 - auc: 0.5455 - val_loss: 0.5265 - val_acc: 0.8160 - val_auc: 0.6636\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4839 - acc: 0.7945 - auc: 0.6392 - val_loss: 0.4267 - val_acc: 0.8214 - val_auc: 0.7461\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4481 - acc: 0.8086 - auc: 0.6973 - val_loss: 0.4075 - val_acc: 0.8254 - val_auc: 0.7688\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4364 - acc: 0.8119 - auc: 0.7220 - val_loss: 0.4005 - val_acc: 0.8273 - val_auc: 0.7774\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4302 - acc: 0.8134 - auc: 0.7345 - val_loss: 0.3975 - val_acc: 0.8281 - val_auc: 0.7800\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4259 - acc: 0.8149 - auc: 0.7426 - val_loss: 0.3959 - val_acc: 0.8290 - val_auc: 0.7819\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4235 - acc: 0.8156 - auc: 0.7467 - val_loss: 0.3951 - val_acc: 0.8289 - val_auc: 0.7823\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4209 - acc: 0.8167 - auc: 0.7507 - val_loss: 0.3943 - val_acc: 0.8297 - val_auc: 0.7830\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4196 - acc: 0.8172 - auc: 0.7526 - val_loss: 0.3940 - val_acc: 0.8298 - val_auc: 0.7846\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4178 - acc: 0.8175 - auc: 0.7549 - val_loss: 0.3938 - val_acc: 0.8302 - val_auc: 0.7849\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4162 - acc: 0.8188 - auc: 0.7575 - val_loss: 0.3934 - val_acc: 0.8302 - val_auc: 0.7854\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4153 - acc: 0.8187 - auc: 0.7589 - val_loss: 0.3933 - val_acc: 0.8304 - val_auc: 0.7847\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8196 - auc: 0.7610 - val_loss: 0.3931 - val_acc: 0.8313 - val_auc: 0.7859\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4128 - acc: 0.8196 - auc: 0.7625 - val_loss: 0.3931 - val_acc: 0.8308 - val_auc: 0.7872\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8201 - auc: 0.7637 - val_loss: 0.3927 - val_acc: 0.8314 - val_auc: 0.7872\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4111 - acc: 0.8207 - auc: 0.7647 - val_loss: 0.3927 - val_acc: 0.8317 - val_auc: 0.7872\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8205 - auc: 0.7659 - val_loss: 0.3928 - val_acc: 0.8317 - val_auc: 0.7876\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8207 - auc: 0.7674 - val_loss: 0.3925 - val_acc: 0.8316 - val_auc: 0.7867\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8211 - auc: 0.7675 - val_loss: 0.3924 - val_acc: 0.8313 - val_auc: 0.7885\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8217 - auc: 0.7682 - val_loss: 0.3925 - val_acc: 0.8315 - val_auc: 0.7879\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8214 - auc: 0.7689 - val_loss: 0.3922 - val_acc: 0.8317 - val_auc: 0.7881\n",
      "Epoch 22/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8217 - auc: 0.7688\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8216 - auc: 0.7686 - val_loss: 0.3922 - val_acc: 0.8321 - val_auc: 0.7884\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8214 - auc: 0.7696 - val_loss: 0.3922 - val_acc: 0.8320 - val_auc: 0.7883\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8216 - auc: 0.7702 - val_loss: 0.3922 - val_acc: 0.8317 - val_auc: 0.7882\n",
      "Epoch 25/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8219 - auc: 0.7706\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7705 - val_loss: 0.3920 - val_acc: 0.8315 - val_auc: 0.7883\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8220 - auc: 0.7714 - val_loss: 0.3920 - val_acc: 0.8313 - val_auc: 0.7885\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8220 - auc: 0.7706 - val_loss: 0.3920 - val_acc: 0.8314 - val_auc: 0.7883\n",
      "Epoch 28/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8220 - auc: 0.7705\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8219 - auc: 0.7706 - val_loss: 0.3921 - val_acc: 0.8314 - val_auc: 0.7883\n",
      "Epoch 29/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8217 - auc: 0.7710Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8217 - auc: 0.7709 - val_loss: 0.3920 - val_acc: 0.8313 - val_auc: 0.7886\n",
      "Epoch 00029: early stopping\n",
      "Fold score: 0.7877145249223153\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.6927 - acc: 0.5810 - auc: 0.5432 - val_loss: 0.5377 - val_acc: 0.8080 - val_auc: 0.6688\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4932 - acc: 0.7847 - auc: 0.6374 - val_loss: 0.4371 - val_acc: 0.8135 - val_auc: 0.7423\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4528 - acc: 0.8068 - auc: 0.6900 - val_loss: 0.4179 - val_acc: 0.8173 - val_auc: 0.7685\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4397 - acc: 0.8111 - auc: 0.7156 - val_loss: 0.4099 - val_acc: 0.8188 - val_auc: 0.7780\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4323 - acc: 0.8132 - auc: 0.7308 - val_loss: 0.4063 - val_acc: 0.8192 - val_auc: 0.7820\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4286 - acc: 0.8142 - auc: 0.7381 - val_loss: 0.4046 - val_acc: 0.8196 - val_auc: 0.7846\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4254 - acc: 0.8157 - auc: 0.7435 - val_loss: 0.4035 - val_acc: 0.8202 - val_auc: 0.7861\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4228 - acc: 0.8164 - auc: 0.7478 - val_loss: 0.4027 - val_acc: 0.8202 - val_auc: 0.7873\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4210 - acc: 0.8172 - auc: 0.7504 - val_loss: 0.4023 - val_acc: 0.8198 - val_auc: 0.7882\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4188 - acc: 0.8180 - auc: 0.7534 - val_loss: 0.4019 - val_acc: 0.8200 - val_auc: 0.7889\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4177 - acc: 0.8186 - auc: 0.7549 - val_loss: 0.4015 - val_acc: 0.8207 - val_auc: 0.7879\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4164 - acc: 0.8190 - auc: 0.7566 - val_loss: 0.4012 - val_acc: 0.8207 - val_auc: 0.7891\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4148 - acc: 0.8193 - auc: 0.7593 - val_loss: 0.4009 - val_acc: 0.8210 - val_auc: 0.7900\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8197 - auc: 0.7611 - val_loss: 0.4007 - val_acc: 0.8221 - val_auc: 0.7908\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4128 - acc: 0.8202 - auc: 0.7621 - val_loss: 0.4005 - val_acc: 0.8219 - val_auc: 0.7897\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8205 - auc: 0.7632 - val_loss: 0.4002 - val_acc: 0.8230 - val_auc: 0.7910\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8206 - auc: 0.7638 - val_loss: 0.4001 - val_acc: 0.8232 - val_auc: 0.7913\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8207 - auc: 0.7652 - val_loss: 0.3999 - val_acc: 0.8230 - val_auc: 0.7907\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8210 - auc: 0.7657 - val_loss: 0.3997 - val_acc: 0.8229 - val_auc: 0.7912\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8215 - auc: 0.7673 - val_loss: 0.3997 - val_acc: 0.8231 - val_auc: 0.7917\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8220 - auc: 0.7680 - val_loss: 0.3995 - val_acc: 0.8236 - val_auc: 0.7921\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8221 - auc: 0.7684 - val_loss: 0.3993 - val_acc: 0.8232 - val_auc: 0.7915\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8214 - auc: 0.7688 - val_loss: 0.3993 - val_acc: 0.8232 - val_auc: 0.7926\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8218 - auc: 0.7698 - val_loss: 0.3993 - val_acc: 0.8231 - val_auc: 0.7922\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8221 - auc: 0.7697 - val_loss: 0.3991 - val_acc: 0.8232 - val_auc: 0.7922\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8224 - auc: 0.7699 - val_loss: 0.3990 - val_acc: 0.8230 - val_auc: 0.7931\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4070 - acc: 0.8224 - auc: 0.7708 - val_loss: 0.3989 - val_acc: 0.8235 - val_auc: 0.7936\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4067 - acc: 0.8225 - auc: 0.7713 - val_loss: 0.3989 - val_acc: 0.8231 - val_auc: 0.7930\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4064 - acc: 0.8223 - auc: 0.7721 - val_loss: 0.3988 - val_acc: 0.8232 - val_auc: 0.7919\n",
      "Epoch 30/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8224 - auc: 0.7716\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4065 - acc: 0.8224 - auc: 0.7716 - val_loss: 0.3987 - val_acc: 0.8232 - val_auc: 0.7932\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4061 - acc: 0.8229 - auc: 0.7721 - val_loss: 0.3987 - val_acc: 0.8232 - val_auc: 0.7931\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4059 - acc: 0.8228 - auc: 0.7727 - val_loss: 0.3987 - val_acc: 0.8232 - val_auc: 0.7932\n",
      "Epoch 33/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8228 - auc: 0.7725\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4059 - acc: 0.8229 - auc: 0.7727 - val_loss: 0.3987 - val_acc: 0.8228 - val_auc: 0.7936\n",
      "Epoch 34/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4059 - acc: 0.8231 - auc: 0.7724 - val_loss: 0.3987 - val_acc: 0.8232 - val_auc: 0.7939\n",
      "Epoch 35/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4063 - acc: 0.8224 - auc: 0.7721 - val_loss: 0.3987 - val_acc: 0.8230 - val_auc: 0.7928\n",
      "Epoch 36/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8226 - auc: 0.7721Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4062 - acc: 0.8226 - auc: 0.7723 - val_loss: 0.3987 - val_acc: 0.8228 - val_auc: 0.7935\n",
      "Epoch 00036: early stopping\n",
      "Fold score: 0.7929408337068581\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5670 - acc: 0.7198 - auc: 0.5654 - val_loss: 0.5012 - val_acc: 0.8162 - val_auc: 0.6965\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4671 - acc: 0.8047 - auc: 0.6557 - val_loss: 0.4192 - val_acc: 0.8204 - val_auc: 0.7610\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4437 - acc: 0.8109 - auc: 0.7046 - val_loss: 0.4054 - val_acc: 0.8257 - val_auc: 0.7750\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4345 - acc: 0.8131 - auc: 0.7248 - val_loss: 0.3999 - val_acc: 0.8280 - val_auc: 0.7816\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4285 - acc: 0.8142 - auc: 0.7370 - val_loss: 0.3977 - val_acc: 0.8292 - val_auc: 0.7830\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4250 - acc: 0.8155 - auc: 0.7434 - val_loss: 0.3964 - val_acc: 0.8285 - val_auc: 0.7843\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4229 - acc: 0.8163 - auc: 0.7468 - val_loss: 0.3955 - val_acc: 0.8278 - val_auc: 0.7851\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4203 - acc: 0.8168 - auc: 0.7511 - val_loss: 0.3954 - val_acc: 0.8276 - val_auc: 0.7861\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4184 - acc: 0.8175 - auc: 0.7542 - val_loss: 0.3950 - val_acc: 0.8274 - val_auc: 0.7861\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4169 - acc: 0.8183 - auc: 0.7563 - val_loss: 0.3948 - val_acc: 0.8273 - val_auc: 0.7872\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4154 - acc: 0.8190 - auc: 0.7584 - val_loss: 0.3947 - val_acc: 0.8274 - val_auc: 0.7871\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4138 - acc: 0.8197 - auc: 0.7609 - val_loss: 0.3945 - val_acc: 0.8276 - val_auc: 0.7875\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4130 - acc: 0.8197 - auc: 0.7621 - val_loss: 0.3943 - val_acc: 0.8276 - val_auc: 0.7884\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4123 - acc: 0.8198 - auc: 0.7631 - val_loss: 0.3943 - val_acc: 0.8271 - val_auc: 0.7880\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8206 - auc: 0.7650 - val_loss: 0.3940 - val_acc: 0.8272 - val_auc: 0.7884\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8207 - auc: 0.7654 - val_loss: 0.3941 - val_acc: 0.8271 - val_auc: 0.7888\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8209 - auc: 0.7670 - val_loss: 0.3939 - val_acc: 0.8272 - val_auc: 0.7897\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8211 - auc: 0.7671 - val_loss: 0.3938 - val_acc: 0.8279 - val_auc: 0.7885\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8216 - auc: 0.7680 - val_loss: 0.3939 - val_acc: 0.8270 - val_auc: 0.7886\n",
      "Epoch 20/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8215 - auc: 0.7684\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8216 - auc: 0.7685 - val_loss: 0.3937 - val_acc: 0.8272 - val_auc: 0.7895\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8213 - auc: 0.7692 - val_loss: 0.3936 - val_acc: 0.8276 - val_auc: 0.7895\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8215 - auc: 0.7688 - val_loss: 0.3937 - val_acc: 0.8271 - val_auc: 0.7891\n",
      "Epoch 23/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8220 - auc: 0.7698\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8220 - auc: 0.7699 - val_loss: 0.3936 - val_acc: 0.8271 - val_auc: 0.7890\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8219 - auc: 0.7698 - val_loss: 0.3935 - val_acc: 0.8274 - val_auc: 0.7906\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8219 - auc: 0.7702 - val_loss: 0.3936 - val_acc: 0.8271 - val_auc: 0.7895\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8216 - auc: 0.7698 - val_loss: 0.3936 - val_acc: 0.8273 - val_auc: 0.7897\n",
      "Epoch 27/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8220 - auc: 0.7710Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4072 - acc: 0.8219 - auc: 0.7710 - val_loss: 0.3935 - val_acc: 0.8272 - val_auc: 0.7897\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7888426795986441\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.5976 - acc: 0.6861 - auc: 0.5539 - val_loss: 0.5150 - val_acc: 0.8134 - val_auc: 0.6585\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4728 - acc: 0.8018 - auc: 0.6498 - val_loss: 0.4284 - val_acc: 0.8183 - val_auc: 0.7417\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4452 - acc: 0.8104 - auc: 0.7019 - val_loss: 0.4124 - val_acc: 0.8212 - val_auc: 0.7638\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4344 - acc: 0.8128 - auc: 0.7249 - val_loss: 0.4059 - val_acc: 0.8232 - val_auc: 0.7711\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4282 - acc: 0.8148 - auc: 0.7375 - val_loss: 0.4030 - val_acc: 0.8228 - val_auc: 0.7747\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4251 - acc: 0.8157 - auc: 0.7435 - val_loss: 0.4016 - val_acc: 0.8239 - val_auc: 0.7766\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4226 - acc: 0.8166 - auc: 0.7475 - val_loss: 0.4008 - val_acc: 0.8251 - val_auc: 0.7785\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4197 - acc: 0.8174 - auc: 0.7524 - val_loss: 0.4001 - val_acc: 0.8246 - val_auc: 0.7793\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4179 - acc: 0.8178 - auc: 0.7548 - val_loss: 0.3999 - val_acc: 0.8247 - val_auc: 0.7787\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4161 - acc: 0.8188 - auc: 0.7575 - val_loss: 0.3996 - val_acc: 0.8242 - val_auc: 0.7803\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4151 - acc: 0.8187 - auc: 0.7592 - val_loss: 0.3993 - val_acc: 0.8252 - val_auc: 0.7804\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8195 - auc: 0.7612 - val_loss: 0.3991 - val_acc: 0.8247 - val_auc: 0.7813\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4130 - acc: 0.8198 - auc: 0.7620 - val_loss: 0.3988 - val_acc: 0.8249 - val_auc: 0.7814\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8203 - auc: 0.7636 - val_loss: 0.3988 - val_acc: 0.8250 - val_auc: 0.7818\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8204 - auc: 0.7646 - val_loss: 0.3986 - val_acc: 0.8250 - val_auc: 0.7830\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8209 - auc: 0.7662 - val_loss: 0.3984 - val_acc: 0.8252 - val_auc: 0.7823\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8212 - auc: 0.7674 - val_loss: 0.3981 - val_acc: 0.8249 - val_auc: 0.7823\n",
      "Epoch 18/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8214 - auc: 0.7677\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8214 - auc: 0.7678 - val_loss: 0.3979 - val_acc: 0.8248 - val_auc: 0.7825\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8211 - auc: 0.7683 - val_loss: 0.3980 - val_acc: 0.8251 - val_auc: 0.7829\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8215 - auc: 0.7687 - val_loss: 0.3979 - val_acc: 0.8250 - val_auc: 0.7831\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7694 - val_loss: 0.3978 - val_acc: 0.8252 - val_auc: 0.7840\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8215 - auc: 0.7690 - val_loss: 0.3979 - val_acc: 0.8253 - val_auc: 0.7834\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8219 - auc: 0.7693 - val_loss: 0.3978 - val_acc: 0.8254 - val_auc: 0.7826\n",
      "Epoch 24/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8219 - auc: 0.7694\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8219 - auc: 0.7694 - val_loss: 0.3978 - val_acc: 0.8257 - val_auc: 0.7833\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8217 - auc: 0.7698 - val_loss: 0.3978 - val_acc: 0.8259 - val_auc: 0.7828\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8216 - auc: 0.7700 - val_loss: 0.3977 - val_acc: 0.8257 - val_auc: 0.7836\n",
      "Epoch 27/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8220 - auc: 0.7703\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8220 - auc: 0.7703 - val_loss: 0.3978 - val_acc: 0.8258 - val_auc: 0.7835\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8217 - auc: 0.7704 - val_loss: 0.3977 - val_acc: 0.8258 - val_auc: 0.7827\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8223 - auc: 0.7705 - val_loss: 0.3977 - val_acc: 0.8258 - val_auc: 0.7834\n",
      "Epoch 30/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8220 - auc: 0.7706\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7707 - val_loss: 0.3977 - val_acc: 0.8257 - val_auc: 0.7829\n",
      "Epoch 31/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8221 - auc: 0.7709Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8220 - auc: 0.7708 - val_loss: 0.3977 - val_acc: 0.8258 - val_auc: 0.7834\n",
      "Epoch 00031: early stopping\n",
      "Fold score: 0.7830150216903466\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.5990 - acc: 0.6847 - auc: 0.5578 - val_loss: 0.5139 - val_acc: 0.8099 - val_auc: 0.6861\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4733 - acc: 0.8011 - auc: 0.6493 - val_loss: 0.4288 - val_acc: 0.8138 - val_auc: 0.7556\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4449 - acc: 0.8101 - auc: 0.7034 - val_loss: 0.4129 - val_acc: 0.8201 - val_auc: 0.7733\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4357 - acc: 0.8125 - auc: 0.7237 - val_loss: 0.4069 - val_acc: 0.8213 - val_auc: 0.7806\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4299 - acc: 0.8137 - auc: 0.7355 - val_loss: 0.4041 - val_acc: 0.8224 - val_auc: 0.7843\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4266 - acc: 0.8148 - auc: 0.7418 - val_loss: 0.4029 - val_acc: 0.8234 - val_auc: 0.7863\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4238 - acc: 0.8155 - auc: 0.7461 - val_loss: 0.4022 - val_acc: 0.8233 - val_auc: 0.7878\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4213 - acc: 0.8166 - auc: 0.7496 - val_loss: 0.4016 - val_acc: 0.8236 - val_auc: 0.7873\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4194 - acc: 0.8175 - auc: 0.7526 - val_loss: 0.4013 - val_acc: 0.8233 - val_auc: 0.7885\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4179 - acc: 0.8180 - auc: 0.7545 - val_loss: 0.4011 - val_acc: 0.8232 - val_auc: 0.7882\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4163 - acc: 0.8185 - auc: 0.7569 - val_loss: 0.4008 - val_acc: 0.8236 - val_auc: 0.7884\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4148 - acc: 0.8192 - auc: 0.7590 - val_loss: 0.4005 - val_acc: 0.8236 - val_auc: 0.7894\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4141 - acc: 0.8197 - auc: 0.7600 - val_loss: 0.4005 - val_acc: 0.8227 - val_auc: 0.7905\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8200 - auc: 0.7627 - val_loss: 0.4002 - val_acc: 0.8229 - val_auc: 0.7916\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4115 - acc: 0.8202 - auc: 0.7641 - val_loss: 0.4001 - val_acc: 0.8233 - val_auc: 0.7908\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8205 - auc: 0.7651 - val_loss: 0.4002 - val_acc: 0.8234 - val_auc: 0.7903\n",
      "Epoch 17/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8210 - auc: 0.7655\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8209 - auc: 0.7657 - val_loss: 0.4001 - val_acc: 0.8232 - val_auc: 0.7910\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8208 - auc: 0.7661 - val_loss: 0.4001 - val_acc: 0.8237 - val_auc: 0.7896\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8210 - auc: 0.7671 - val_loss: 0.4001 - val_acc: 0.8236 - val_auc: 0.7917\n",
      "Epoch 20/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8213 - auc: 0.7673\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7672 - val_loss: 0.4001 - val_acc: 0.8229 - val_auc: 0.7907\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8218 - auc: 0.7678 - val_loss: 0.4000 - val_acc: 0.8227 - val_auc: 0.7913\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8213 - auc: 0.7678 - val_loss: 0.4000 - val_acc: 0.8227 - val_auc: 0.7908\n",
      "Epoch 23/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8215 - auc: 0.7676\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8215 - auc: 0.7676 - val_loss: 0.4000 - val_acc: 0.8226 - val_auc: 0.7915\n",
      "Epoch 24/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8214 - auc: 0.7680Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8215 - auc: 0.7679 - val_loss: 0.4000 - val_acc: 0.8226 - val_auc: 0.7914\n",
      "Epoch 00024: early stopping\n",
      "Fold score: 0.7904440686972942\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.5742 - acc: 0.7140 - auc: 0.5587 - val_loss: 0.5069 - val_acc: 0.8173 - val_auc: 0.6751\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4693 - acc: 0.8050 - auc: 0.6506 - val_loss: 0.4264 - val_acc: 0.8195 - val_auc: 0.7442\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4437 - acc: 0.8114 - auc: 0.7037 - val_loss: 0.4122 - val_acc: 0.8223 - val_auc: 0.7617\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4340 - acc: 0.8134 - auc: 0.7251 - val_loss: 0.4070 - val_acc: 0.8215 - val_auc: 0.7675\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4284 - acc: 0.8150 - auc: 0.7362 - val_loss: 0.4044 - val_acc: 0.8214 - val_auc: 0.7705\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4249 - acc: 0.8157 - auc: 0.7434 - val_loss: 0.4030 - val_acc: 0.8233 - val_auc: 0.7729\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4220 - acc: 0.8166 - auc: 0.7482 - val_loss: 0.4021 - val_acc: 0.8232 - val_auc: 0.7735\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4198 - acc: 0.8179 - auc: 0.7515 - val_loss: 0.4016 - val_acc: 0.8235 - val_auc: 0.7738\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4178 - acc: 0.8182 - auc: 0.7551 - val_loss: 0.4010 - val_acc: 0.8235 - val_auc: 0.7757\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4164 - acc: 0.8190 - auc: 0.7567 - val_loss: 0.4008 - val_acc: 0.8239 - val_auc: 0.7760\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8195 - auc: 0.7594 - val_loss: 0.4006 - val_acc: 0.8244 - val_auc: 0.7774\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4137 - acc: 0.8195 - auc: 0.7608 - val_loss: 0.4002 - val_acc: 0.8244 - val_auc: 0.7758\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4132 - acc: 0.8200 - auc: 0.7614 - val_loss: 0.3998 - val_acc: 0.8251 - val_auc: 0.7771\n",
      "Epoch 14/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8206 - auc: 0.7637\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8206 - auc: 0.7639 - val_loss: 0.4001 - val_acc: 0.8251 - val_auc: 0.7774\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8205 - auc: 0.7644 - val_loss: 0.3998 - val_acc: 0.8252 - val_auc: 0.7772\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8209 - auc: 0.7658 - val_loss: 0.3997 - val_acc: 0.8250 - val_auc: 0.7774\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8207 - auc: 0.7659 - val_loss: 0.3997 - val_acc: 0.8249 - val_auc: 0.7787\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8212 - auc: 0.7658 - val_loss: 0.3996 - val_acc: 0.8245 - val_auc: 0.7777\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8207 - auc: 0.7663 - val_loss: 0.3995 - val_acc: 0.8248 - val_auc: 0.7779\n",
      "Epoch 20/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8210 - auc: 0.7667\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8210 - auc: 0.7669 - val_loss: 0.3995 - val_acc: 0.8249 - val_auc: 0.7782\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8213 - auc: 0.7668 - val_loss: 0.3995 - val_acc: 0.8248 - val_auc: 0.7778\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8214 - auc: 0.7673 - val_loss: 0.3994 - val_acc: 0.8248 - val_auc: 0.7776\n",
      "Epoch 23/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8212 - auc: 0.7674\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7674 - val_loss: 0.3993 - val_acc: 0.8246 - val_auc: 0.7784\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8212 - auc: 0.7675 - val_loss: 0.3993 - val_acc: 0.8246 - val_auc: 0.7778\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8212 - auc: 0.7675 - val_loss: 0.3993 - val_acc: 0.8247 - val_auc: 0.7777\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8213 - auc: 0.7686 - val_loss: 0.3992 - val_acc: 0.8246 - val_auc: 0.7794\n",
      "Epoch 27/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8213 - auc: 0.7682Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4089 - acc: 0.8214 - auc: 0.7682 - val_loss: 0.3992 - val_acc: 0.8244 - val_auc: 0.7780\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7776383037452118\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.6518 - acc: 0.6198 - auc: 0.5832 - val_loss: 0.5264 - val_acc: 0.8132 - val_auc: 0.6946\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4804 - acc: 0.7938 - auc: 0.6569 - val_loss: 0.4269 - val_acc: 0.8174 - val_auc: 0.7538\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4444 - acc: 0.8099 - auc: 0.7058 - val_loss: 0.4100 - val_acc: 0.8198 - val_auc: 0.7715\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4335 - acc: 0.8129 - auc: 0.7271 - val_loss: 0.4046 - val_acc: 0.8222 - val_auc: 0.7759\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4287 - acc: 0.8143 - auc: 0.7372 - val_loss: 0.4026 - val_acc: 0.8230 - val_auc: 0.7785\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4252 - acc: 0.8154 - auc: 0.7436 - val_loss: 0.4017 - val_acc: 0.8224 - val_auc: 0.7790\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4230 - acc: 0.8161 - auc: 0.7472 - val_loss: 0.4012 - val_acc: 0.8238 - val_auc: 0.7802\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4206 - acc: 0.8172 - auc: 0.7510 - val_loss: 0.4008 - val_acc: 0.8240 - val_auc: 0.7797\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4191 - acc: 0.8180 - auc: 0.7530 - val_loss: 0.4008 - val_acc: 0.8237 - val_auc: 0.7794\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4170 - acc: 0.8184 - auc: 0.7563 - val_loss: 0.4006 - val_acc: 0.8237 - val_auc: 0.7811\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4159 - acc: 0.8191 - auc: 0.7574 - val_loss: 0.4004 - val_acc: 0.8238 - val_auc: 0.7799\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4149 - acc: 0.8191 - auc: 0.7592 - val_loss: 0.4004 - val_acc: 0.8238 - val_auc: 0.7813\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8195 - auc: 0.7600 - val_loss: 0.4004 - val_acc: 0.8243 - val_auc: 0.7814\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8201 - auc: 0.7630 - val_loss: 0.4003 - val_acc: 0.8242 - val_auc: 0.7811\n",
      "Epoch 15/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8205 - auc: 0.7638\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4117 - acc: 0.8204 - auc: 0.7638 - val_loss: 0.4001 - val_acc: 0.8242 - val_auc: 0.7804\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8204 - auc: 0.7646 - val_loss: 0.4001 - val_acc: 0.8242 - val_auc: 0.7811\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8207 - auc: 0.7651 - val_loss: 0.4000 - val_acc: 0.8242 - val_auc: 0.7826\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8207 - auc: 0.7656 - val_loss: 0.4001 - val_acc: 0.8240 - val_auc: 0.7817\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8212 - auc: 0.7664 - val_loss: 0.4000 - val_acc: 0.8242 - val_auc: 0.7812\n",
      "Epoch 20/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8213 - auc: 0.7671\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8213 - auc: 0.7671 - val_loss: 0.3999 - val_acc: 0.8246 - val_auc: 0.7819\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7674 - val_loss: 0.3999 - val_acc: 0.8247 - val_auc: 0.7812\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8209 - auc: 0.7670 - val_loss: 0.3999 - val_acc: 0.8246 - val_auc: 0.7826\n",
      "Epoch 23/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8214 - auc: 0.7672\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8213 - auc: 0.7672 - val_loss: 0.3999 - val_acc: 0.8249 - val_auc: 0.7815\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8216 - auc: 0.7674 - val_loss: 0.3999 - val_acc: 0.8245 - val_auc: 0.7815\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8211 - auc: 0.7671 - val_loss: 0.3999 - val_acc: 0.8247 - val_auc: 0.7815\n",
      "Epoch 26/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8213 - auc: 0.7675\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8213 - auc: 0.7675 - val_loss: 0.3999 - val_acc: 0.8246 - val_auc: 0.7812\n",
      "Epoch 27/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8212 - auc: 0.7682Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8212 - auc: 0.7681 - val_loss: 0.3999 - val_acc: 0.8248 - val_auc: 0.7821\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7816417007165537\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5942 - acc: 0.6901 - auc: 0.5523 - val_loss: 0.5155 - val_acc: 0.8133 - val_auc: 0.6770\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4733 - acc: 0.8016 - auc: 0.6467 - val_loss: 0.4304 - val_acc: 0.8142 - val_auc: 0.7482\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4469 - acc: 0.8103 - auc: 0.6975 - val_loss: 0.4136 - val_acc: 0.8186 - val_auc: 0.7705\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4356 - acc: 0.8129 - auc: 0.7224 - val_loss: 0.4064 - val_acc: 0.8202 - val_auc: 0.7786\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4297 - acc: 0.8141 - auc: 0.7350 - val_loss: 0.4029 - val_acc: 0.8229 - val_auc: 0.7829\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4258 - acc: 0.8155 - auc: 0.7421 - val_loss: 0.4015 - val_acc: 0.8217 - val_auc: 0.7835\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4231 - acc: 0.8164 - auc: 0.7469 - val_loss: 0.4007 - val_acc: 0.8214 - val_auc: 0.7845\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4210 - acc: 0.8167 - auc: 0.7499 - val_loss: 0.4002 - val_acc: 0.8223 - val_auc: 0.7867\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4193 - acc: 0.8175 - auc: 0.7525 - val_loss: 0.3998 - val_acc: 0.8217 - val_auc: 0.7858\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4174 - acc: 0.8183 - auc: 0.7552 - val_loss: 0.3996 - val_acc: 0.8216 - val_auc: 0.7869\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4162 - acc: 0.8189 - auc: 0.7568 - val_loss: 0.3991 - val_acc: 0.8221 - val_auc: 0.7878\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4145 - acc: 0.8190 - auc: 0.7598 - val_loss: 0.3991 - val_acc: 0.8223 - val_auc: 0.7875\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8199 - auc: 0.7607 - val_loss: 0.3988 - val_acc: 0.8227 - val_auc: 0.7866\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4128 - acc: 0.8201 - auc: 0.7621 - val_loss: 0.3984 - val_acc: 0.8224 - val_auc: 0.7888\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8201 - auc: 0.7630 - val_loss: 0.3984 - val_acc: 0.8229 - val_auc: 0.7882\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8209 - auc: 0.7649 - val_loss: 0.3984 - val_acc: 0.8231 - val_auc: 0.7888\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8205 - auc: 0.7649 - val_loss: 0.3982 - val_acc: 0.8238 - val_auc: 0.7900\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8210 - auc: 0.7661 - val_loss: 0.3983 - val_acc: 0.8242 - val_auc: 0.7888\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8212 - auc: 0.7674 - val_loss: 0.3981 - val_acc: 0.8240 - val_auc: 0.7897\n",
      "Epoch 20/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8214 - auc: 0.7673\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7673 - val_loss: 0.3981 - val_acc: 0.8237 - val_auc: 0.7891\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8216 - auc: 0.7683 - val_loss: 0.3979 - val_acc: 0.8236 - val_auc: 0.7901\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8216 - auc: 0.7691 - val_loss: 0.3980 - val_acc: 0.8236 - val_auc: 0.7895\n",
      "Epoch 23/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8216 - auc: 0.7687\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8215 - auc: 0.7687 - val_loss: 0.3981 - val_acc: 0.8238 - val_auc: 0.7891\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7694 - val_loss: 0.3979 - val_acc: 0.8238 - val_auc: 0.7899\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8220 - auc: 0.7696 - val_loss: 0.3979 - val_acc: 0.8239 - val_auc: 0.7887\n",
      "Epoch 26/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8220 - auc: 0.7697\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8219 - auc: 0.7697 - val_loss: 0.3979 - val_acc: 0.8240 - val_auc: 0.7876\n",
      "Epoch 27/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8217 - auc: 0.7697Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8217 - auc: 0.7698 - val_loss: 0.3979 - val_acc: 0.8238 - val_auc: 0.7894\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7888717452873805\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5064 - acc: 0.7948 - auc: 0.5654 - val_loss: 0.4865 - val_acc: 0.8184 - val_auc: 0.6886\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4581 - acc: 0.8109 - auc: 0.6638 - val_loss: 0.4213 - val_acc: 0.8223 - val_auc: 0.7509\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4397 - acc: 0.8130 - auc: 0.7101 - val_loss: 0.4080 - val_acc: 0.8273 - val_auc: 0.7681\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4304 - acc: 0.8152 - auc: 0.7304 - val_loss: 0.4018 - val_acc: 0.8286 - val_auc: 0.7759\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4253 - acc: 0.8162 - auc: 0.7407 - val_loss: 0.3985 - val_acc: 0.8289 - val_auc: 0.7801\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4214 - acc: 0.8167 - auc: 0.7479 - val_loss: 0.3966 - val_acc: 0.8301 - val_auc: 0.7809\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4197 - acc: 0.8179 - auc: 0.7512 - val_loss: 0.3962 - val_acc: 0.8309 - val_auc: 0.7826\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4176 - acc: 0.8186 - auc: 0.7546 - val_loss: 0.3955 - val_acc: 0.8305 - val_auc: 0.7832\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4164 - acc: 0.8186 - auc: 0.7566 - val_loss: 0.3953 - val_acc: 0.8303 - val_auc: 0.7842\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4148 - acc: 0.8192 - auc: 0.7589 - val_loss: 0.3949 - val_acc: 0.8309 - val_auc: 0.7851\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8195 - auc: 0.7613 - val_loss: 0.3947 - val_acc: 0.8307 - val_auc: 0.7831\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4125 - acc: 0.8199 - auc: 0.7628 - val_loss: 0.3948 - val_acc: 0.8302 - val_auc: 0.7847\n",
      "Epoch 13/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8201 - auc: 0.7633\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4121 - acc: 0.8201 - auc: 0.7633 - val_loss: 0.3945 - val_acc: 0.8307 - val_auc: 0.7845\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8204 - auc: 0.7646 - val_loss: 0.3946 - val_acc: 0.8307 - val_auc: 0.7832\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8210 - auc: 0.7653 - val_loss: 0.3945 - val_acc: 0.8310 - val_auc: 0.7855\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8208 - auc: 0.7657 - val_loss: 0.3944 - val_acc: 0.8313 - val_auc: 0.7847\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8209 - auc: 0.7657 - val_loss: 0.3943 - val_acc: 0.8307 - val_auc: 0.7855\n",
      "Epoch 18/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8212 - auc: 0.7663\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8213 - auc: 0.7665 - val_loss: 0.3944 - val_acc: 0.8309 - val_auc: 0.7849\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8211 - auc: 0.7667 - val_loss: 0.3943 - val_acc: 0.8309 - val_auc: 0.7846\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8208 - auc: 0.7674 - val_loss: 0.3943 - val_acc: 0.8313 - val_auc: 0.7848\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8208 - auc: 0.7666 - val_loss: 0.3943 - val_acc: 0.8315 - val_auc: 0.7862\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8212 - auc: 0.7672 - val_loss: 0.3943 - val_acc: 0.8313 - val_auc: 0.7848\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8209 - auc: 0.7680 - val_loss: 0.3943 - val_acc: 0.8319 - val_auc: 0.7854\n",
      "Epoch 24/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8211 - auc: 0.7679\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8211 - auc: 0.7679 - val_loss: 0.3942 - val_acc: 0.8317 - val_auc: 0.7855\n",
      "Epoch 25/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8213 - auc: 0.7675Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7675 - val_loss: 0.3943 - val_acc: 0.8317 - val_auc: 0.7853\n",
      "Epoch 00025: early stopping\n",
      "Fold score: 0.7852530387356406\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5115 - acc: 0.7824 - auc: 0.5739 - val_loss: 0.4811 - val_acc: 0.8153 - val_auc: 0.7050\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4564 - acc: 0.8093 - auc: 0.6731 - val_loss: 0.4202 - val_acc: 0.8206 - val_auc: 0.7560\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4387 - acc: 0.8122 - auc: 0.7154 - val_loss: 0.4100 - val_acc: 0.8214 - val_auc: 0.7693\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4311 - acc: 0.8136 - auc: 0.7316 - val_loss: 0.4057 - val_acc: 0.8213 - val_auc: 0.7737\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4268 - acc: 0.8149 - auc: 0.7402 - val_loss: 0.4041 - val_acc: 0.8226 - val_auc: 0.7767\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4235 - acc: 0.8155 - auc: 0.7458 - val_loss: 0.4032 - val_acc: 0.8240 - val_auc: 0.7772\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4207 - acc: 0.8171 - auc: 0.7503 - val_loss: 0.4027 - val_acc: 0.8246 - val_auc: 0.7793\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4188 - acc: 0.8177 - auc: 0.7530 - val_loss: 0.4023 - val_acc: 0.8250 - val_auc: 0.7790\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4176 - acc: 0.8184 - auc: 0.7550 - val_loss: 0.4018 - val_acc: 0.8250 - val_auc: 0.7796\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8187 - auc: 0.7582 - val_loss: 0.4018 - val_acc: 0.8241 - val_auc: 0.7792\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8196 - auc: 0.7600 - val_loss: 0.4016 - val_acc: 0.8244 - val_auc: 0.7805\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4134 - acc: 0.8196 - auc: 0.7613 - val_loss: 0.4014 - val_acc: 0.8246 - val_auc: 0.7808\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8201 - auc: 0.7633 - val_loss: 0.4012 - val_acc: 0.8248 - val_auc: 0.7796\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8202 - auc: 0.7644 - val_loss: 0.4012 - val_acc: 0.8248 - val_auc: 0.7804\n",
      "Epoch 15/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8204 - auc: 0.7651\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8204 - auc: 0.7652 - val_loss: 0.4012 - val_acc: 0.8249 - val_auc: 0.7802\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8205 - auc: 0.7657 - val_loss: 0.4012 - val_acc: 0.8249 - val_auc: 0.7812\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8210 - auc: 0.7664 - val_loss: 0.4011 - val_acc: 0.8250 - val_auc: 0.7808\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8211 - auc: 0.7676 - val_loss: 0.4011 - val_acc: 0.8250 - val_auc: 0.7811\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8210 - auc: 0.7679 - val_loss: 0.4011 - val_acc: 0.8253 - val_auc: 0.7815\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7673 - val_loss: 0.4011 - val_acc: 0.8254 - val_auc: 0.7819\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8214 - auc: 0.7675 - val_loss: 0.4011 - val_acc: 0.8252 - val_auc: 0.7805\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8217 - auc: 0.7684 - val_loss: 0.4009 - val_acc: 0.8253 - val_auc: 0.7812\n",
      "Epoch 23/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8219 - auc: 0.7687\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8219 - auc: 0.7688 - val_loss: 0.4011 - val_acc: 0.8253 - val_auc: 0.7813\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8219 - auc: 0.7688 - val_loss: 0.4010 - val_acc: 0.8253 - val_auc: 0.7808\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8215 - auc: 0.7686 - val_loss: 0.4010 - val_acc: 0.8254 - val_auc: 0.7816\n",
      "Epoch 26/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8215 - auc: 0.7690\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8216 - auc: 0.7691 - val_loss: 0.4009 - val_acc: 0.8257 - val_auc: 0.7819\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8216 - auc: 0.7695 - val_loss: 0.4010 - val_acc: 0.8255 - val_auc: 0.7812\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7696 - val_loss: 0.4009 - val_acc: 0.8257 - val_auc: 0.7814\n",
      "Epoch 29/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8218 - auc: 0.7694\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8218 - auc: 0.7696 - val_loss: 0.4010 - val_acc: 0.8255 - val_auc: 0.7804\n",
      "Epoch 30/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8214 - auc: 0.7695Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8214 - auc: 0.7694 - val_loss: 0.4010 - val_acc: 0.8257 - val_auc: 0.7810\n",
      "Epoch 00030: early stopping\n",
      "Fold score: 0.7811316173211335\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5950 - acc: 0.6914 - auc: 0.5435 - val_loss: 0.5163 - val_acc: 0.8199 - val_auc: 0.6557\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4748 - acc: 0.8040 - auc: 0.6413 - val_loss: 0.4273 - val_acc: 0.8215 - val_auc: 0.7376\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4464 - acc: 0.8110 - auc: 0.6968 - val_loss: 0.4098 - val_acc: 0.8241 - val_auc: 0.7614\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4336 - acc: 0.8138 - auc: 0.7248 - val_loss: 0.4023 - val_acc: 0.8275 - val_auc: 0.7700\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4277 - acc: 0.8149 - auc: 0.7368 - val_loss: 0.3990 - val_acc: 0.8278 - val_auc: 0.7747\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4236 - acc: 0.8161 - auc: 0.7450 - val_loss: 0.3976 - val_acc: 0.8283 - val_auc: 0.7772\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4219 - acc: 0.8167 - auc: 0.7481 - val_loss: 0.3966 - val_acc: 0.8287 - val_auc: 0.7776\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4193 - acc: 0.8172 - auc: 0.7526 - val_loss: 0.3961 - val_acc: 0.8282 - val_auc: 0.7791\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4179 - acc: 0.8181 - auc: 0.7545 - val_loss: 0.3955 - val_acc: 0.8279 - val_auc: 0.7794\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4164 - acc: 0.8187 - auc: 0.7568 - val_loss: 0.3955 - val_acc: 0.8284 - val_auc: 0.7796\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4144 - acc: 0.8194 - auc: 0.7600 - val_loss: 0.3950 - val_acc: 0.8290 - val_auc: 0.7795\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8199 - auc: 0.7614 - val_loss: 0.3949 - val_acc: 0.8291 - val_auc: 0.7808\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8200 - auc: 0.7626 - val_loss: 0.3948 - val_acc: 0.8289 - val_auc: 0.7809\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4123 - acc: 0.8199 - auc: 0.7631 - val_loss: 0.3946 - val_acc: 0.8288 - val_auc: 0.7810\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4117 - acc: 0.8205 - auc: 0.7641 - val_loss: 0.3948 - val_acc: 0.8290 - val_auc: 0.7808\n",
      "Epoch 16/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8208 - auc: 0.7656\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8208 - auc: 0.7656 - val_loss: 0.3946 - val_acc: 0.8288 - val_auc: 0.7806\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8211 - auc: 0.7664 - val_loss: 0.3944 - val_acc: 0.8292 - val_auc: 0.7817\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8209 - auc: 0.7674 - val_loss: 0.3945 - val_acc: 0.8294 - val_auc: 0.7811\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8212 - auc: 0.7668 - val_loss: 0.3944 - val_acc: 0.8296 - val_auc: 0.7819\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8211 - auc: 0.7673 - val_loss: 0.3944 - val_acc: 0.8290 - val_auc: 0.7814\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8217 - auc: 0.7676 - val_loss: 0.3943 - val_acc: 0.8292 - val_auc: 0.7819\n",
      "Epoch 22/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8211 - auc: 0.7681\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8212 - auc: 0.7679 - val_loss: 0.3943 - val_acc: 0.8292 - val_auc: 0.7812\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8215 - auc: 0.7685 - val_loss: 0.3941 - val_acc: 0.8296 - val_auc: 0.7825\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8215 - auc: 0.7689 - val_loss: 0.3942 - val_acc: 0.8292 - val_auc: 0.7813\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8216 - auc: 0.7683 - val_loss: 0.3942 - val_acc: 0.8292 - val_auc: 0.7817\n",
      "Epoch 26/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8216 - auc: 0.7685\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8216 - auc: 0.7685 - val_loss: 0.3941 - val_acc: 0.8293 - val_auc: 0.7807\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7696 - val_loss: 0.3941 - val_acc: 0.8293 - val_auc: 0.7833\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8216 - auc: 0.7688 - val_loss: 0.3942 - val_acc: 0.8292 - val_auc: 0.7830\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8218 - auc: 0.7692 - val_loss: 0.3941 - val_acc: 0.8293 - val_auc: 0.7824\n",
      "Epoch 30/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8214 - auc: 0.7694\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8214 - auc: 0.7694 - val_loss: 0.3940 - val_acc: 0.8292 - val_auc: 0.7829\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8214 - auc: 0.7695 - val_loss: 0.3940 - val_acc: 0.8296 - val_auc: 0.7824\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8213 - auc: 0.7693 - val_loss: 0.3941 - val_acc: 0.8293 - val_auc: 0.7822\n",
      "Epoch 33/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8217 - auc: 0.7694\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7694 - val_loss: 0.3942 - val_acc: 0.8292 - val_auc: 0.7817\n",
      "Epoch 34/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8221 - auc: 0.7695 - val_loss: 0.3941 - val_acc: 0.8293 - val_auc: 0.7826\n",
      "Epoch 35/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8221 - auc: 0.7701 - val_loss: 0.3941 - val_acc: 0.8298 - val_auc: 0.7822\n",
      "Epoch 36/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8218 - auc: 0.7689 - val_loss: 0.3941 - val_acc: 0.8294 - val_auc: 0.7834\n",
      "Epoch 37/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8220 - auc: 0.7694Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8221 - auc: 0.7693 - val_loss: 0.3941 - val_acc: 0.8292 - val_auc: 0.7825\n",
      "Epoch 00037: early stopping\n",
      "Fold score: 0.782238513662669\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5480 - acc: 0.7495 - auc: 0.5466 - val_loss: 0.5006 - val_acc: 0.8166 - val_auc: 0.6834\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4655 - acc: 0.8079 - auc: 0.6518 - val_loss: 0.4230 - val_acc: 0.8196 - val_auc: 0.7550\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4438 - acc: 0.8117 - auc: 0.7025 - val_loss: 0.4077 - val_acc: 0.8254 - val_auc: 0.7726\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4336 - acc: 0.8135 - auc: 0.7251 - val_loss: 0.4004 - val_acc: 0.8265 - val_auc: 0.7820\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4278 - acc: 0.8151 - auc: 0.7372 - val_loss: 0.3967 - val_acc: 0.8275 - val_auc: 0.7858\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4241 - acc: 0.8159 - auc: 0.7441 - val_loss: 0.3950 - val_acc: 0.8288 - val_auc: 0.7877\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4212 - acc: 0.8168 - auc: 0.7491 - val_loss: 0.3941 - val_acc: 0.8298 - val_auc: 0.7889\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4193 - acc: 0.8176 - auc: 0.7523 - val_loss: 0.3938 - val_acc: 0.8299 - val_auc: 0.7898\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4174 - acc: 0.8181 - auc: 0.7554 - val_loss: 0.3934 - val_acc: 0.8303 - val_auc: 0.7905\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4158 - acc: 0.8187 - auc: 0.7578 - val_loss: 0.3928 - val_acc: 0.8295 - val_auc: 0.7905\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4149 - acc: 0.8191 - auc: 0.7590 - val_loss: 0.3926 - val_acc: 0.8294 - val_auc: 0.7912\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4139 - acc: 0.8193 - auc: 0.7606 - val_loss: 0.3927 - val_acc: 0.8295 - val_auc: 0.7912\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4130 - acc: 0.8198 - auc: 0.7617 - val_loss: 0.3925 - val_acc: 0.8298 - val_auc: 0.7916\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8202 - auc: 0.7637 - val_loss: 0.3924 - val_acc: 0.8299 - val_auc: 0.7920\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8203 - auc: 0.7644 - val_loss: 0.3923 - val_acc: 0.8291 - val_auc: 0.7918\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8206 - auc: 0.7661 - val_loss: 0.3922 - val_acc: 0.8298 - val_auc: 0.7917\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8212 - auc: 0.7666 - val_loss: 0.3920 - val_acc: 0.8293 - val_auc: 0.7930\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8215 - auc: 0.7672 - val_loss: 0.3923 - val_acc: 0.8306 - val_auc: 0.7922\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8215 - auc: 0.7681 - val_loss: 0.3921 - val_acc: 0.8301 - val_auc: 0.7926\n",
      "Epoch 20/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8214 - auc: 0.7683\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8215 - auc: 0.7684 - val_loss: 0.3921 - val_acc: 0.8295 - val_auc: 0.7927\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8215 - auc: 0.7695 - val_loss: 0.3918 - val_acc: 0.8300 - val_auc: 0.7932\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8212 - auc: 0.7694 - val_loss: 0.3918 - val_acc: 0.8300 - val_auc: 0.7925\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8216 - auc: 0.7695 - val_loss: 0.3918 - val_acc: 0.8298 - val_auc: 0.7929\n",
      "Epoch 24/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8215 - auc: 0.7701\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8215 - auc: 0.7701 - val_loss: 0.3918 - val_acc: 0.8298 - val_auc: 0.7929\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8215 - auc: 0.7705 - val_loss: 0.3918 - val_acc: 0.8296 - val_auc: 0.7943\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7707 - val_loss: 0.3917 - val_acc: 0.8297 - val_auc: 0.7926\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8220 - auc: 0.7703 - val_loss: 0.3918 - val_acc: 0.8295 - val_auc: 0.7925\n",
      "Epoch 28/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8221 - auc: 0.7706\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8221 - auc: 0.7706 - val_loss: 0.3917 - val_acc: 0.8297 - val_auc: 0.7931\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8217 - auc: 0.7710 - val_loss: 0.3917 - val_acc: 0.8297 - val_auc: 0.7928\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8225 - auc: 0.7714 - val_loss: 0.3917 - val_acc: 0.8298 - val_auc: 0.7931\n",
      "Epoch 31/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8222 - auc: 0.7710\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8223 - auc: 0.7710 - val_loss: 0.3916 - val_acc: 0.8297 - val_auc: 0.7928\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8220 - auc: 0.7712 - val_loss: 0.3916 - val_acc: 0.8297 - val_auc: 0.7929\n",
      "Epoch 33/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8220 - auc: 0.7701 - val_loss: 0.3917 - val_acc: 0.8298 - val_auc: 0.7919\n",
      "Epoch 34/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8221 - auc: 0.7712\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8221 - auc: 0.7711 - val_loss: 0.3917 - val_acc: 0.8296 - val_auc: 0.7929\n",
      "Epoch 35/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8220 - auc: 0.7709Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8220 - auc: 0.7710 - val_loss: 0.3917 - val_acc: 0.8297 - val_auc: 0.7929\n",
      "Epoch 00035: early stopping\n",
      "Fold score: 0.7927085965678053\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6479 - acc: 0.6257 - auc: 0.5399 - val_loss: 0.5327 - val_acc: 0.8138 - val_auc: 0.6514\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4870 - acc: 0.7949 - auc: 0.6306 - val_loss: 0.4322 - val_acc: 0.8165 - val_auc: 0.7444\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4505 - acc: 0.8091 - auc: 0.6911 - val_loss: 0.4116 - val_acc: 0.8211 - val_auc: 0.7717\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4372 - acc: 0.8126 - auc: 0.7193 - val_loss: 0.4035 - val_acc: 0.8227 - val_auc: 0.7800\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4310 - acc: 0.8138 - auc: 0.7324 - val_loss: 0.3996 - val_acc: 0.8227 - val_auc: 0.7854\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4271 - acc: 0.8150 - auc: 0.7394 - val_loss: 0.3975 - val_acc: 0.8233 - val_auc: 0.7876\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4239 - acc: 0.8159 - auc: 0.7455 - val_loss: 0.3962 - val_acc: 0.8241 - val_auc: 0.7891\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4213 - acc: 0.8169 - auc: 0.7496 - val_loss: 0.3955 - val_acc: 0.8248 - val_auc: 0.7908\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4193 - acc: 0.8177 - auc: 0.7524 - val_loss: 0.3949 - val_acc: 0.8250 - val_auc: 0.7914\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4175 - acc: 0.8181 - auc: 0.7553 - val_loss: 0.3946 - val_acc: 0.8254 - val_auc: 0.7918\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4161 - acc: 0.8189 - auc: 0.7572 - val_loss: 0.3943 - val_acc: 0.8257 - val_auc: 0.7920\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4145 - acc: 0.8194 - auc: 0.7595 - val_loss: 0.3942 - val_acc: 0.8247 - val_auc: 0.7926\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8197 - auc: 0.7609 - val_loss: 0.3942 - val_acc: 0.8259 - val_auc: 0.7934\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4129 - acc: 0.8199 - auc: 0.7623 - val_loss: 0.3939 - val_acc: 0.8261 - val_auc: 0.7927\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8205 - auc: 0.7635 - val_loss: 0.3940 - val_acc: 0.8256 - val_auc: 0.7924\n",
      "Epoch 16/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4110 - acc: 0.8208 - auc: 0.7647\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4111 - acc: 0.8207 - auc: 0.7648 - val_loss: 0.3938 - val_acc: 0.8256 - val_auc: 0.7922\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8210 - auc: 0.7662 - val_loss: 0.3938 - val_acc: 0.8261 - val_auc: 0.7927\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8212 - auc: 0.7657 - val_loss: 0.3937 - val_acc: 0.8265 - val_auc: 0.7938\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8211 - auc: 0.7665 - val_loss: 0.3937 - val_acc: 0.8263 - val_auc: 0.7925\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8209 - auc: 0.7666 - val_loss: 0.3937 - val_acc: 0.8267 - val_auc: 0.7934\n",
      "Epoch 21/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.8215 - auc: 0.7671\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8215 - auc: 0.7672 - val_loss: 0.3937 - val_acc: 0.8267 - val_auc: 0.7930\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4092 - acc: 0.8214 - auc: 0.7676 - val_loss: 0.3936 - val_acc: 0.8266 - val_auc: 0.7934\n",
      "Epoch 23/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8214 - auc: 0.7673Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7672 - val_loss: 0.3936 - val_acc: 0.8265 - val_auc: 0.7930\n",
      "Epoch 00023: early stopping\n",
      "Fold score: 0.792427954466006\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5735 - acc: 0.7161 - auc: 0.5580 - val_loss: 0.5134 - val_acc: 0.8121 - val_auc: 0.6845\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4666 - acc: 0.8063 - auc: 0.6569 - val_loss: 0.4306 - val_acc: 0.8152 - val_auc: 0.7514\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4425 - acc: 0.8122 - auc: 0.7045 - val_loss: 0.4153 - val_acc: 0.8190 - val_auc: 0.7692\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4322 - acc: 0.8144 - auc: 0.7268 - val_loss: 0.4093 - val_acc: 0.8192 - val_auc: 0.7765\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4275 - acc: 0.8158 - auc: 0.7367 - val_loss: 0.4063 - val_acc: 0.8199 - val_auc: 0.7799\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4236 - acc: 0.8166 - auc: 0.7440 - val_loss: 0.4048 - val_acc: 0.8207 - val_auc: 0.7814\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4212 - acc: 0.8172 - auc: 0.7485 - val_loss: 0.4038 - val_acc: 0.8210 - val_auc: 0.7830\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4191 - acc: 0.8177 - auc: 0.7519 - val_loss: 0.4032 - val_acc: 0.8208 - val_auc: 0.7819\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4172 - acc: 0.8186 - auc: 0.7549 - val_loss: 0.4027 - val_acc: 0.8202 - val_auc: 0.7833\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4161 - acc: 0.8187 - auc: 0.7569 - val_loss: 0.4025 - val_acc: 0.8202 - val_auc: 0.7842\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4146 - acc: 0.8190 - auc: 0.7594 - val_loss: 0.4022 - val_acc: 0.8210 - val_auc: 0.7836\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4135 - acc: 0.8197 - auc: 0.7609 - val_loss: 0.4020 - val_acc: 0.8212 - val_auc: 0.7846\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8201 - auc: 0.7621 - val_loss: 0.4017 - val_acc: 0.8211 - val_auc: 0.7844\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8201 - auc: 0.7634 - val_loss: 0.4018 - val_acc: 0.8220 - val_auc: 0.7846\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8210 - auc: 0.7651 - val_loss: 0.4018 - val_acc: 0.8221 - val_auc: 0.7849\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8209 - auc: 0.7655 - val_loss: 0.4014 - val_acc: 0.8217 - val_auc: 0.7844\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8213 - auc: 0.7670 - val_loss: 0.4014 - val_acc: 0.8224 - val_auc: 0.7854\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8211 - auc: 0.7672 - val_loss: 0.4015 - val_acc: 0.8227 - val_auc: 0.7852\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8215 - auc: 0.7686 - val_loss: 0.4012 - val_acc: 0.8228 - val_auc: 0.7849\n",
      "Epoch 20/100\n",
      "586752/588000 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8217 - auc: 0.7690\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8217 - auc: 0.7689 - val_loss: 0.4013 - val_acc: 0.8227 - val_auc: 0.7852\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8217 - auc: 0.7692 - val_loss: 0.4012 - val_acc: 0.8227 - val_auc: 0.7858\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8215 - auc: 0.7691 - val_loss: 0.4012 - val_acc: 0.8227 - val_auc: 0.7850\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8216 - auc: 0.7694 - val_loss: 0.4013 - val_acc: 0.8227 - val_auc: 0.7857\n",
      "Epoch 24/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8219 - auc: 0.7702\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8219 - auc: 0.7702 - val_loss: 0.4013 - val_acc: 0.8229 - val_auc: 0.7848\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7705 - val_loss: 0.4013 - val_acc: 0.8228 - val_auc: 0.7854\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8221 - auc: 0.7700 - val_loss: 0.4012 - val_acc: 0.8228 - val_auc: 0.7850\n",
      "Epoch 27/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8221 - auc: 0.7707Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8221 - auc: 0.7708 - val_loss: 0.4012 - val_acc: 0.8226 - val_auc: 0.7859\n",
      "Epoch 00027: early stopping\n",
      "Fold score: 0.7847966376200342\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6410 - acc: 0.6326 - auc: 0.5536 - val_loss: 0.5280 - val_acc: 0.8130 - val_auc: 0.6776\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4842 - acc: 0.7944 - auc: 0.6394 - val_loss: 0.4312 - val_acc: 0.8170 - val_auc: 0.7482\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4491 - acc: 0.8088 - auc: 0.6944 - val_loss: 0.4110 - val_acc: 0.8215 - val_auc: 0.7739\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4357 - acc: 0.8123 - auc: 0.7219 - val_loss: 0.4025 - val_acc: 0.8228 - val_auc: 0.7824\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4298 - acc: 0.8138 - auc: 0.7344 - val_loss: 0.3989 - val_acc: 0.8239 - val_auc: 0.7865\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4253 - acc: 0.8154 - auc: 0.7429 - val_loss: 0.3971 - val_acc: 0.8245 - val_auc: 0.7889\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4228 - acc: 0.8156 - auc: 0.7474 - val_loss: 0.3962 - val_acc: 0.8262 - val_auc: 0.7894\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4207 - acc: 0.8170 - auc: 0.7504 - val_loss: 0.3954 - val_acc: 0.8266 - val_auc: 0.7913\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4188 - acc: 0.8179 - auc: 0.7532 - val_loss: 0.3950 - val_acc: 0.8262 - val_auc: 0.7922\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4170 - acc: 0.8185 - auc: 0.7561 - val_loss: 0.3947 - val_acc: 0.8264 - val_auc: 0.7919\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8187 - auc: 0.7584 - val_loss: 0.3944 - val_acc: 0.8266 - val_auc: 0.7919\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8192 - auc: 0.7595 - val_loss: 0.3942 - val_acc: 0.8267 - val_auc: 0.7928\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8203 - auc: 0.7610 - val_loss: 0.3941 - val_acc: 0.8264 - val_auc: 0.7942\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4125 - acc: 0.8203 - auc: 0.7628 - val_loss: 0.3939 - val_acc: 0.8266 - val_auc: 0.7939\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8202 - auc: 0.7635 - val_loss: 0.3938 - val_acc: 0.8265 - val_auc: 0.7947\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8204 - auc: 0.7646 - val_loss: 0.3938 - val_acc: 0.8266 - val_auc: 0.7941\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8209 - auc: 0.7661 - val_loss: 0.3936 - val_acc: 0.8267 - val_auc: 0.7948\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8213 - auc: 0.7667 - val_loss: 0.3936 - val_acc: 0.8267 - val_auc: 0.7940\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8211 - auc: 0.7669 - val_loss: 0.3936 - val_acc: 0.8261 - val_auc: 0.7944\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8210 - auc: 0.7678 - val_loss: 0.3935 - val_acc: 0.8263 - val_auc: 0.7954\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8215 - auc: 0.7684 - val_loss: 0.3935 - val_acc: 0.8263 - val_auc: 0.7954\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8218 - auc: 0.7691 - val_loss: 0.3933 - val_acc: 0.8263 - val_auc: 0.7949\n",
      "Epoch 23/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8221 - auc: 0.7696\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8222 - auc: 0.7699 - val_loss: 0.3933 - val_acc: 0.8262 - val_auc: 0.7938\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8221 - auc: 0.7703 - val_loss: 0.3932 - val_acc: 0.8263 - val_auc: 0.7950\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8221 - auc: 0.7700 - val_loss: 0.3933 - val_acc: 0.8265 - val_auc: 0.7955\n",
      "Epoch 26/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8220 - auc: 0.7703\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8221 - auc: 0.7704 - val_loss: 0.3932 - val_acc: 0.8268 - val_auc: 0.7948\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8222 - auc: 0.7705 - val_loss: 0.3932 - val_acc: 0.8269 - val_auc: 0.7955\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8220 - auc: 0.7709 - val_loss: 0.3932 - val_acc: 0.8268 - val_auc: 0.7959\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8224 - auc: 0.7714 - val_loss: 0.3932 - val_acc: 0.8267 - val_auc: 0.7953\n",
      "Epoch 30/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8224 - auc: 0.7706Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4071 - acc: 0.8224 - auc: 0.7709 - val_loss: 0.3932 - val_acc: 0.8268 - val_auc: 0.7947\n",
      "Epoch 00030: early stopping\n",
      "Fold score: 0.7946270952103573\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.7127 - acc: 0.5559 - auc: 0.5418 - val_loss: 0.5465 - val_acc: 0.8108 - val_auc: 0.6673\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.5003 - acc: 0.7828 - auc: 0.6280 - val_loss: 0.4369 - val_acc: 0.8156 - val_auc: 0.7403\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4554 - acc: 0.8072 - auc: 0.6841 - val_loss: 0.4169 - val_acc: 0.8190 - val_auc: 0.7634\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4400 - acc: 0.8116 - auc: 0.7143 - val_loss: 0.4086 - val_acc: 0.8211 - val_auc: 0.7733\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4330 - acc: 0.8132 - auc: 0.7291 - val_loss: 0.4050 - val_acc: 0.8214 - val_auc: 0.7766\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4285 - acc: 0.8145 - auc: 0.7380 - val_loss: 0.4034 - val_acc: 0.8227 - val_auc: 0.7801\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4251 - acc: 0.8154 - auc: 0.7438 - val_loss: 0.4026 - val_acc: 0.8240 - val_auc: 0.7794\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4222 - acc: 0.8166 - auc: 0.7484 - val_loss: 0.4021 - val_acc: 0.8246 - val_auc: 0.7794\n",
      "Epoch 9/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8171 - auc: 0.7512\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4204 - acc: 0.8171 - auc: 0.7512 - val_loss: 0.4018 - val_acc: 0.8239 - val_auc: 0.7798\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4190 - acc: 0.8179 - auc: 0.7533 - val_loss: 0.4017 - val_acc: 0.8248 - val_auc: 0.7809\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4182 - acc: 0.8182 - auc: 0.7543 - val_loss: 0.4016 - val_acc: 0.8239 - val_auc: 0.7812\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4172 - acc: 0.8183 - auc: 0.7556 - val_loss: 0.4015 - val_acc: 0.8238 - val_auc: 0.7814\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4162 - acc: 0.8186 - auc: 0.7570 - val_loss: 0.4013 - val_acc: 0.8242 - val_auc: 0.7808\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8191 - auc: 0.7582 - val_loss: 0.4013 - val_acc: 0.8240 - val_auc: 0.7818\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8191 - auc: 0.7582 - val_loss: 0.4013 - val_acc: 0.8238 - val_auc: 0.7808\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4138 - acc: 0.8196 - auc: 0.7607 - val_loss: 0.4011 - val_acc: 0.8232 - val_auc: 0.7821\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4137 - acc: 0.8202 - auc: 0.7608 - val_loss: 0.4011 - val_acc: 0.8234 - val_auc: 0.7819\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4133 - acc: 0.8199 - auc: 0.7612 - val_loss: 0.4011 - val_acc: 0.8233 - val_auc: 0.7822\n",
      "Epoch 19/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8202 - auc: 0.7623\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8202 - auc: 0.7623 - val_loss: 0.4010 - val_acc: 0.8241 - val_auc: 0.7819\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4126 - acc: 0.8203 - auc: 0.7623 - val_loss: 0.4010 - val_acc: 0.8236 - val_auc: 0.7816\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8205 - auc: 0.7632 - val_loss: 0.4009 - val_acc: 0.8237 - val_auc: 0.7817\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8205 - auc: 0.7640 - val_loss: 0.4009 - val_acc: 0.8235 - val_auc: 0.7823\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8210 - auc: 0.7635 - val_loss: 0.4009 - val_acc: 0.8233 - val_auc: 0.7817\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4115 - acc: 0.8205 - auc: 0.7642 - val_loss: 0.4009 - val_acc: 0.8236 - val_auc: 0.7816\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8207 - auc: 0.7644 - val_loss: 0.4008 - val_acc: 0.8242 - val_auc: 0.7826\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8209 - auc: 0.7643 - val_loss: 0.4008 - val_acc: 0.8240 - val_auc: 0.7823\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8213 - auc: 0.7650 - val_loss: 0.4008 - val_acc: 0.8239 - val_auc: 0.7826\n",
      "Epoch 28/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8209 - auc: 0.7648Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8209 - auc: 0.7647 - val_loss: 0.4008 - val_acc: 0.8238 - val_auc: 0.7831\n",
      "Epoch 00028: early stopping\n",
      "Fold score: 0.7817999920835882\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6278 - acc: 0.6511 - auc: 0.5647 - val_loss: 0.5204 - val_acc: 0.8122 - val_auc: 0.6726\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4799 - acc: 0.7964 - auc: 0.6442 - val_loss: 0.4319 - val_acc: 0.8162 - val_auc: 0.7394\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4483 - acc: 0.8090 - auc: 0.6968 - val_loss: 0.4159 - val_acc: 0.8202 - val_auc: 0.7610\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4373 - acc: 0.8119 - auc: 0.7206 - val_loss: 0.4094 - val_acc: 0.8225 - val_auc: 0.7691\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4312 - acc: 0.8130 - auc: 0.7332 - val_loss: 0.4065 - val_acc: 0.8227 - val_auc: 0.7745\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4274 - acc: 0.8149 - auc: 0.7399 - val_loss: 0.4047 - val_acc: 0.8227 - val_auc: 0.7762\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4244 - acc: 0.8152 - auc: 0.7450 - val_loss: 0.4037 - val_acc: 0.8228 - val_auc: 0.7780\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4216 - acc: 0.8165 - auc: 0.7494 - val_loss: 0.4028 - val_acc: 0.8238 - val_auc: 0.7797\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4196 - acc: 0.8175 - auc: 0.7522 - val_loss: 0.4022 - val_acc: 0.8242 - val_auc: 0.7811\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4181 - acc: 0.8177 - auc: 0.7548 - val_loss: 0.4020 - val_acc: 0.8248 - val_auc: 0.7804\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4160 - acc: 0.8187 - auc: 0.7575 - val_loss: 0.4018 - val_acc: 0.8242 - val_auc: 0.7808\n",
      "Epoch 12/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8192 - auc: 0.7591\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4150 - acc: 0.8192 - auc: 0.7589 - val_loss: 0.4015 - val_acc: 0.8243 - val_auc: 0.7812\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4144 - acc: 0.8195 - auc: 0.7599 - val_loss: 0.4015 - val_acc: 0.8246 - val_auc: 0.7800\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4136 - acc: 0.8197 - auc: 0.7611 - val_loss: 0.4013 - val_acc: 0.8247 - val_auc: 0.7817\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4129 - acc: 0.8198 - auc: 0.7620 - val_loss: 0.4012 - val_acc: 0.8249 - val_auc: 0.7815\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4124 - acc: 0.8204 - auc: 0.7631 - val_loss: 0.4011 - val_acc: 0.8244 - val_auc: 0.7818\n",
      "Epoch 17/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8201 - auc: 0.7643\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8201 - auc: 0.7643 - val_loss: 0.4010 - val_acc: 0.8246 - val_auc: 0.7817\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8203 - auc: 0.7634 - val_loss: 0.4010 - val_acc: 0.8244 - val_auc: 0.7825\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8205 - auc: 0.7638 - val_loss: 0.4010 - val_acc: 0.8243 - val_auc: 0.7817\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4119 - acc: 0.8203 - auc: 0.7637 - val_loss: 0.4010 - val_acc: 0.8248 - val_auc: 0.7820\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4116 - acc: 0.8204 - auc: 0.7643 - val_loss: 0.4008 - val_acc: 0.8247 - val_auc: 0.7828\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8207 - auc: 0.7653 - val_loss: 0.4009 - val_acc: 0.8249 - val_auc: 0.7827\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8208 - auc: 0.7652 - val_loss: 0.4008 - val_acc: 0.8248 - val_auc: 0.7820\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8209 - auc: 0.7657 - val_loss: 0.4008 - val_acc: 0.8248 - val_auc: 0.7831\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8209 - auc: 0.7657 - val_loss: 0.4007 - val_acc: 0.8248 - val_auc: 0.7832\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8210 - auc: 0.7658 - val_loss: 0.4007 - val_acc: 0.8249 - val_auc: 0.7834\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8207 - auc: 0.7664 - val_loss: 0.4007 - val_acc: 0.8248 - val_auc: 0.7840\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8210 - auc: 0.7666 - val_loss: 0.4007 - val_acc: 0.8248 - val_auc: 0.7832\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8207 - auc: 0.7667 - val_loss: 0.4007 - val_acc: 0.8248 - val_auc: 0.7827\n",
      "Epoch 30/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8210 - auc: 0.7661\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4101 - acc: 0.8211 - auc: 0.7660 - val_loss: 0.4006 - val_acc: 0.8245 - val_auc: 0.7839\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8210 - auc: 0.7672 - val_loss: 0.4007 - val_acc: 0.8248 - val_auc: 0.7826\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8214 - auc: 0.7668 - val_loss: 0.4007 - val_acc: 0.8248 - val_auc: 0.7832\n",
      "Epoch 33/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8212 - auc: 0.7670\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8212 - auc: 0.7669 - val_loss: 0.4006 - val_acc: 0.8247 - val_auc: 0.7834\n",
      "Epoch 34/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8215 - auc: 0.7674 - val_loss: 0.4006 - val_acc: 0.8247 - val_auc: 0.7830\n",
      "Epoch 35/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8211 - auc: 0.7679 - val_loss: 0.4006 - val_acc: 0.8247 - val_auc: 0.7831\n",
      "Epoch 36/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8213 - auc: 0.7676\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7676 - val_loss: 0.4006 - val_acc: 0.8248 - val_auc: 0.7826\n",
      "Epoch 37/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8214 - auc: 0.7673Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 6us/sample - loss: 0.4095 - acc: 0.8213 - auc: 0.7673 - val_loss: 0.4006 - val_acc: 0.8245 - val_auc: 0.7821\n",
      "Epoch 00037: early stopping\n",
      "Fold score: 0.7829166453823944\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.5458 - acc: 0.7483 - auc: 0.5543 - val_loss: 0.5020 - val_acc: 0.8093 - val_auc: 0.6739\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4689 - acc: 0.8061 - auc: 0.6453 - val_loss: 0.4364 - val_acc: 0.8129 - val_auc: 0.7374\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4468 - acc: 0.8110 - auc: 0.6968 - val_loss: 0.4216 - val_acc: 0.8173 - val_auc: 0.7605\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4366 - acc: 0.8129 - auc: 0.7202 - val_loss: 0.4139 - val_acc: 0.8188 - val_auc: 0.7709\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4303 - acc: 0.8143 - auc: 0.7337 - val_loss: 0.4105 - val_acc: 0.8201 - val_auc: 0.7749\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4261 - acc: 0.8154 - auc: 0.7412 - val_loss: 0.4088 - val_acc: 0.8200 - val_auc: 0.7775\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4229 - acc: 0.8164 - auc: 0.7467 - val_loss: 0.4077 - val_acc: 0.8209 - val_auc: 0.7795\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4209 - acc: 0.8172 - auc: 0.7497 - val_loss: 0.4072 - val_acc: 0.8215 - val_auc: 0.7785\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4188 - acc: 0.8179 - auc: 0.7528 - val_loss: 0.4067 - val_acc: 0.8211 - val_auc: 0.7799\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4171 - acc: 0.8180 - auc: 0.7556 - val_loss: 0.4064 - val_acc: 0.8214 - val_auc: 0.7813\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4156 - acc: 0.8186 - auc: 0.7577 - val_loss: 0.4059 - val_acc: 0.8217 - val_auc: 0.7824\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4146 - acc: 0.8196 - auc: 0.7588 - val_loss: 0.4054 - val_acc: 0.8212 - val_auc: 0.7831\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4133 - acc: 0.8196 - auc: 0.7612 - val_loss: 0.4055 - val_acc: 0.8217 - val_auc: 0.7827\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8202 - auc: 0.7628 - val_loss: 0.4052 - val_acc: 0.8217 - val_auc: 0.7821\n",
      "Epoch 15/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.8203 - auc: 0.7635\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8203 - auc: 0.7636 - val_loss: 0.4050 - val_acc: 0.8220 - val_auc: 0.7826\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8205 - auc: 0.7649 - val_loss: 0.4049 - val_acc: 0.8220 - val_auc: 0.7842\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8209 - auc: 0.7656 - val_loss: 0.4048 - val_acc: 0.8219 - val_auc: 0.7834\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8209 - auc: 0.7658 - val_loss: 0.4047 - val_acc: 0.8223 - val_auc: 0.7838\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8212 - auc: 0.7667 - val_loss: 0.4047 - val_acc: 0.8220 - val_auc: 0.7844\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8212 - auc: 0.7668 - val_loss: 0.4047 - val_acc: 0.8224 - val_auc: 0.7845\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8215 - auc: 0.7669 - val_loss: 0.4047 - val_acc: 0.8223 - val_auc: 0.7845\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8214 - auc: 0.7681 - val_loss: 0.4046 - val_acc: 0.8222 - val_auc: 0.7836\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8214 - auc: 0.7683 - val_loss: 0.4045 - val_acc: 0.8223 - val_auc: 0.7857\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8214 - auc: 0.7680 - val_loss: 0.4044 - val_acc: 0.8218 - val_auc: 0.7838\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8215 - auc: 0.7689 - val_loss: 0.4044 - val_acc: 0.8217 - val_auc: 0.7854\n",
      "Epoch 26/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8218 - auc: 0.7687\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8218 - auc: 0.7687 - val_loss: 0.4044 - val_acc: 0.8220 - val_auc: 0.7858\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8216 - auc: 0.7690 - val_loss: 0.4043 - val_acc: 0.8217 - val_auc: 0.7837\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8218 - auc: 0.7695 - val_loss: 0.4042 - val_acc: 0.8219 - val_auc: 0.7849\n",
      "Epoch 29/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8217 - auc: 0.7692\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7692 - val_loss: 0.4043 - val_acc: 0.8217 - val_auc: 0.7852\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8214 - auc: 0.7698 - val_loss: 0.4043 - val_acc: 0.8219 - val_auc: 0.7859\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8222 - auc: 0.7700 - val_loss: 0.4042 - val_acc: 0.8218 - val_auc: 0.7851\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8217 - auc: 0.7703 - val_loss: 0.4042 - val_acc: 0.8217 - val_auc: 0.7856\n",
      "Epoch 33/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8214 - auc: 0.7697Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8215 - auc: 0.7696 - val_loss: 0.4042 - val_acc: 0.8219 - val_auc: 0.7841\n",
      "Epoch 00033: early stopping\n",
      "Fold score: 0.7843469406971121\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6606 - acc: 0.6072 - auc: 0.5556 - val_loss: 0.5406 - val_acc: 0.8079 - val_auc: 0.6686\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4841 - acc: 0.7954 - auc: 0.6476 - val_loss: 0.4370 - val_acc: 0.8123 - val_auc: 0.7423\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4468 - acc: 0.8105 - auc: 0.6987 - val_loss: 0.4185 - val_acc: 0.8151 - val_auc: 0.7651\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4352 - acc: 0.8129 - auc: 0.7221 - val_loss: 0.4109 - val_acc: 0.8198 - val_auc: 0.7753\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4291 - acc: 0.8148 - auc: 0.7344 - val_loss: 0.4071 - val_acc: 0.8217 - val_auc: 0.7779\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4249 - acc: 0.8156 - auc: 0.7424 - val_loss: 0.4053 - val_acc: 0.8223 - val_auc: 0.7808\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4223 - acc: 0.8163 - auc: 0.7474 - val_loss: 0.4042 - val_acc: 0.8224 - val_auc: 0.7815\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4201 - acc: 0.8174 - auc: 0.7507 - val_loss: 0.4036 - val_acc: 0.8232 - val_auc: 0.7834\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4177 - acc: 0.8182 - auc: 0.7545 - val_loss: 0.4032 - val_acc: 0.8237 - val_auc: 0.7831\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4172 - acc: 0.8186 - auc: 0.7555 - val_loss: 0.4030 - val_acc: 0.8234 - val_auc: 0.7842\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8190 - auc: 0.7581 - val_loss: 0.4027 - val_acc: 0.8243 - val_auc: 0.7836\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8192 - auc: 0.7599 - val_loss: 0.4024 - val_acc: 0.8251 - val_auc: 0.7843\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4129 - acc: 0.8203 - auc: 0.7619 - val_loss: 0.4022 - val_acc: 0.8256 - val_auc: 0.7845\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4126 - acc: 0.8199 - auc: 0.7622 - val_loss: 0.4022 - val_acc: 0.8254 - val_auc: 0.7850\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8205 - auc: 0.7640 - val_loss: 0.4020 - val_acc: 0.8259 - val_auc: 0.7850\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8209 - auc: 0.7651 - val_loss: 0.4019 - val_acc: 0.8253 - val_auc: 0.7861\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8211 - auc: 0.7666 - val_loss: 0.4018 - val_acc: 0.8257 - val_auc: 0.7860\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8213 - auc: 0.7669 - val_loss: 0.4017 - val_acc: 0.8263 - val_auc: 0.7857\n",
      "Epoch 19/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8216 - auc: 0.7671\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8216 - auc: 0.7672 - val_loss: 0.4018 - val_acc: 0.8260 - val_auc: 0.7853\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8212 - auc: 0.7680 - val_loss: 0.4016 - val_acc: 0.8262 - val_auc: 0.7855\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8217 - auc: 0.7678 - val_loss: 0.4017 - val_acc: 0.8263 - val_auc: 0.7851\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8217 - auc: 0.7683 - val_loss: 0.4017 - val_acc: 0.8264 - val_auc: 0.7862\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8218 - auc: 0.7691 - val_loss: 0.4017 - val_acc: 0.8263 - val_auc: 0.7856\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4080 - acc: 0.8218 - auc: 0.7691 - val_loss: 0.4016 - val_acc: 0.8267 - val_auc: 0.7856\n",
      "Epoch 25/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8220 - auc: 0.7695\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8220 - auc: 0.7696 - val_loss: 0.4017 - val_acc: 0.8266 - val_auc: 0.7850\n",
      "Epoch 26/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8219 - auc: 0.7704Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7704 - val_loss: 0.4016 - val_acc: 0.8264 - val_auc: 0.7864\n",
      "Epoch 00026: early stopping\n",
      "Fold score: 0.7851613550304483\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5144 - acc: 0.7807 - auc: 0.5755 - val_loss: 0.4860 - val_acc: 0.8166 - val_auc: 0.7051\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4580 - acc: 0.8089 - auc: 0.6687 - val_loss: 0.4184 - val_acc: 0.8213 - val_auc: 0.7584\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4395 - acc: 0.8124 - auc: 0.7128 - val_loss: 0.4075 - val_acc: 0.8241 - val_auc: 0.7706\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4316 - acc: 0.8140 - auc: 0.7302 - val_loss: 0.4031 - val_acc: 0.8250 - val_auc: 0.7752\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4270 - acc: 0.8149 - auc: 0.7393 - val_loss: 0.4013 - val_acc: 0.8261 - val_auc: 0.7772\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4238 - acc: 0.8161 - auc: 0.7450 - val_loss: 0.4003 - val_acc: 0.8271 - val_auc: 0.7779\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4208 - acc: 0.8170 - auc: 0.7502 - val_loss: 0.3995 - val_acc: 0.8274 - val_auc: 0.7795\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4196 - acc: 0.8177 - auc: 0.7518 - val_loss: 0.3992 - val_acc: 0.8272 - val_auc: 0.7801\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4171 - acc: 0.8184 - auc: 0.7559 - val_loss: 0.3988 - val_acc: 0.8276 - val_auc: 0.7801\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4159 - acc: 0.8186 - auc: 0.7576 - val_loss: 0.3984 - val_acc: 0.8275 - val_auc: 0.7813\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4144 - acc: 0.8191 - auc: 0.7599 - val_loss: 0.3980 - val_acc: 0.8276 - val_auc: 0.7820\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4130 - acc: 0.8196 - auc: 0.7621 - val_loss: 0.3981 - val_acc: 0.8280 - val_auc: 0.7823\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4121 - acc: 0.8201 - auc: 0.7632 - val_loss: 0.3979 - val_acc: 0.8286 - val_auc: 0.7822\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4117 - acc: 0.8204 - auc: 0.7639 - val_loss: 0.3975 - val_acc: 0.8284 - val_auc: 0.7825\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8207 - auc: 0.7656 - val_loss: 0.3974 - val_acc: 0.8287 - val_auc: 0.7823\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8206 - auc: 0.7662 - val_loss: 0.3974 - val_acc: 0.8284 - val_auc: 0.7825\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8212 - auc: 0.7670 - val_loss: 0.3974 - val_acc: 0.8283 - val_auc: 0.7836\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8214 - auc: 0.7677 - val_loss: 0.3972 - val_acc: 0.8285 - val_auc: 0.7826\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8215 - auc: 0.7684 - val_loss: 0.3972 - val_acc: 0.8286 - val_auc: 0.7826\n",
      "Epoch 20/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8217 - auc: 0.7694\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8218 - auc: 0.7696 - val_loss: 0.3968 - val_acc: 0.8288 - val_auc: 0.7828\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8219 - auc: 0.7694 - val_loss: 0.3970 - val_acc: 0.8283 - val_auc: 0.7830\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8219 - auc: 0.7708 - val_loss: 0.3970 - val_acc: 0.8283 - val_auc: 0.7841\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8218 - auc: 0.7706 - val_loss: 0.3969 - val_acc: 0.8283 - val_auc: 0.7832\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4075 - acc: 0.8218 - auc: 0.7704 - val_loss: 0.3968 - val_acc: 0.8288 - val_auc: 0.7832\n",
      "Epoch 25/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8220 - auc: 0.7711\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8221 - auc: 0.7711 - val_loss: 0.3969 - val_acc: 0.8286 - val_auc: 0.7825\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8222 - auc: 0.7705 - val_loss: 0.3969 - val_acc: 0.8283 - val_auc: 0.7853\n",
      "Epoch 27/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4076 - acc: 0.8220 - auc: 0.7703 - val_loss: 0.3970 - val_acc: 0.8284 - val_auc: 0.7832\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4072 - acc: 0.8221 - auc: 0.7709 - val_loss: 0.3970 - val_acc: 0.8282 - val_auc: 0.7848\n",
      "Epoch 29/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8223 - auc: 0.7714\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8223 - auc: 0.7713 - val_loss: 0.3969 - val_acc: 0.8284 - val_auc: 0.7841\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8222 - auc: 0.7712 - val_loss: 0.3968 - val_acc: 0.8287 - val_auc: 0.7832\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8220 - auc: 0.7708 - val_loss: 0.3969 - val_acc: 0.8287 - val_auc: 0.7840\n",
      "Epoch 32/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8220 - auc: 0.7714\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4068 - acc: 0.8220 - auc: 0.7714 - val_loss: 0.3969 - val_acc: 0.8280 - val_auc: 0.7832\n",
      "Epoch 33/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4067 - acc: 0.8223 - auc: 0.7717 - val_loss: 0.3968 - val_acc: 0.8287 - val_auc: 0.7827\n",
      "Epoch 34/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4069 - acc: 0.8220 - auc: 0.7715 - val_loss: 0.3968 - val_acc: 0.8286 - val_auc: 0.7843\n",
      "Epoch 35/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8220 - auc: 0.7712\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8220 - auc: 0.7713 - val_loss: 0.3968 - val_acc: 0.8286 - val_auc: 0.7840\n",
      "Epoch 36/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8220 - auc: 0.7712Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4070 - acc: 0.8220 - auc: 0.7713 - val_loss: 0.3970 - val_acc: 0.8280 - val_auc: 0.7828\n",
      "Epoch 00036: early stopping\n",
      "Fold score: 0.7834851808905382\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.5812 - acc: 0.7042 - auc: 0.5729 - val_loss: 0.5084 - val_acc: 0.8209 - val_auc: 0.6861\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4653 - acc: 0.8048 - auc: 0.6654 - val_loss: 0.4173 - val_acc: 0.8247 - val_auc: 0.7550\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4397 - acc: 0.8120 - auc: 0.7123 - val_loss: 0.4025 - val_acc: 0.8295 - val_auc: 0.7712\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4318 - acc: 0.8141 - auc: 0.7297 - val_loss: 0.3970 - val_acc: 0.8292 - val_auc: 0.7778\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4265 - acc: 0.8156 - auc: 0.7401 - val_loss: 0.3945 - val_acc: 0.8310 - val_auc: 0.7817\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4239 - acc: 0.8160 - auc: 0.7454 - val_loss: 0.3933 - val_acc: 0.8305 - val_auc: 0.7829\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4211 - acc: 0.8173 - auc: 0.7498 - val_loss: 0.3925 - val_acc: 0.8303 - val_auc: 0.7821\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4193 - acc: 0.8177 - auc: 0.7527 - val_loss: 0.3920 - val_acc: 0.8302 - val_auc: 0.7841\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4175 - acc: 0.8183 - auc: 0.7553 - val_loss: 0.3919 - val_acc: 0.8302 - val_auc: 0.7844\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4162 - acc: 0.8185 - auc: 0.7573 - val_loss: 0.3915 - val_acc: 0.8307 - val_auc: 0.7854\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4149 - acc: 0.8192 - auc: 0.7592 - val_loss: 0.3915 - val_acc: 0.8307 - val_auc: 0.7834\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4137 - acc: 0.8197 - auc: 0.7612 - val_loss: 0.3911 - val_acc: 0.8307 - val_auc: 0.7862\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4129 - acc: 0.8196 - auc: 0.7625 - val_loss: 0.3909 - val_acc: 0.8302 - val_auc: 0.7858\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4123 - acc: 0.8200 - auc: 0.7632 - val_loss: 0.3906 - val_acc: 0.8304 - val_auc: 0.7860\n",
      "Epoch 15/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8205 - auc: 0.7646\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8205 - auc: 0.7647 - val_loss: 0.3907 - val_acc: 0.8298 - val_auc: 0.7862\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4107 - acc: 0.8205 - auc: 0.7657 - val_loss: 0.3905 - val_acc: 0.8297 - val_auc: 0.7867\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8205 - auc: 0.7655 - val_loss: 0.3906 - val_acc: 0.8299 - val_auc: 0.7870\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8205 - auc: 0.7662 - val_loss: 0.3902 - val_acc: 0.8299 - val_auc: 0.7866\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8213 - auc: 0.7668 - val_loss: 0.3903 - val_acc: 0.8302 - val_auc: 0.7873\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8211 - auc: 0.7673 - val_loss: 0.3901 - val_acc: 0.8300 - val_auc: 0.7865\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8209 - auc: 0.7678 - val_loss: 0.3902 - val_acc: 0.8302 - val_auc: 0.7885\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4091 - acc: 0.8212 - auc: 0.7682 - val_loss: 0.3901 - val_acc: 0.8303 - val_auc: 0.7876\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8214 - auc: 0.7673 - val_loss: 0.3899 - val_acc: 0.8302 - val_auc: 0.7881\n",
      "Epoch 24/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8212 - auc: 0.7690\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8212 - auc: 0.7691 - val_loss: 0.3901 - val_acc: 0.8303 - val_auc: 0.7879\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8215 - auc: 0.7691 - val_loss: 0.3900 - val_acc: 0.8303 - val_auc: 0.7876\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8214 - auc: 0.7692 - val_loss: 0.3900 - val_acc: 0.8301 - val_auc: 0.7882\n",
      "Epoch 27/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8217 - auc: 0.7696\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8217 - auc: 0.7695 - val_loss: 0.3900 - val_acc: 0.8300 - val_auc: 0.7880\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8216 - auc: 0.7692 - val_loss: 0.3899 - val_acc: 0.8298 - val_auc: 0.7877\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8220 - auc: 0.7696 - val_loss: 0.3899 - val_acc: 0.8299 - val_auc: 0.7878\n",
      "Epoch 30/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8219 - auc: 0.7694\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8218 - auc: 0.7694 - val_loss: 0.3900 - val_acc: 0.8298 - val_auc: 0.7884\n",
      "Epoch 31/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8220 - auc: 0.7702Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8220 - auc: 0.7702 - val_loss: 0.3899 - val_acc: 0.8298 - val_auc: 0.7871\n",
      "Epoch 00031: early stopping\n",
      "Fold score: 0.7876102658485765\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 7us/sample - loss: 0.6553 - acc: 0.6208 - auc: 0.5578 - val_loss: 0.5266 - val_acc: 0.8082 - val_auc: 0.6805\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4853 - acc: 0.7919 - auc: 0.6424 - val_loss: 0.4330 - val_acc: 0.8129 - val_auc: 0.7514\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4502 - acc: 0.8075 - auc: 0.6947 - val_loss: 0.4158 - val_acc: 0.8167 - val_auc: 0.7729\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4384 - acc: 0.8112 - auc: 0.7182 - val_loss: 0.4089 - val_acc: 0.8185 - val_auc: 0.7812\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4327 - acc: 0.8131 - auc: 0.7302 - val_loss: 0.4056 - val_acc: 0.8190 - val_auc: 0.7841\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4282 - acc: 0.8148 - auc: 0.7381 - val_loss: 0.4041 - val_acc: 0.8195 - val_auc: 0.7862\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4254 - acc: 0.8158 - auc: 0.7428 - val_loss: 0.4031 - val_acc: 0.8199 - val_auc: 0.7859\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4224 - acc: 0.8164 - auc: 0.7476 - val_loss: 0.4024 - val_acc: 0.8202 - val_auc: 0.7886\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4205 - acc: 0.8172 - auc: 0.7506 - val_loss: 0.4018 - val_acc: 0.8207 - val_auc: 0.7886\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4184 - acc: 0.8179 - auc: 0.7539 - val_loss: 0.4014 - val_acc: 0.8207 - val_auc: 0.7895\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4170 - acc: 0.8181 - auc: 0.7559 - val_loss: 0.4011 - val_acc: 0.8213 - val_auc: 0.7893\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4159 - acc: 0.8192 - auc: 0.7573 - val_loss: 0.4008 - val_acc: 0.8215 - val_auc: 0.7893\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4143 - acc: 0.8196 - auc: 0.7596 - val_loss: 0.4005 - val_acc: 0.8216 - val_auc: 0.7911\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4139 - acc: 0.8199 - auc: 0.7605 - val_loss: 0.4004 - val_acc: 0.8216 - val_auc: 0.7918\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4129 - acc: 0.8200 - auc: 0.7616 - val_loss: 0.4004 - val_acc: 0.8211 - val_auc: 0.7910\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4115 - acc: 0.8206 - auc: 0.7638 - val_loss: 0.4001 - val_acc: 0.8213 - val_auc: 0.7922\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8211 - auc: 0.7647 - val_loss: 0.3999 - val_acc: 0.8213 - val_auc: 0.7915\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8209 - auc: 0.7655 - val_loss: 0.3997 - val_acc: 0.8215 - val_auc: 0.7914\n",
      "Epoch 19/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8211 - auc: 0.7668\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4096 - acc: 0.8212 - auc: 0.7669 - val_loss: 0.3996 - val_acc: 0.8218 - val_auc: 0.7911\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4090 - acc: 0.8212 - auc: 0.7679 - val_loss: 0.3995 - val_acc: 0.8216 - val_auc: 0.7917\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4088 - acc: 0.8218 - auc: 0.7681 - val_loss: 0.3995 - val_acc: 0.8221 - val_auc: 0.7916\n",
      "Epoch 22/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8218 - auc: 0.7685\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4086 - acc: 0.8218 - auc: 0.7685 - val_loss: 0.3994 - val_acc: 0.8221 - val_auc: 0.7923\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8217 - auc: 0.7691 - val_loss: 0.3994 - val_acc: 0.8218 - val_auc: 0.7918\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4083 - acc: 0.8219 - auc: 0.7690 - val_loss: 0.3994 - val_acc: 0.8218 - val_auc: 0.7929\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8220 - auc: 0.7691 - val_loss: 0.3994 - val_acc: 0.8221 - val_auc: 0.7920\n",
      "Epoch 26/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8222 - auc: 0.7690Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4082 - acc: 0.8222 - auc: 0.7691 - val_loss: 0.3994 - val_acc: 0.8222 - val_auc: 0.7925\n",
      "Epoch 00026: early stopping\n",
      "Fold score: 0.791416590372204\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5684 - acc: 0.7235 - auc: 0.5410 - val_loss: 0.5021 - val_acc: 0.8155 - val_auc: 0.6885\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4680 - acc: 0.8054 - auc: 0.6514 - val_loss: 0.4224 - val_acc: 0.8192 - val_auc: 0.7542\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4440 - acc: 0.8111 - auc: 0.7036 - val_loss: 0.4086 - val_acc: 0.8222 - val_auc: 0.7713\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4349 - acc: 0.8127 - auc: 0.7238 - val_loss: 0.4028 - val_acc: 0.8250 - val_auc: 0.7778\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4289 - acc: 0.8148 - auc: 0.7359 - val_loss: 0.4000 - val_acc: 0.8251 - val_auc: 0.7803\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4255 - acc: 0.8154 - auc: 0.7423 - val_loss: 0.3987 - val_acc: 0.8254 - val_auc: 0.7819\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4222 - acc: 0.8163 - auc: 0.7478 - val_loss: 0.3981 - val_acc: 0.8253 - val_auc: 0.7827\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4206 - acc: 0.8170 - auc: 0.7503 - val_loss: 0.3975 - val_acc: 0.8253 - val_auc: 0.7836\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4178 - acc: 0.8180 - auc: 0.7546 - val_loss: 0.3974 - val_acc: 0.8254 - val_auc: 0.7834\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4171 - acc: 0.8184 - auc: 0.7557 - val_loss: 0.3973 - val_acc: 0.8252 - val_auc: 0.7851\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4153 - acc: 0.8191 - auc: 0.7584 - val_loss: 0.3968 - val_acc: 0.8258 - val_auc: 0.7843\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4141 - acc: 0.8196 - auc: 0.7601 - val_loss: 0.3967 - val_acc: 0.8258 - val_auc: 0.7848\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4133 - acc: 0.8199 - auc: 0.7615 - val_loss: 0.3968 - val_acc: 0.8257 - val_auc: 0.7867\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4123 - acc: 0.8202 - auc: 0.7632 - val_loss: 0.3967 - val_acc: 0.8256 - val_auc: 0.7849\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8206 - auc: 0.7644 - val_loss: 0.3964 - val_acc: 0.8254 - val_auc: 0.7860\n",
      "Epoch 16/100\n",
      "582656/588000 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8208 - auc: 0.7652\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8207 - auc: 0.7652 - val_loss: 0.3963 - val_acc: 0.8253 - val_auc: 0.7860\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4103 - acc: 0.8208 - auc: 0.7661 - val_loss: 0.3963 - val_acc: 0.8260 - val_auc: 0.7855\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4100 - acc: 0.8205 - auc: 0.7666 - val_loss: 0.3963 - val_acc: 0.8263 - val_auc: 0.7863\n",
      "Epoch 19/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8212 - auc: 0.7670\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4097 - acc: 0.8212 - auc: 0.7670 - val_loss: 0.3962 - val_acc: 0.8262 - val_auc: 0.7854\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8214 - auc: 0.7678 - val_loss: 0.3962 - val_acc: 0.8264 - val_auc: 0.7861\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8212 - auc: 0.7671 - val_loss: 0.3962 - val_acc: 0.8263 - val_auc: 0.7861\n",
      "Epoch 22/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8213 - auc: 0.7678\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4094 - acc: 0.8213 - auc: 0.7677 - val_loss: 0.3962 - val_acc: 0.8264 - val_auc: 0.7866\n",
      "Epoch 23/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8214 - auc: 0.7676Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8214 - auc: 0.7677 - val_loss: 0.3962 - val_acc: 0.8263 - val_auc: 0.7869\n",
      "Epoch 00023: early stopping\n",
      "Fold score: 0.7852839557907928\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6373 - acc: 0.6395 - auc: 0.5393 - val_loss: 0.5272 - val_acc: 0.8175 - val_auc: 0.6485\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4821 - acc: 0.7981 - auc: 0.6374 - val_loss: 0.4304 - val_acc: 0.8207 - val_auc: 0.7328\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4493 - acc: 0.8096 - auc: 0.6931 - val_loss: 0.4131 - val_acc: 0.8228 - val_auc: 0.7539\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4379 - acc: 0.8121 - auc: 0.7180 - val_loss: 0.4058 - val_acc: 0.8246 - val_auc: 0.7648\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4309 - acc: 0.8142 - auc: 0.7322 - val_loss: 0.4019 - val_acc: 0.8272 - val_auc: 0.7696\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4271 - acc: 0.8154 - auc: 0.7398 - val_loss: 0.3998 - val_acc: 0.8271 - val_auc: 0.7730\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4233 - acc: 0.8164 - auc: 0.7463 - val_loss: 0.3985 - val_acc: 0.8275 - val_auc: 0.7760\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4213 - acc: 0.8173 - auc: 0.7494 - val_loss: 0.3976 - val_acc: 0.8281 - val_auc: 0.7771\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4195 - acc: 0.8176 - auc: 0.7525 - val_loss: 0.3971 - val_acc: 0.8280 - val_auc: 0.7776\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4172 - acc: 0.8179 - auc: 0.7561 - val_loss: 0.3967 - val_acc: 0.8287 - val_auc: 0.7783\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8191 - auc: 0.7582 - val_loss: 0.3964 - val_acc: 0.8285 - val_auc: 0.7786\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4144 - acc: 0.8198 - auc: 0.7600 - val_loss: 0.3962 - val_acc: 0.8289 - val_auc: 0.7793\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4137 - acc: 0.8202 - auc: 0.7607 - val_loss: 0.3960 - val_acc: 0.8285 - val_auc: 0.7787\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8202 - auc: 0.7626 - val_loss: 0.3958 - val_acc: 0.8284 - val_auc: 0.7797\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8202 - auc: 0.7641 - val_loss: 0.3958 - val_acc: 0.8288 - val_auc: 0.7795\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4109 - acc: 0.8205 - auc: 0.7654 - val_loss: 0.3958 - val_acc: 0.8289 - val_auc: 0.7796\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4105 - acc: 0.8209 - auc: 0.7657 - val_loss: 0.3958 - val_acc: 0.8287 - val_auc: 0.7802\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4098 - acc: 0.8212 - auc: 0.7671 - val_loss: 0.3955 - val_acc: 0.8289 - val_auc: 0.7806\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4095 - acc: 0.8215 - auc: 0.7677 - val_loss: 0.3954 - val_acc: 0.8289 - val_auc: 0.7813\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8213 - auc: 0.7684 - val_loss: 0.3953 - val_acc: 0.8293 - val_auc: 0.7812\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4087 - acc: 0.8214 - auc: 0.7687 - val_loss: 0.3952 - val_acc: 0.8291 - val_auc: 0.7817\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4079 - acc: 0.8218 - auc: 0.7700 - val_loss: 0.3951 - val_acc: 0.8291 - val_auc: 0.7807\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4077 - acc: 0.8218 - auc: 0.7701 - val_loss: 0.3951 - val_acc: 0.8289 - val_auc: 0.7801\n",
      "Epoch 24/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4073 - acc: 0.8220 - auc: 0.7710 - val_loss: 0.3950 - val_acc: 0.8293 - val_auc: 0.7822\n",
      "Epoch 25/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4074 - acc: 0.8222 - auc: 0.7706 - val_loss: 0.3950 - val_acc: 0.8292 - val_auc: 0.7802\n",
      "Epoch 26/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4071 - acc: 0.8221 - auc: 0.7713 - val_loss: 0.3949 - val_acc: 0.8294 - val_auc: 0.7822\n",
      "Epoch 27/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8220 - auc: 0.7716\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4068 - acc: 0.8220 - auc: 0.7716 - val_loss: 0.3951 - val_acc: 0.8293 - val_auc: 0.7819\n",
      "Epoch 28/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4067 - acc: 0.8225 - auc: 0.7717 - val_loss: 0.3949 - val_acc: 0.8295 - val_auc: 0.7820\n",
      "Epoch 29/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4064 - acc: 0.8224 - auc: 0.7725 - val_loss: 0.3949 - val_acc: 0.8298 - val_auc: 0.7826\n",
      "Epoch 30/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4066 - acc: 0.8224 - auc: 0.7718 - val_loss: 0.3949 - val_acc: 0.8297 - val_auc: 0.7817\n",
      "Epoch 31/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4064 - acc: 0.8224 - auc: 0.7722 - val_loss: 0.3949 - val_acc: 0.8292 - val_auc: 0.7827\n",
      "Epoch 32/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4060 - acc: 0.8228 - auc: 0.7729 - val_loss: 0.3948 - val_acc: 0.8294 - val_auc: 0.7813\n",
      "Epoch 33/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4063 - acc: 0.8227 - auc: 0.7724 - val_loss: 0.3949 - val_acc: 0.8288 - val_auc: 0.7817\n",
      "Epoch 34/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8226 - auc: 0.7730\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4059 - acc: 0.8227 - auc: 0.7732 - val_loss: 0.3948 - val_acc: 0.8290 - val_auc: 0.7820\n",
      "Epoch 35/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4058 - acc: 0.8226 - auc: 0.7732 - val_loss: 0.3948 - val_acc: 0.8291 - val_auc: 0.7823\n",
      "Epoch 36/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4058 - acc: 0.8226 - auc: 0.7732 - val_loss: 0.3947 - val_acc: 0.8293 - val_auc: 0.7824\n",
      "Epoch 37/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8226 - auc: 0.7732\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4057 - acc: 0.8226 - auc: 0.7732 - val_loss: 0.3948 - val_acc: 0.8292 - val_auc: 0.7815\n",
      "Epoch 38/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4055 - acc: 0.8229 - auc: 0.7737 - val_loss: 0.3947 - val_acc: 0.8292 - val_auc: 0.7817\n",
      "Epoch 39/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4058 - acc: 0.8227 - auc: 0.7731 - val_loss: 0.3947 - val_acc: 0.8294 - val_auc: 0.7816\n",
      "Epoch 40/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8225 - auc: 0.7735\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.874999952633516e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4056 - acc: 0.8227 - auc: 0.7736 - val_loss: 0.3948 - val_acc: 0.8291 - val_auc: 0.7814\n",
      "Epoch 41/100\n",
      "587776/588000 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8225 - auc: 0.7737Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4055 - acc: 0.8225 - auc: 0.7737 - val_loss: 0.3948 - val_acc: 0.8293 - val_auc: 0.7822\n",
      "Epoch 00041: early stopping\n",
      "Fold score: 0.7816854135604797\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5183 - acc: 0.7841 - auc: 0.5486 - val_loss: 0.4866 - val_acc: 0.8141 - val_auc: 0.7060\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4597 - acc: 0.8097 - auc: 0.6628 - val_loss: 0.4205 - val_acc: 0.8177 - val_auc: 0.7603\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4413 - acc: 0.8121 - auc: 0.7078 - val_loss: 0.4078 - val_acc: 0.8217 - val_auc: 0.7758\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4314 - acc: 0.8142 - auc: 0.7293 - val_loss: 0.4023 - val_acc: 0.8238 - val_auc: 0.7835\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4260 - acc: 0.8158 - auc: 0.7400 - val_loss: 0.3997 - val_acc: 0.8230 - val_auc: 0.7853\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4230 - acc: 0.8166 - auc: 0.7458 - val_loss: 0.3984 - val_acc: 0.8239 - val_auc: 0.7873\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4201 - acc: 0.8177 - auc: 0.7505 - val_loss: 0.3974 - val_acc: 0.8242 - val_auc: 0.7888\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4184 - acc: 0.8180 - auc: 0.7536 - val_loss: 0.3967 - val_acc: 0.8241 - val_auc: 0.7878\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4169 - acc: 0.8185 - auc: 0.7560 - val_loss: 0.3965 - val_acc: 0.8241 - val_auc: 0.7902\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4155 - acc: 0.8189 - auc: 0.7578 - val_loss: 0.3965 - val_acc: 0.8240 - val_auc: 0.7895\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4142 - acc: 0.8195 - auc: 0.7599 - val_loss: 0.3962 - val_acc: 0.8249 - val_auc: 0.7894\n",
      "Epoch 12/100\n",
      "581632/588000 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8200 - auc: 0.7615\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4133 - acc: 0.8199 - auc: 0.7614 - val_loss: 0.3959 - val_acc: 0.8255 - val_auc: 0.7903\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4121 - acc: 0.8201 - auc: 0.7633 - val_loss: 0.3959 - val_acc: 0.8253 - val_auc: 0.7895\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4122 - acc: 0.8203 - auc: 0.7633 - val_loss: 0.3959 - val_acc: 0.8251 - val_auc: 0.7907\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4118 - acc: 0.8205 - auc: 0.7636 - val_loss: 0.3959 - val_acc: 0.8254 - val_auc: 0.7910\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4113 - acc: 0.8206 - auc: 0.7646 - val_loss: 0.3958 - val_acc: 0.8251 - val_auc: 0.7901\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8206 - auc: 0.7648 - val_loss: 0.3958 - val_acc: 0.8255 - val_auc: 0.7907\n",
      "Epoch 18/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8210 - auc: 0.7658\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8210 - auc: 0.7658 - val_loss: 0.3958 - val_acc: 0.8257 - val_auc: 0.7901\n",
      "Epoch 19/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8207 - auc: 0.7662Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8206 - auc: 0.7662 - val_loss: 0.3957 - val_acc: 0.8257 - val_auc: 0.7911\n",
      "Epoch 00019: early stopping\n",
      "Fold score: 0.7894387468404501\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.6221 - acc: 0.6601 - auc: 0.5403 - val_loss: 0.5189 - val_acc: 0.8116 - val_auc: 0.6539\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4837 - acc: 0.7968 - auc: 0.6300 - val_loss: 0.4363 - val_acc: 0.8142 - val_auc: 0.7341\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4539 - acc: 0.8076 - auc: 0.6851 - val_loss: 0.4196 - val_acc: 0.8178 - val_auc: 0.7604\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4418 - acc: 0.8105 - auc: 0.7117 - val_loss: 0.4123 - val_acc: 0.8197 - val_auc: 0.7683\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4345 - acc: 0.8126 - auc: 0.7268 - val_loss: 0.4088 - val_acc: 0.8199 - val_auc: 0.7738\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4298 - acc: 0.8141 - auc: 0.7358 - val_loss: 0.4069 - val_acc: 0.8189 - val_auc: 0.7767\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4271 - acc: 0.8147 - auc: 0.7403 - val_loss: 0.4057 - val_acc: 0.8197 - val_auc: 0.7787\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4231 - acc: 0.8163 - auc: 0.7470 - val_loss: 0.4049 - val_acc: 0.8194 - val_auc: 0.7800\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4212 - acc: 0.8164 - auc: 0.7500 - val_loss: 0.4041 - val_acc: 0.8206 - val_auc: 0.7799\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4193 - acc: 0.8177 - auc: 0.7524 - val_loss: 0.4035 - val_acc: 0.8199 - val_auc: 0.7822\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4174 - acc: 0.8184 - auc: 0.7554 - val_loss: 0.4033 - val_acc: 0.8200 - val_auc: 0.7816\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4159 - acc: 0.8191 - auc: 0.7576 - val_loss: 0.4031 - val_acc: 0.8209 - val_auc: 0.7803\n",
      "Epoch 13/100\n",
      "585728/588000 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8193 - auc: 0.7593\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4147 - acc: 0.8193 - auc: 0.7594 - val_loss: 0.4029 - val_acc: 0.8207 - val_auc: 0.7811\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4134 - acc: 0.8197 - auc: 0.7614 - val_loss: 0.4029 - val_acc: 0.8207 - val_auc: 0.7833\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4134 - acc: 0.8197 - auc: 0.7612 - val_loss: 0.4028 - val_acc: 0.8207 - val_auc: 0.7824\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4127 - acc: 0.8198 - auc: 0.7621 - val_loss: 0.4026 - val_acc: 0.8207 - val_auc: 0.7824\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8202 - auc: 0.7634 - val_loss: 0.4025 - val_acc: 0.8202 - val_auc: 0.7835\n",
      "Epoch 18/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4120 - acc: 0.8203 - auc: 0.7633 - val_loss: 0.4025 - val_acc: 0.8206 - val_auc: 0.7823\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4112 - acc: 0.8206 - auc: 0.7644 - val_loss: 0.4025 - val_acc: 0.8207 - val_auc: 0.7824\n",
      "Epoch 20/100\n",
      "584704/588000 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8207 - auc: 0.7648\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4110 - acc: 0.8207 - auc: 0.7649 - val_loss: 0.4024 - val_acc: 0.8211 - val_auc: 0.7827\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8210 - auc: 0.7654 - val_loss: 0.4023 - val_acc: 0.8209 - val_auc: 0.7826\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8210 - auc: 0.7657 - val_loss: 0.4024 - val_acc: 0.8213 - val_auc: 0.7828\n",
      "Epoch 23/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8208 - auc: 0.7656\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.749999905267032e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4106 - acc: 0.8207 - auc: 0.7655 - val_loss: 0.4023 - val_acc: 0.8211 - val_auc: 0.7831\n",
      "Epoch 24/100\n",
      "583680/588000 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8212 - auc: 0.7660Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4102 - acc: 0.8212 - auc: 0.7660 - val_loss: 0.4023 - val_acc: 0.8211 - val_auc: 0.7832\n",
      "Epoch 00024: early stopping\n",
      "Fold score: 0.782191261065979\n",
      "Expected number of inputs: 20\n",
      "Train on 588000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "588000/588000 [==============================] - 4s 6us/sample - loss: 0.5276 - acc: 0.7711 - auc: 0.5508 - val_loss: 0.4906 - val_acc: 0.8161 - val_auc: 0.6829\n",
      "Epoch 2/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4630 - acc: 0.8084 - auc: 0.6558 - val_loss: 0.4230 - val_acc: 0.8207 - val_auc: 0.7493\n",
      "Epoch 3/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4427 - acc: 0.8119 - auc: 0.7054 - val_loss: 0.4099 - val_acc: 0.8239 - val_auc: 0.7673\n",
      "Epoch 4/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4329 - acc: 0.8138 - auc: 0.7269 - val_loss: 0.4038 - val_acc: 0.8245 - val_auc: 0.7749\n",
      "Epoch 5/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4273 - acc: 0.8154 - auc: 0.7378 - val_loss: 0.4010 - val_acc: 0.8254 - val_auc: 0.7778\n",
      "Epoch 6/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4233 - acc: 0.8160 - auc: 0.7454 - val_loss: 0.3997 - val_acc: 0.8257 - val_auc: 0.7799\n",
      "Epoch 7/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4211 - acc: 0.8173 - auc: 0.7489 - val_loss: 0.3989 - val_acc: 0.8263 - val_auc: 0.7812\n",
      "Epoch 8/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4191 - acc: 0.8179 - auc: 0.7520 - val_loss: 0.3982 - val_acc: 0.8264 - val_auc: 0.7811\n",
      "Epoch 9/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4170 - acc: 0.8190 - auc: 0.7556 - val_loss: 0.3976 - val_acc: 0.8262 - val_auc: 0.7819\n",
      "Epoch 10/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4159 - acc: 0.8188 - auc: 0.7573 - val_loss: 0.3973 - val_acc: 0.8257 - val_auc: 0.7834\n",
      "Epoch 11/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4144 - acc: 0.8195 - auc: 0.7599 - val_loss: 0.3972 - val_acc: 0.8260 - val_auc: 0.7830\n",
      "Epoch 12/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4133 - acc: 0.8193 - auc: 0.7617 - val_loss: 0.3970 - val_acc: 0.8257 - val_auc: 0.7828\n",
      "Epoch 13/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4123 - acc: 0.8203 - auc: 0.7628 - val_loss: 0.3969 - val_acc: 0.8267 - val_auc: 0.7836\n",
      "Epoch 14/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4114 - acc: 0.8202 - auc: 0.7643 - val_loss: 0.3967 - val_acc: 0.8267 - val_auc: 0.7840\n",
      "Epoch 15/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4108 - acc: 0.8209 - auc: 0.7655 - val_loss: 0.3966 - val_acc: 0.8266 - val_auc: 0.7854\n",
      "Epoch 16/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4104 - acc: 0.8209 - auc: 0.7661 - val_loss: 0.3963 - val_acc: 0.8272 - val_auc: 0.7845\n",
      "Epoch 17/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4099 - acc: 0.8209 - auc: 0.7668 - val_loss: 0.3962 - val_acc: 0.8267 - val_auc: 0.7850\n",
      "Epoch 18/100\n",
      "579584/588000 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8208 - auc: 0.7680\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4093 - acc: 0.8209 - auc: 0.7679 - val_loss: 0.3960 - val_acc: 0.8265 - val_auc: 0.7852\n",
      "Epoch 19/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4089 - acc: 0.8214 - auc: 0.7684 - val_loss: 0.3959 - val_acc: 0.8273 - val_auc: 0.7852\n",
      "Epoch 20/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4085 - acc: 0.8215 - auc: 0.7690 - val_loss: 0.3959 - val_acc: 0.8272 - val_auc: 0.7841\n",
      "Epoch 21/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4084 - acc: 0.8219 - auc: 0.7691 - val_loss: 0.3958 - val_acc: 0.8272 - val_auc: 0.7856\n",
      "Epoch 22/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8219 - auc: 0.7696 - val_loss: 0.3959 - val_acc: 0.8273 - val_auc: 0.7855\n",
      "Epoch 23/100\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4081 - acc: 0.8217 - auc: 0.7693 - val_loss: 0.3959 - val_acc: 0.8272 - val_auc: 0.7851\n",
      "Epoch 24/100\n",
      "578560/588000 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8218 - auc: 0.7699\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.499999810534064e-06.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8219 - auc: 0.7701 - val_loss: 0.3957 - val_acc: 0.8273 - val_auc: 0.7855\n",
      "Epoch 25/100\n",
      "580608/588000 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8217 - auc: 0.7697Restoring model weights from the end of the best epoch.\n",
      "588000/588000 [==============================] - 3s 5us/sample - loss: 0.4078 - acc: 0.8218 - auc: 0.7699 - val_loss: 0.3957 - val_acc: 0.8275 - val_auc: 0.7864\n",
      "Epoch 00025: early stopping\n",
      "Fold score: 0.784459628694629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "n_splits = 50\n",
    "\n",
    "trained_estimators = []\n",
    "histories = []\n",
    "scores = []\n",
    "\n",
    "cv = KFold(n_splits=n_splits, random_state=42)\n",
    "for train_idx, valid_idx in cv.split(x_train, y_train):\n",
    "    \n",
    "    x_train_train = x_train[train_idx]\n",
    "    y_train_train = y_train[train_idx]\n",
    "    x_train_valid = x_train[valid_idx]\n",
    "    y_train_valid = y_train[valid_idx]\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    estimator = make_model(x_train, categorial_part)\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=10,\n",
    "                                          verbose=1, mode='max', restore_best_weights=True)\n",
    "    \n",
    "    rl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, min_lr=1e-6, mode='max', verbose=1)\n",
    "    \n",
    "    history = estimator.fit(make_inputs(x_train_train, categorial_part), y_train_train, batch_size=1024, epochs=100, callbacks=[es, rl],\n",
    "                            validation_data=(make_inputs(x_train_valid, categorial_part), y_train_valid))\n",
    "    trained_estimators.append(estimator)\n",
    "    histories.append(history)\n",
    "    \n",
    "    oof_part = estimator.predict(make_inputs(x_train_valid, categorial_part))\n",
    "    score = roc_auc_score(y_train_valid, oof_part)\n",
    "    print('Fold score:', score)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.785694722039701\n"
     ]
    }
   ],
   "source": [
    "print('Mean score:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-8113cd96bd4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy - Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(18,18))\n",
    "\n",
    "# AUC\n",
    "for h in histories:\n",
    "    axs[0,0].plot(h.history['auc'], color='g')\n",
    "axs[0,0].set_title('Model AUC - Train')\n",
    "axs[0,0].set_ylabel('AUC')\n",
    "axs[0,0].set_xlabel('Epoch')\n",
    "\n",
    "for h in histories:\n",
    "    axs[0,1].plot(h.history['val_auc'], color='b')\n",
    "axs[0,1].set_title('Model AUC - Test')\n",
    "axs[0,1].set_ylabel('AUC')\n",
    "axs[0,1].set_xlabel('Epoch')\n",
    "\n",
    "# accuracy\n",
    "for h in histories:\n",
    "    axs[1,0].plot(h.history['accuracy'], color='g')\n",
    "axs[1,0].set_title('Model accuracy - Train')\n",
    "axs[1,0].set_ylabel('Accuracy')\n",
    "axs[1,0].set_xlabel('Epoch')\n",
    "\n",
    "for h in histories:\n",
    "    axs[1,1].plot(h.history['val_accuracy'], color='b')\n",
    "axs[1,1].set_title('Model accuracy - Test')\n",
    "axs[1,1].set_ylabel('Accuracy')\n",
    "axs[1,1].set_xlabel('Epoch')\n",
    "\n",
    "# loss\n",
    "for h in histories:\n",
    "    axs[2,0].plot(h.history['loss'], color='g')\n",
    "axs[2,0].set_title('Model loss - Train')\n",
    "axs[2,0].set_ylabel('Loss')\n",
    "axs[2,0].set_xlabel('Epoch')\n",
    "\n",
    "for h in histories:\n",
    "    axs[2,1].plot(h.history['val_loss'], color='b')\n",
    "axs[2,1].set_title('Model loss - Test')\n",
    "axs[2,1].set_ylabel('Loss')\n",
    "axs[2,1].set_xlabel('Epoch')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7fbe281740f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_estimators[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1112\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1113\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1114\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3795\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3874\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3875\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3876\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Placeholder:0\", shape=(), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8553dda17ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_test_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorial_part\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrained_estimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;31m# Reset the state of loss metric wrappers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1114\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1116\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros(x_test.shape[0])\n",
    "x_test_inputs = make_inputs(x_test, categorial_part)\n",
    "for estimator in trained_estimators:\n",
    "    y_pred += estimator.predict(x_test_inputs).reshape(-1) / len(trained_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keras = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blend Logit and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.add(y_pred_logit, y_pred_rdc,y_pred_keras) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/cat-in-the-dat-ii/sample_submission.csv', index_col='id')\n",
    "submission['target'] = y_pred\n",
    "submission.to_csv('logit_keras.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600000</th>\n",
       "      <td>0.146665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600001</th>\n",
       "      <td>0.280411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600002</th>\n",
       "      <td>0.163589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600003</th>\n",
       "      <td>0.113939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600004</th>\n",
       "      <td>0.157558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target\n",
       "id              \n",
       "600000  0.146665\n",
       "600001  0.280411\n",
       "600002  0.163589\n",
       "600003  0.113939\n",
       "600004  0.157558"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
